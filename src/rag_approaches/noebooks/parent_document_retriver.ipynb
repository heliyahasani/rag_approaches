{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parent Document Retrivier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A method that involves chopping big parts (parent chunks) into even smaller portions (child chunks). Because they are broken up into smaller sections, the information they convey is more focused and retains its informational value over text paragraphs.\n",
    "\n",
    "There's a tiny issue with all of this:\n",
    "\n",
    "We need to segment our documents into manageable parts if we want to be exact when looking for the most pertinent information.\n",
    "However, it is also crucial to give the LLM a solid context, which is accomplished by giving it in bigger portions.\n",
    "The primary goal is to further divide the big pieces—the parent chunks and documents—into smaller ones—the child chunks and documents. After that, return the parents chunks to which the top K child document belongs and search for the most pertinent top K documents using the child chunks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "    \n",
    "## Text Splitting & Docloader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory path (adjust as needed)\n",
    "directory_path = \"/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post\"\n",
    "\n",
    "# Use glob to find all .txt files in the directory and subdirectories\n",
    "txt_files = glob.glob(f\"{directory_path}**/*.txt\", recursive=True)\n",
    "\n",
    "# Initialize an empty list for loaders\n",
    "loaders = [TextLoader(path) for path in txt_files]\n",
    "\n",
    "# Initialize an empty list to store documents\n",
    "docs = []\n",
    "\n",
    "# Loop through each loader, load the document, and extend the docs list\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heliya/Desktop/rag_approaches/.venv/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "\n",
    "bge_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={'device': 'cuda'},\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This text splitter is used to create the child documents\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "\n",
    "# The vectorstore to use to index both parent documents and child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\",\n",
    "    embedding_function= bge_embeddings # or your preferred embedding function\n",
    ")\n",
    "\n",
    "# Function to index parent documents and their child chunks with metadata\n",
    "def index_documents_with_metadata(parent_documents):\n",
    "    for doc in parent_documents:\n",
    "        # Add or update metadata for the parent document\n",
    "        doc.metadata.update({\n",
    "            'document_id': doc.metadata.get('source', 'unknown'),  # Example to add unique identifier\n",
    "        })\n",
    "        # Index the parent document with metadata\n",
    "        vectorstore.add_texts([doc.page_content], metadatas=[{\"type\": \"parent\", **doc.metadata}])\n",
    "        \n",
    "        # Split the parent document into child chunks and index those with metadata\n",
    "        child_chunks = child_splitter.split_text(doc.page_content)\n",
    "        vectorstore.add_texts(child_chunks, metadatas=[{\"type\": \"child\", \"parent_id\": doc.metadata.get('document_id')} for _ in child_chunks])\n",
    "\n",
    "\n",
    "\n",
    "# Indexing all documents with metadata into the vectorstore\n",
    "index_documents_with_metadata(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedParentDocumentRetriever:\n",
    "    def __init__(self, vectorstore, child_splitter, parent_splitter=None):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.child_splitter = child_splitter\n",
    "        self.parent_splitter = parent_splitter\n",
    "\n",
    "    def retrieve(self, query, retrieve_parents=True, retrieve_children=True):\n",
    "        # Prepare filters based on what you want to retrieve\n",
    "        filters = []\n",
    "        if retrieve_parents:\n",
    "            filters.append({\"type\": \"parent\"})\n",
    "        if retrieve_children:\n",
    "            filters.append({\"type\": \"child\"})\n",
    "        \n",
    "        results = []\n",
    "        for filter in filters:\n",
    "            results.extend(self.vectorstore.similarity_search(query, filter=filter))\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Set up the retriever\n",
    "retriever = UnifiedParentDocumentRetriever(\n",
    "    vectorstore=vectorstore, \n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=None  # Assuming you may or may not use this, it's optional since we load each parent without chunking I have passed as None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve both parent and child documents\n",
    "results = retriever.retrieve(query=\"what is langsmith\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'document_id': '/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt', 'source': '/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt', 'type': 'parent'}, page_content='URL: https://blog.langchain.dev/peering-into-the-soul-of-ai-decision-making-with-langsmith/\\nTitle: Peering Into the Soul of AI Decision-Making with LangSmith\\n\\nEditor\\'s Note: This post was written by Paul Thomson from Commandbar. They\\'ve been awesome partners as they brought their application into production with LangSmith, and we\\'re excited to share their story getting there.\\n\\nDo you ever wonder why you’re getting unhinged responses from ChatGPT sometimes? Or why the heck Midjourney is giving your creations 7 weird fingers? As intelligent as AI is supposed to be, it does produce some pretty unintelligent responses sometimes.\\n\\nNow, if you’re using GPT to write your next “let ‘em down easy breakup message”, the stakes are low - it doesn’t really matter. But if a core product feature is leveraging AI and your customers depend on super-intelligent perfection, you’re going to want some security and assurances that the outputs are up to scratch. Enter, LangSmith.\\n\\nSince the launch of HelpHub, we were trying to do things on hard mode when it came to iterating and improving functionality. That is, of course, until the LangChain team tantalized us onto their LangSmith beta. What we didn’t expect was how immediate the downstream improvements were to our flagship AI-powered product.\\n\\nBut is LangSmith robust enough for us to rely on entirely for our LLM-powered QA? Or is it just another nice-to-have feature for our ENG team?\\n\\nIf you’re at the intersection of product, LLMs, and user experience, we’ve just walked so you can run. Time to read on.\\n\\nWhat Is LangSmith?\\n\\nLangSmith is a framework built on the shoulders of LangChain. It’s designed to track the inner workings of LLMs and AI agents within your product.\\n\\nThose LLM inner-workings can be categorized into 4 main buckets - each with its own flair of usefulness. Here’s a breakdown of how they all work in unison and what you can expect.\\n\\n\\n\\nDebugging:\\n\\nWhen your LLM starts throwing curveballs instead of answers, you don\\'t just want to sit there catching them. With LangSmith, you can roll up your sleeves and play detective. We use the debugging tools to dive into perplexing agent loops, frustratingly slow chains, and to scrutinize prompts like they\\'re suspects in a lineup.\\n\\nTesting:\\n\\nTesting LLM applications without LangSmith is like trying to assemble IKEA furniture without the manual: sure, you could wing it, but do you really want to risk it? Baked into LangSmith is the option to utilize existing datasets or create new ones, and run them against your chains. Visual feedback on outputs and accuracy metrics are presented within the interface, streamlining the testing process for our eng team (we really like this).\\n\\nEvaluating:\\n\\nBeyond mere testing, evaluation in LangSmith delves into the performance nuances of LLM runs. While the built-in evaluators offer a preliminary analysis, the true power lies in guiding your focus towards crucial examples (more on how we do that later). As your datasets grow, LangSmith ensures you never miss a beat, making evaluations both comprehensive and insightful. Because \"good enough\" isn\\'t in your vocabulary, right?\\n\\nMonitoring:\\n\\nThink of LangSmith\\'s monitoring as your AI’s babysitter: always vigilant, never distracted, and ready to report every little mischief. It\\'ll give you the play-by-play, ensure everything\\'s in order, and notify you if things get out of hand. We even went a step ahead and piped these flags directly into Slack giving us almost realtime monitoring when our users hit a deadend with chat conversations.\\n\\nhttps://twitter.com/zhanghaili0610\\n\\nLangChain vs LangSmith: What’s the difference?\\n\\nWhile LangChain is the muscle doing the heavy lifting with Chains, Prompts, and Agents, understanding the \\'why\\' behind the decisions LLMs make is a maze we often found ourselves lost in. That\\'s where LangSmith shines, acting as an AI compass built into LangChain, guiding us through the intricate decision pathways and results that our chatbot generates.\\n\\n\"LangChain\\'s (the company\\'s) goal is to make it as easy as possible to develop LLM applications\"\\n\\nsaid Harrison Chase, co-founder and CEO of LangChain.\\n\\n\"To that end, we realized pretty early that what was needed - and missing - wasn\\'t just an open source tool like LangChain, but also a complementary platform for managing these new types of applications. To that end, we built LangSmith - which is usable with or without LangChain and let\\'s users easily debug, monitor, test, evaluate, and now (with the recently launched Hub) share and collaborate on their LLM applications.”\\n\\n\\n\\nWhat Are LangSmith Traces?\\n\\nTraces in the world of LangSmith are analogous to logs when programming; they allow us to easily see what text came in and out of chains and LLMs. Think of them as detailed breadcrumbs illuminating the AI\\'s journey. Each trace, like a footprint on a sandy beach, represents a pivotal AI decision. Traces don\\'t merely depict the path taken; they shed light on the underlying thought process and actions taken at each juncture.\\n\\nHere’s what one of our traces looks like inside LangSmith:\\n\\nAll the individual traces are consolidated into datasets:\\n\\n\\n\\nDo You Really Need To Use LangSmith?\\n\\nWhen generative AI works, it feels like watching a viral “satisfying video” montage - so delightful. But when it doesn’t, it sucks, and sometimes it sucks real bad.\\n\\nTake it from us, as we integrated AI more heavily and widely across our products, we’ve been more conscious than ever that the quality of the outputs matches the quality and trust that our customers have in our product. G2 review flex here.\\n\\nTruth is, until we powered up LangSmith, we truly had no way of postmorteming responses from OpenAI or testing how prompt changes, or even upgrading to a new model like GPT-4, would affect the answers.\\n\\nHow We Use LangSmith at CommandBar: AI-Powered User Assistance\\n\\nWe’ve covered a lot of ground so far on how LangSmith works. From here on out, we’re ripping the covers off and showing you what’s under the hood of our HelpHub <> LangSmith setup. To give you a little context, first, let’s dive into what HelpHub is.\\n\\nHelpHub is a GPT-Powered chatbot for any site. It syncs with any public URL or your existing help center to assist users in getting instantaneous support while circumventing customer service teams or manually screening docs.\\n\\nWhile we utilize our own search with ElasticSearch, we rely on LangChain to merge those search results into meaningful prompts for HelpHub. This synergy allows us to search hundreds of docs and sites in milliseconds, source relevant context, compute reasoning, and deliver an answer to our users (with citations!) almost instantly.\\n\\nIt’s through this core integration with LangChain that we’re able to capture traces via LangSmith. primarily to finetune and optimize our chatbot’s functionality and sandbox future improvements for our users.\\n\\nAccurate AI Responses Are A Necessity, Not A Nice-To-Have\\n\\nWe pride ourselves on offering precise and relevant answers to user queries and that has always been a strong USP for us. However, with the aforementioned challenges of unhinged AI-generated responses not always aligning with user expectations, once we flicked on LangSmith, we took our prototyping and QA from mediocre guesswork to David Blane quality witchcraft.\\n\\nSince its integration, LangSmith has traced over X0 million tokens for HelpHub (about XM tokens a week!).\\n\\nReal-world Example of LangSmith In Our Production\\n\\nBelow is an example from Gus, one of our mighty talented LangSmith connoisseurs, caught in one of our traces.\\n\\nWhat he’s referring to in the screenshot is the fact that each prompt from HelpHub should reference the source document that it’s referencing when giving users an answer. We do this primarily to legitimize the LLMs response and give our HelpHub customer’s peace of mind that their end users are in fact getting to the help resource they need (instead of an LLM just hallucinating and giving any response it wants.)\\n\\nFrom here, we went into LangSmith and saw that the LLM actually returned no source, even though we asked it to. Ideally, the source should be returned on the first line in the “Output” section above the actual answer.\\n\\nWe updated our prompt to be more firm when asking for the sources:\\n\\nPreviously the snippet in the prompt responsible for this was: Return the source ID with the most relevance to your answer.\\n\\nWe update that piece of the prompt to: ALWAYS return the source ID with the most relevance to your answer prior to answering the question .\\n\\nWe then tested everything using LangSmith evals, to make sure that it fixes the issue before pushing to production.\\n\\nou can now clearly see the citations coming through with the responses in the traces, and we’re good to ship the changes to the prompt to prod.\\n\\nThe Verdict: Are We Betting The House On LangSmith?\\n\\nWhen a product like LangSmith comes along, it can feel really natural to default to the path of least resistance and offhand all responsibility. As we start to add additional functionality to HelpHub, there’s an inherent risk that GPT is going to lead users astray, and that’s just not an option we’re willing to entertain.\\n\\nSo, in short, yes, we are putting a lot of trust right now in LangSmith and scaling our prototyping and debugging rapidly. The systems we’re building internally have already been instrumental in improving user experience, and as you’ve read earlier, many of these insights and improvements have come directly from those real-time traces from users chatting with HelpHub in the wild.\\n\\nLeveraging User Feedback For Improvements:\\n\\nWe believe that every piece of user feedback, whether positive or negative, is a goldmine of insights. And with LangSmith\\'s help (plus a little ingenuity on our side), we\\'ve turned these insights into actionable improvements.\\n\\nHere\\'s how our feedback loop works:\\n\\nReal-time Feedback Collection: As users interact with HelpHub, they have the opportunity to provide feedback on the AI-generated responses, signaling with a simple \"thumbs up\" or \"thumbs down\". In-depth Analysis with LangSmith: Instead of just collecting feedback in the form or positive or negative signals, we delve deeper (particularly for the negative signals). Using LangSmith, we’re able to attribute each signal to an individual trace. In that trace, we can map the exact sequence of steps the model took to generate that response. We essentially replay GPT\\'s thought process and LangChain’s actions, giving us the insights into what went right and where it veered off track. Categorizing Into Datasets: Central to our refinement process is LangSmith\\'s use of dynamic datasets. We maintain multiple datasets, each tailored to different query types and user interactions. These datasets are essentially compilations of identical states of our environment at the time the trace was captured. This ensures that when there\\'s an update to the prompt or LLM, a new dataset starts to compile those traces, preventing any contamination. Automating ENG-team Signals: When a user provides feedback, say a thumbs down, it\\'s immediately flagged to our team via Slack. We built this little snippet to help the team screen traces and prioritize the ones that need attention right away. Iterating Quickly: We rigorously review the feedback, analyze the corresponding traces, and then, based on our insights, make informed adjustments to the model\\'s role, prompts, or configurations to try and curb whatever jankiness was happening. This iterative process ensures our AI chatbot is continually refining its understanding, resonating more with user needs, and exceeding expectations over time.\\n\\nBy combining granular AI insights through LangSmith with user feedback, we’ve created a pretty tight loop of perpetual improvement with HelpHub. This was such an important unlock for us as we build in tactical functionality to our AI.\\n\\n\\n\\nAdvice for Product Teams Considering LangSmith\\n\\nWhen you\\'re in the thick of product development, the whirlwind of AI and LLMs can be overwhelming. We\\'ve been there, boots on the ground, making sense of it all. From our journey with LangSmith and HelpHub, here\\'s some hard-earned wisdom for fellow product teams.\\n\\nStart with Data, and Start Now:\\n\\nAI thrives on data. But don’t wait for the \\'perfect\\' moment or the \\'perfect\\' dataset. Start collecting now. Setting up LangSmith takes literally 5 minutes if you’re already using LangChain. Every bit of data, every interaction, adds a layer of understanding. But, a word to the wise: quality matters. Make sure the data reflects real-world scenarios, ensuring your AI resonates with genuine user needs.\\n\\nDive Deep with Traces: Don\\'t just skim the surface. Use LangSmith\\'s trace feature to dive deep into AI decision-making. Every trace is a lesson, a chance to improve. Experiment with Prompts: One of LangSmith\\'s standout features is its ability to test a new prompt across multiple examples without manual entry each time. This makes it incredibly efficient to iterate on your setup, ensuring you get the desired output from the AI. Note, in addition, the Playground is also an amazing tool to dig around with for testing prompts and adjustments to traces too. Lean on the Community: There\\'s a whole community of LangSmith users out there. Swap stories, share challenges, and celebrate successes. You\\'re not alone on this journey. Stay on Your Toes: AI doesn’t stand still, and neither should you. Keep an eye on LangSmith\\'s updates. New features? Dive in. Test, iterate, refine.\\n\\nConclusion\\n\\nAfter diving deep with LangSmith\\'s traces, experimenting with prompts, testing, and iterating on our LLM environment, here\\'s the real talk: LangSmith isn\\'t just a tool for us - it\\'s become a critical inclusion in our stack. We\\'ve moved from crossing our fingers and toes hoping our AI works to knowing exactly how and why.\\n\\nSo, to our fellow AI product people trailblazers, dive into LangSmith. You’d be silly not to if you’re already using LangChain!'),\n",
       " Document(metadata={'document_id': '/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_announcing-langsmith_.txt', 'source': '/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_announcing-langsmith_.txt', 'type': 'parent'}, page_content='URL: https://blog.langchain.dev/announcing-langsmith/\\nTitle: Announcing LangSmith, a unified platform for debugging, testing, evaluating, and monitoring your LLM applications\\n\\nLangChain exists to make it as easy as possible to develop LLM-powered applications.\\n\\nWe started with an open-source Python package when the main blocker for building LLM-powered applications was getting a simple prototype working. We remember seeing Nat Friedman tweet in late 2022 that there was “not enough tinkering happening.” The LangChain open-source packages are aimed at addressing this and we see lots of tinkering happening now (Nat agrees)–people are building everything from chatbots over internal company documents to an AI dungeon master for a Dungeons and Dragons game.\\n\\nThe blocker has now changed. While it’s easy to build a prototype of an application in ~5 lines of LangChain code, it’s still deceptively hard to take an application from prototype to production. The main issue that we see today is application performance–something that works ~30% of the time is good enough for a Twitter demo, but not nearly good enough for production.\\n\\nToday, we’re introducing LangSmith, a platform to help developers close the gap between prototype and production. It’s designed for building and iterating on products that can harness the power–and wrangle the complexity–of LLMs.\\n\\nLangSmith is now in closed beta. So if you’re looking for a robust, unified, system for debugging, testing, evaluating, and monitoring your LLM applications, sign up here.\\n\\n\\n\\nHow did we get here?\\n\\nGiven the stochastic nature of LLMs, it is not easy–and there’s currently no straightforward way–to answer the simple question of “what’s happening in these models?,” let alone getting them to work reliably. The builders we hear from are running into the same roadblocks (and it’s true for our team, too):\\n\\nUnderstanding what exactly the final prompt to the LLM call is (after all the prompt template formatting, this final prompt can be long and obfuscated)\\n\\nUnderstanding what exactly is returned from the LLM call at each step (before it is post-processed or transformed in any way)\\n\\nUnderstanding the exact sequence of calls to LLM (or other resources), and how they are chained together\\n\\nTracking token usage\\n\\nManaging costs\\n\\nTracking (and debugging) latency\\n\\nNot having a good dataset to evaluate their application over\\n\\nNot having good metrics with which to evaluate their application\\n\\nUnderstanding how users are interacting with the product\\n\\nAll of these problems have parallels in traditional software engineering. And, in response, a set of practices and tools for debugging, testing, logging, monitoring, etc. has emerged to help developers abstract away common infrastructure and focus on what really matters - building their applications. LLM application developers deserve the same.\\n\\nLangSmith aspires to be that platform. Over the last few months, we’ve been working directly with some early design partners and testing it on our own internal workflows, and we’ve found LangSmith helps teams in 5 core ways:\\n\\nDebugging\\n\\nLangSmith gives you full visibility into model inputs and output of every step in the chain of events. This makes it easy for teams to experiment with new chains and prompt templates, and spot the source of unexpected results, errors, or latency issues. We’ll also expose latency and token usage so that you can identify which calls are causing issues.\\n\\nWe’ve also made it easy to change and rerun examples from the UI. We added this feature after seeing teams take logs of bad examples and copy-paste into the OpenAI playground to tweak the prompt until they got a good result. We wanted to eliminate that friction, and now with the click of a button, you can go from a log to a playground where you can actively edit. This is currently supported for both OpenAI and Anthropic models, with support for more coming soon. We’re also working on supporting this for chains in general.\\n\\nThis deep visibility into model performance has been particularly helpful for teams developing complex applications. LangSmith helped Streamlit and Snowflake implement agents that could intelligently and reliably answer questions about their data.\\n\\n\"LangChain has been instrumental in helping us prototype intelligent agents at Snowflake,” said Adrien Treuille, Director of Product at Snowflake. “LangSmith was easy to integrate, and the agnostic open source API made it very flexible to adapt to our implementation,” tacked on Richard Meng, Senior Software Engineer at Snowflake.\\n\\nBoston Consulting Group also built a highly-customized, and highly performant, series of applications on top of LangChain’s framework by relying on this same infrastructure.\\n\\n“We are proud of being one of the early LangChain design partners and users of LangSmith,” Said Dan Sack, Managing Director and Partner at BCG. “The use of LangSmith has been key to bringing production-ready LLM applications to our clients. LangSmith\\'s ease of integration and intuitive UI enabled us to have an evaluation pipeline up and running very quickly. Additionally, tracing and evaluating the complex agent prompt chains is much easier, reducing the time required to debug and refine our prompts, and giving us the confidence to move to deployment.”\\n\\nIn another example of debugging in action, we partnered with DeepLearningAI to equip learners in the recently-released LangChain courses with access to LangSmith. This allowed students to easily visualize the exact sequence of calls, and the inputs and outputs at each step in the chain with precision. Students can understand exactly what the chains, prompts, and LLMs were doing, which helps build intuition as they learn to create new and more sophisticated applications.\\n\\n\\n\\nTesting\\n\\nOne of the main questions we see developers grapple with is: “If I change this chain/prompt, how does that affect my outputs?” The most effective way to answer this question is to curate a dataset of examples that you care about, and then run any changed prompts/chains over this dataset. LangSmith first makes it easy to create these datasets from traces or by uploading datasets you’ve curated manually. You can then easily run chains and prompts over those data sets.\\n\\nThe first helpful step is simply manually looking at the new inputs and outputs. Although this may seem unsatisfyingly basic, it actually has some benefits - many of the companies we’ve spoken to actually like some manual touch points because it allows them to develop better intuition about how to interact with LLMs. This intuition can prove incredibly valuable when trying to think about how to improve the application. The main unlock we hope to provide is a clear interface for letting developers easily see the inputs and outputs for each data point, as without that visibility they cannot build up that intuition.\\n\\nToday, we primarily hear from teams that want to bring their prototype into production, and are narrowing in on specific prompts they’d like to improve. Klarna is building industry-leading AI integrations that go beyond a simple call to a language model, and instead rely on a series of calls. As they focus on a specific section, LangSmith has provided the tools and data they need to ensure no regressions occur.\\n\\nIn parallel, we’re starting to hear from more and more ambitious teams that are striving for a more effective approach.\\n\\nEvaluating\\n\\nLangSmith integrates seamlessly with our open source collection of evaluation modules. These modules have two main types of evaluation: heuristics and LLMs. Heuristic Evaluations will use logic like regexes to evaluate the correctness of an answer. LLM evaluations will use LLMs to evaluate themselves.\\n\\nWe are extremely bullish on LLM assisted evaluation over the long term. Critics of this approach will say that it’s conceptually shaky and practically costly (time and money). But, we’ve been seeing some very compelling evidence come out of top labs that this is a viable strategy. And, as we collectively make improvements to these models–both private and open source–and usage becomes more ubiquitous, we expect costs to come down considerably.\\n\\nMonitoring\\n\\nWhile debugging, testing, and evaluating can help you get from prototype to production, the work doesn’t stop once you ship. Developers need to actively track performance, and ideally, optimize that performance based on feedback. We consistently see developers relying on LangSmith to track the system-level performance of their application (like latency and cost), track the model/chain performance (through associating feedback with runs), debug issues (diving into a particular run that went wrong), and establish a broad understanding of how users are interacting with their application and what their experience is like.\\n\\nAmbitious startups like Mendable, Multi-On and Quivr, who are already serving thousands of users are actively using LangSmith to not only monitor overall usage, but also use those insights to take action on critical issues.\\n\\n“Thanks to Langchain smith we were able to analyze our LLM calls, understand the performance of the different chain methods ( stuff vs reduce) for QA and improve upon it. It even helped us debug and understand errors we made. We are consistently using it to improve our prompt engineering and look forward to the new features,” said Stan Girard, Head of GenAI at Theodo and creator of Quivr.\\n\\n\\n\\nA unified platform\\n\\nWhile each of these product areas provide unique value, often at a specific point in time in the development process, we believe a great deal of the long term impact of LangSmith will come from having a single, fully-integrated hub to do this work from. We see teams with all kinds of Rube Goldberg-machine-like processes for managing their LLM applications, and we want to make that a thing of the past.\\n\\nAs a very simple example, we considered it to be table stakes for LangSmith to help users easily create datasets from existing logs and use them immediately for testing and evaluation, seamlessly connecting the logging/debugging workflows to the testing/evaluation ones.\\n\\n\\n\\nFintual, a Latin American startup with big dreams to help their citizens build wealth through a personalized financial advisor, found LangSmith early in their LLM development journey. “As soon as we heard about LangSmith, we moved our entire development stack onto it. We could build the evaluation, testing, and monitoring tools we needed in-house. But it would be 1000x worse, take us 10x longer, and require a team 2x the size,” said Fintual leader Jose Pena.\\n\\n“Because we are building financial products, the bar for accuracy, personalization, and security is particularly high. LangSmith helps us build products we are confident putting in front of users.”\\n\\nWe can’t wait to bring these benefits to more teams. And we’ve got a long list of features on the roadmap like analytics, playgrounds, collaboration, in-context learning, prompt creation, and more.\\n\\nFinally, we recognize that we cannot build ALL the functionality you will need to make it easy to make your applications production ready today. We’ve made it possible to export datasets in the format OpenAI evals expects so you can contribute them there. This data can be also used directly to fine tune your models on the Fireworks platform (and we aim to make it easy to plug into other fine-tuning systems as well). Finally, we’ve made logs exportable in a generic format and worked with teams like Context to ensure that you can load them into their analytics engine and run analytics over them in there.\\n\\n\\n\\nWe can’t wait to see what you build.'),\n",
       " Document(metadata={'document_id': '/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_announcing-langsmith_?ref=commandbar.ghost.io.txt', 'source': '/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_announcing-langsmith_?ref=commandbar.ghost.io.txt', 'type': 'parent'}, page_content='URL: https://blog.langchain.dev/announcing-langsmith/?ref=commandbar.ghost.io\\nTitle: Announcing LangSmith, a unified platform for debugging, testing, evaluating, and monitoring your LLM applications\\n\\nLangChain exists to make it as easy as possible to develop LLM-powered applications.\\n\\nWe started with an open-source Python package when the main blocker for building LLM-powered applications was getting a simple prototype working. We remember seeing Nat Friedman tweet in late 2022 that there was “not enough tinkering happening.” The LangChain open-source packages are aimed at addressing this and we see lots of tinkering happening now (Nat agrees)–people are building everything from chatbots over internal company documents to an AI dungeon master for a Dungeons and Dragons game.\\n\\nThe blocker has now changed. While it’s easy to build a prototype of an application in ~5 lines of LangChain code, it’s still deceptively hard to take an application from prototype to production. The main issue that we see today is application performance–something that works ~30% of the time is good enough for a Twitter demo, but not nearly good enough for production.\\n\\nToday, we’re introducing LangSmith, a platform to help developers close the gap between prototype and production. It’s designed for building and iterating on products that can harness the power–and wrangle the complexity–of LLMs.\\n\\nLangSmith is now in closed beta. So if you’re looking for a robust, unified, system for debugging, testing, evaluating, and monitoring your LLM applications, sign up here.\\n\\n\\n\\nHow did we get here?\\n\\nGiven the stochastic nature of LLMs, it is not easy–and there’s currently no straightforward way–to answer the simple question of “what’s happening in these models?,” let alone getting them to work reliably. The builders we hear from are running into the same roadblocks (and it’s true for our team, too):\\n\\nUnderstanding what exactly the final prompt to the LLM call is (after all the prompt template formatting, this final prompt can be long and obfuscated)\\n\\nUnderstanding what exactly is returned from the LLM call at each step (before it is post-processed or transformed in any way)\\n\\nUnderstanding the exact sequence of calls to LLM (or other resources), and how they are chained together\\n\\nTracking token usage\\n\\nManaging costs\\n\\nTracking (and debugging) latency\\n\\nNot having a good dataset to evaluate their application over\\n\\nNot having good metrics with which to evaluate their application\\n\\nUnderstanding how users are interacting with the product\\n\\nAll of these problems have parallels in traditional software engineering. And, in response, a set of practices and tools for debugging, testing, logging, monitoring, etc. has emerged to help developers abstract away common infrastructure and focus on what really matters - building their applications. LLM application developers deserve the same.\\n\\nLangSmith aspires to be that platform. Over the last few months, we’ve been working directly with some early design partners and testing it on our own internal workflows, and we’ve found LangSmith helps teams in 5 core ways:\\n\\nDebugging\\n\\nLangSmith gives you full visibility into model inputs and output of every step in the chain of events. This makes it easy for teams to experiment with new chains and prompt templates, and spot the source of unexpected results, errors, or latency issues. We’ll also expose latency and token usage so that you can identify which calls are causing issues.\\n\\nWe’ve also made it easy to change and rerun examples from the UI. We added this feature after seeing teams take logs of bad examples and copy-paste into the OpenAI playground to tweak the prompt until they got a good result. We wanted to eliminate that friction, and now with the click of a button, you can go from a log to a playground where you can actively edit. This is currently supported for both OpenAI and Anthropic models, with support for more coming soon. We’re also working on supporting this for chains in general.\\n\\nThis deep visibility into model performance has been particularly helpful for teams developing complex applications. LangSmith helped Streamlit and Snowflake implement agents that could intelligently and reliably answer questions about their data.\\n\\n\"LangChain has been instrumental in helping us prototype intelligent agents at Snowflake,” said Adrien Treuille, Director of Product at Snowflake. “LangSmith was easy to integrate, and the agnostic open source API made it very flexible to adapt to our implementation,” tacked on Richard Meng, Senior Software Engineer at Snowflake.\\n\\nBoston Consulting Group also built a highly-customized, and highly performant, series of applications on top of LangChain’s framework by relying on this same infrastructure.\\n\\n“We are proud of being one of the early LangChain design partners and users of LangSmith,” Said Dan Sack, Managing Director and Partner at BCG. “The use of LangSmith has been key to bringing production-ready LLM applications to our clients. LangSmith\\'s ease of integration and intuitive UI enabled us to have an evaluation pipeline up and running very quickly. Additionally, tracing and evaluating the complex agent prompt chains is much easier, reducing the time required to debug and refine our prompts, and giving us the confidence to move to deployment.”\\n\\nIn another example of debugging in action, we partnered with DeepLearningAI to equip learners in the recently-released LangChain courses with access to LangSmith. This allowed students to easily visualize the exact sequence of calls, and the inputs and outputs at each step in the chain with precision. Students can understand exactly what the chains, prompts, and LLMs were doing, which helps build intuition as they learn to create new and more sophisticated applications.\\n\\n\\n\\nTesting\\n\\nOne of the main questions we see developers grapple with is: “If I change this chain/prompt, how does that affect my outputs?” The most effective way to answer this question is to curate a dataset of examples that you care about, and then run any changed prompts/chains over this dataset. LangSmith first makes it easy to create these datasets from traces or by uploading datasets you’ve curated manually. You can then easily run chains and prompts over those data sets.\\n\\nThe first helpful step is simply manually looking at the new inputs and outputs. Although this may seem unsatisfyingly basic, it actually has some benefits - many of the companies we’ve spoken to actually like some manual touch points because it allows them to develop better intuition about how to interact with LLMs. This intuition can prove incredibly valuable when trying to think about how to improve the application. The main unlock we hope to provide is a clear interface for letting developers easily see the inputs and outputs for each data point, as without that visibility they cannot build up that intuition.\\n\\nToday, we primarily hear from teams that want to bring their prototype into production, and are narrowing in on specific prompts they’d like to improve. Klarna is building industry-leading AI integrations that go beyond a simple call to a language model, and instead rely on a series of calls. As they focus on a specific section, LangSmith has provided the tools and data they need to ensure no regressions occur.\\n\\nIn parallel, we’re starting to hear from more and more ambitious teams that are striving for a more effective approach.\\n\\nEvaluating\\n\\nLangSmith integrates seamlessly with our open source collection of evaluation modules. These modules have two main types of evaluation: heuristics and LLMs. Heuristic Evaluations will use logic like regexes to evaluate the correctness of an answer. LLM evaluations will use LLMs to evaluate themselves.\\n\\nWe are extremely bullish on LLM assisted evaluation over the long term. Critics of this approach will say that it’s conceptually shaky and practically costly (time and money). But, we’ve been seeing some very compelling evidence come out of top labs that this is a viable strategy. And, as we collectively make improvements to these models–both private and open source–and usage becomes more ubiquitous, we expect costs to come down considerably.\\n\\nMonitoring\\n\\nWhile debugging, testing, and evaluating can help you get from prototype to production, the work doesn’t stop once you ship. Developers need to actively track performance, and ideally, optimize that performance based on feedback. We consistently see developers relying on LangSmith to track the system-level performance of their application (like latency and cost), track the model/chain performance (through associating feedback with runs), debug issues (diving into a particular run that went wrong), and establish a broad understanding of how users are interacting with their application and what their experience is like.\\n\\nAmbitious startups like Mendable, Multi-On and Quivr, who are already serving thousands of users are actively using LangSmith to not only monitor overall usage, but also use those insights to take action on critical issues.\\n\\n“Thanks to Langchain smith we were able to analyze our LLM calls, understand the performance of the different chain methods ( stuff vs reduce) for QA and improve upon it. It even helped us debug and understand errors we made. We are consistently using it to improve our prompt engineering and look forward to the new features,” said Stan Girard, Head of GenAI at Theodo and creator of Quivr.\\n\\n\\n\\nA unified platform\\n\\nWhile each of these product areas provide unique value, often at a specific point in time in the development process, we believe a great deal of the long term impact of LangSmith will come from having a single, fully-integrated hub to do this work from. We see teams with all kinds of Rube Goldberg-machine-like processes for managing their LLM applications, and we want to make that a thing of the past.\\n\\nAs a very simple example, we considered it to be table stakes for LangSmith to help users easily create datasets from existing logs and use them immediately for testing and evaluation, seamlessly connecting the logging/debugging workflows to the testing/evaluation ones.\\n\\n\\n\\nFintual, a Latin American startup with big dreams to help their citizens build wealth through a personalized financial advisor, found LangSmith early in their LLM development journey. “As soon as we heard about LangSmith, we moved our entire development stack onto it. We could build the evaluation, testing, and monitoring tools we needed in-house. But it would be 1000x worse, take us 10x longer, and require a team 2x the size,” said Fintual leader Jose Pena.\\n\\n“Because we are building financial products, the bar for accuracy, personalization, and security is particularly high. LangSmith helps us build products we are confident putting in front of users.”\\n\\nWe can’t wait to bring these benefits to more teams. And we’ve got a long list of features on the roadmap like analytics, playgrounds, collaboration, in-context learning, prompt creation, and more.\\n\\nFinally, we recognize that we cannot build ALL the functionality you will need to make it easy to make your applications production ready today. We’ve made it possible to export datasets in the format OpenAI evals expects so you can contribute them there. This data can be also used directly to fine tune your models on the Fireworks platform (and we aim to make it easy to plug into other fine-tuning systems as well). Finally, we’ve made logs exportable in a generic format and worked with teams like Context to ensure that you can load them into their analytics engine and run analytics over them in there.\\n\\n\\n\\nWe can’t wait to see what you build.'),\n",
       " Document(metadata={'document_id': '/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_fine-tune-your-llms-with-langsmith-and-lilac_.txt', 'source': '/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_fine-tune-your-llms-with-langsmith-and-lilac_.txt', 'type': 'parent'}, page_content='URL: https://blog.langchain.dev/fine-tune-your-llms-with-langsmith-and-lilac/\\nTitle: Fine-tune your LLMs with LangSmith and Lilac\\n\\nIn taking your LLM from prototype into production, many have turned to fine-tuning models to get more consistent and high-quality behavior in their applications. Services like OpenAI and HuggingFace make it easy to fine-tune a model on your application-specific data. All it takes is a JSON file!\\n\\nThe tricky part is deciding what to include in that data. Once your LLM is deployed, it could be prompted given any input - how do you make sure it will respond appropriately for the user or machine it is meant to interact with?\\n\\nFor this, there is no real substitute for high-quality data taken from your unique application context. This is where LangSmith and Lilac can help out.\\n\\nLangSmith + Lilac\\n\\nTo understand and improve any language model application, it’s important to be able to quickly explore and organize the data the model is seeing. To achieve this, LangSmith and Lilac provide complementary capabilities:\\n\\nLangSmith: Efficiently collects, connects, and manages datasets generated by your LLM applications at scale. Use this to capture quality examples (and failure cases) and user feedback you can use for fine-tuning.\\n\\nEfficiently collects, connects, and manages datasets generated by your LLM applications at scale. Use this to capture quality examples (and failure cases) and user feedback you can use for fine-tuning. Lilac: Offers advanced analytics to structure, filter, and refine datasets, making it easy to continuously improve your data pipeline.\\n\\nWe wanted to share how to connect these two powerful tools to kickstart your fine-tuning workflows.\\n\\nFine-tuning a Q&A Chatbot\\n\\nIn the following sections, we will use LangSmith and Lilac to curate a dataset to fine-tune an LLM powering a chatbot that uses retrieval-augmented generation (RAG) to answer questions about your documentation. For our example, we will use a dataset sampled from a Q&A app for LangChain’s docs. The overall process is outlined in the image below:\\n\\nDataset Curation Pipeline with LangSmith + Lilac\\n\\nThe main steps are:\\n\\nCapture traces from the prototype and convert to a candidate dataset Import into Lilac to label, filter, and enrich. Fine-tune a model on the enriched dataset. Use the fine-tuned model in an improved application.\\n\\nCapture traces\\n\\nLangChain make it easy to design a prototype using prompt chaining. At first, the application may not be fully optimized or may run into errors when the prompt engineering is incomplete, but we can quickly create an alpha version of a feature to kickstart the dataset curation process. When building with LangChain, we can easily trace all the execution steps to LangSmith by setting a couple of environment variables.\\n\\nThen in LangSmith, we can select runs to add to a candidate dataset in the UI or programmatically (see the notebook).\\n\\nImport to Lilac\\n\\n💡 The sections below give a high level overview of the Lilac UI. For a deeper dive reproducing this workflow, see the python cookbook.\\n\\nLilac provides a native integration with LangSmith datasets. After installing Lilac locally, set the LANGCHAIN_API_KEY in the environment and you should see a list of LangSmith datasets auto-populated in the Lilac UI. Select the one you’ve earmarked for fine-tuning, and Lilac will handle the rest.\\n\\nThe “Add dataset” page in the Lilac UI with the LangSmith data loader.\\n\\nCurate your dataset\\n\\nNow that we have our dataset in Lilac, we can run Lilac’s signals, concepts and labels to help organize and filter the dataset. Our goal is to select distinct examples demonstrating good language model generations for a variety of input types. Let’s see how Lilac can help us structure our dataset.\\n\\nSignals\\n\\nRight off the bat, Lilac provides two useful signals you can apply to your dataset: Near-duplicates and PII detection. Filtering near-duplicates for inputs is important to make sure the model gets diverse information and reduce changes of memorization. To compute a signal from the UI, expand the schema in the top left corner, and select “Compute Signal” from the context menu of the field you want to enrich.\\n\\nComputing a signal via the context menu of the answer field in the Lilac schema viewer.\\n\\nConcepts\\n\\nIn addition to signals, Lilac offers concepts, a powerful way to organize the data along axes that you care about. A concept is simply a collection of positive (text that is related to the concept) and negative examples (either the opposite, or unrelated to the concept). Lilac comes with several built-in concepts, like toxicity, profanity, sentiment, etc, or you can create your own. Before we apply a concept to the dataset, we need to compute text embeddings on the field that we care about.\\n\\nComputing embeddings for the question field to enable concept and semantic search via Lilac’s search box.\\n\\nOnce we’ve computed embeddings, we can preview a concept by selecting it from the search box menu.\\n\\nSelecting profanity on the answer field for previewing.\\n\\nTo compute a concept for the entire dataset, choose “Compute concept” from the context menu in the schema viewer.\\n\\nComputing a concept via the context menu of the answer field in the schema viewer.\\n\\nIn addition to concepts, embeddings enable two other useful functionalities for exploring the data: semantic search and finding similar examples.\\n\\nSemantic search for “forget all previous instructions” via Lilac’s search box.\\n\\nFinding questions similar to “what is 1213 divided….” via the Lilac UI.\\n\\nLabels\\n\\nIn addition to automated labeling with signals and concepts, Lilac allows you to tag individual rows with custom labels that can be later used to prune your dataset.\\n\\nAdding a calculation label to an example in Lilac.\\n\\nWhen you add a new label, just like signals and concepts, it creates a new top-level column in your dataset. These can then be used to power additional analytics.\\n\\nExport the dataset\\n\\nOnce we’ve computed the information needed for filtering, you can export the enriched dataset via python, as shown in the notebook or via Lilac’s UI, which will create a browser download of a json file. We recommend the python API for downloading large amounts of data, or if you need a better control over the selection of data.\\n\\nLilac’s Download data modal dialog.\\n\\nOnce we exported the enriched dataset, we can easily filter out the examples in python using the enriched fields.\\n\\nFine-tune\\n\\nWith the dataset in hand, it’s time to fine-tune! It’s easy to convert from LangChain’s message format to the formats expected by OpenAI, HuggingFace or other training frameworks. You can check out the linked notebook for more info!\\n\\nUse in your Chain\\n\\nOnce we have the fine-tuned LLM, we can switch to it with a update to the “model” argument in our LLM.\\n\\nfrom langchain.chat_models import ChatOpenAI llm = ChatOpenAI(model=\"ft:gpt-3.5-turbo-0613:{openaiOrg}::{modelId}\")\\n\\nAssuming we’ve structured the data appropriately, this model will have more awareness for the structure and style you wish to use in generating responses.\\n\\nConclusion\\n\\nThis is a simple overview of the process for going from traces to fine-tuned model by integrating Lilac and LangSmith. With the data process in place, you can continuously improve each components in your contextual reasoning application LangSmith makes it easy to collect user and model-assisted feedback to save time when capturing data, and Lilac helps you analyze, label, and organize all the text data so you can refine your model appropriately.'),\n",
       " Document(metadata={'parent_id': '/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt', 'type': 'child'}, page_content='What Is LangSmith?\\n\\nLangSmith is a framework built on the shoulders of LangChain. It’s designed to track the inner workings of LLMs and AI agents within your product.\\n\\nThose LLM inner-workings can be categorized into 4 main buckets - each with its own flair of usefulness. Here’s a breakdown of how they all work in unison and what you can expect.\\n\\n\\n\\nDebugging:'),\n",
       " Document(metadata={'parent_id': '/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt', 'type': 'child'}, page_content='What Are LangSmith Traces?'),\n",
       " Document(metadata={'parent_id': '/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt', 'type': 'child'}, page_content='If you’re at the intersection of product, LLMs, and user experience, we’ve just walked so you can run. Time to read on.\\n\\nWhat Is LangSmith?\\n\\nLangSmith is a framework built on the shoulders of LangChain. It’s designed to track the inner workings of LLMs and AI agents within your product.'),\n",
       " Document(metadata={'parent_id': '/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt', 'type': 'child'}, page_content='But is LangSmith robust enough for us to rely on entirely for our LLM-powered QA? Or is it just another nice-to-have feature for our ENG team?\\n\\nIf you’re at the intersection of product, LLMs, and user experience, we’ve just walked so you can run. Time to read on.\\n\\nWhat Is LangSmith?')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve only parent documents\n",
    "parent_results = retriever.retrieve(query=\"what is langsmith\", retrieve_parents=True, retrieve_children=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'document_id': '/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt', 'source': '/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt', 'type': 'parent'}, page_content='URL: https://blog.langchain.dev/peering-into-the-soul-of-ai-decision-making-with-langsmith/\\nTitle: Peering Into the Soul of AI Decision-Making with LangSmith\\n\\nEditor\\'s Note: This post was written by Paul Thomson from Commandbar. They\\'ve been awesome partners as they brought their application into production with LangSmith, and we\\'re excited to share their story getting there.\\n\\nDo you ever wonder why you’re getting unhinged responses from ChatGPT sometimes? Or why the heck Midjourney is giving your creations 7 weird fingers? As intelligent as AI is supposed to be, it does produce some pretty unintelligent responses sometimes.\\n\\nNow, if you’re using GPT to write your next “let ‘em down easy breakup message”, the stakes are low - it doesn’t really matter. But if a core product feature is leveraging AI and your customers depend on super-intelligent perfection, you’re going to want some security and assurances that the outputs are up to scratch. Enter, LangSmith.\\n\\nSince the launch of HelpHub, we were trying to do things on hard mode when it came to iterating and improving functionality. That is, of course, until the LangChain team tantalized us onto their LangSmith beta. What we didn’t expect was how immediate the downstream improvements were to our flagship AI-powered product.\\n\\nBut is LangSmith robust enough for us to rely on entirely for our LLM-powered QA? Or is it just another nice-to-have feature for our ENG team?\\n\\nIf you’re at the intersection of product, LLMs, and user experience, we’ve just walked so you can run. Time to read on.\\n\\nWhat Is LangSmith?\\n\\nLangSmith is a framework built on the shoulders of LangChain. It’s designed to track the inner workings of LLMs and AI agents within your product.\\n\\nThose LLM inner-workings can be categorized into 4 main buckets - each with its own flair of usefulness. Here’s a breakdown of how they all work in unison and what you can expect.\\n\\n\\n\\nDebugging:\\n\\nWhen your LLM starts throwing curveballs instead of answers, you don\\'t just want to sit there catching them. With LangSmith, you can roll up your sleeves and play detective. We use the debugging tools to dive into perplexing agent loops, frustratingly slow chains, and to scrutinize prompts like they\\'re suspects in a lineup.\\n\\nTesting:\\n\\nTesting LLM applications without LangSmith is like trying to assemble IKEA furniture without the manual: sure, you could wing it, but do you really want to risk it? Baked into LangSmith is the option to utilize existing datasets or create new ones, and run them against your chains. Visual feedback on outputs and accuracy metrics are presented within the interface, streamlining the testing process for our eng team (we really like this).\\n\\nEvaluating:\\n\\nBeyond mere testing, evaluation in LangSmith delves into the performance nuances of LLM runs. While the built-in evaluators offer a preliminary analysis, the true power lies in guiding your focus towards crucial examples (more on how we do that later). As your datasets grow, LangSmith ensures you never miss a beat, making evaluations both comprehensive and insightful. Because \"good enough\" isn\\'t in your vocabulary, right?\\n\\nMonitoring:\\n\\nThink of LangSmith\\'s monitoring as your AI’s babysitter: always vigilant, never distracted, and ready to report every little mischief. It\\'ll give you the play-by-play, ensure everything\\'s in order, and notify you if things get out of hand. We even went a step ahead and piped these flags directly into Slack giving us almost realtime monitoring when our users hit a deadend with chat conversations.\\n\\nhttps://twitter.com/zhanghaili0610\\n\\nLangChain vs LangSmith: What’s the difference?\\n\\nWhile LangChain is the muscle doing the heavy lifting with Chains, Prompts, and Agents, understanding the \\'why\\' behind the decisions LLMs make is a maze we often found ourselves lost in. That\\'s where LangSmith shines, acting as an AI compass built into LangChain, guiding us through the intricate decision pathways and results that our chatbot generates.\\n\\n\"LangChain\\'s (the company\\'s) goal is to make it as easy as possible to develop LLM applications\"\\n\\nsaid Harrison Chase, co-founder and CEO of LangChain.\\n\\n\"To that end, we realized pretty early that what was needed - and missing - wasn\\'t just an open source tool like LangChain, but also a complementary platform for managing these new types of applications. To that end, we built LangSmith - which is usable with or without LangChain and let\\'s users easily debug, monitor, test, evaluate, and now (with the recently launched Hub) share and collaborate on their LLM applications.”\\n\\n\\n\\nWhat Are LangSmith Traces?\\n\\nTraces in the world of LangSmith are analogous to logs when programming; they allow us to easily see what text came in and out of chains and LLMs. Think of them as detailed breadcrumbs illuminating the AI\\'s journey. Each trace, like a footprint on a sandy beach, represents a pivotal AI decision. Traces don\\'t merely depict the path taken; they shed light on the underlying thought process and actions taken at each juncture.\\n\\nHere’s what one of our traces looks like inside LangSmith:\\n\\nAll the individual traces are consolidated into datasets:\\n\\n\\n\\nDo You Really Need To Use LangSmith?\\n\\nWhen generative AI works, it feels like watching a viral “satisfying video” montage - so delightful. But when it doesn’t, it sucks, and sometimes it sucks real bad.\\n\\nTake it from us, as we integrated AI more heavily and widely across our products, we’ve been more conscious than ever that the quality of the outputs matches the quality and trust that our customers have in our product. G2 review flex here.\\n\\nTruth is, until we powered up LangSmith, we truly had no way of postmorteming responses from OpenAI or testing how prompt changes, or even upgrading to a new model like GPT-4, would affect the answers.\\n\\nHow We Use LangSmith at CommandBar: AI-Powered User Assistance\\n\\nWe’ve covered a lot of ground so far on how LangSmith works. From here on out, we’re ripping the covers off and showing you what’s under the hood of our HelpHub <> LangSmith setup. To give you a little context, first, let’s dive into what HelpHub is.\\n\\nHelpHub is a GPT-Powered chatbot for any site. It syncs with any public URL or your existing help center to assist users in getting instantaneous support while circumventing customer service teams or manually screening docs.\\n\\nWhile we utilize our own search with ElasticSearch, we rely on LangChain to merge those search results into meaningful prompts for HelpHub. This synergy allows us to search hundreds of docs and sites in milliseconds, source relevant context, compute reasoning, and deliver an answer to our users (with citations!) almost instantly.\\n\\nIt’s through this core integration with LangChain that we’re able to capture traces via LangSmith. primarily to finetune and optimize our chatbot’s functionality and sandbox future improvements for our users.\\n\\nAccurate AI Responses Are A Necessity, Not A Nice-To-Have\\n\\nWe pride ourselves on offering precise and relevant answers to user queries and that has always been a strong USP for us. However, with the aforementioned challenges of unhinged AI-generated responses not always aligning with user expectations, once we flicked on LangSmith, we took our prototyping and QA from mediocre guesswork to David Blane quality witchcraft.\\n\\nSince its integration, LangSmith has traced over X0 million tokens for HelpHub (about XM tokens a week!).\\n\\nReal-world Example of LangSmith In Our Production\\n\\nBelow is an example from Gus, one of our mighty talented LangSmith connoisseurs, caught in one of our traces.\\n\\nWhat he’s referring to in the screenshot is the fact that each prompt from HelpHub should reference the source document that it’s referencing when giving users an answer. We do this primarily to legitimize the LLMs response and give our HelpHub customer’s peace of mind that their end users are in fact getting to the help resource they need (instead of an LLM just hallucinating and giving any response it wants.)\\n\\nFrom here, we went into LangSmith and saw that the LLM actually returned no source, even though we asked it to. Ideally, the source should be returned on the first line in the “Output” section above the actual answer.\\n\\nWe updated our prompt to be more firm when asking for the sources:\\n\\nPreviously the snippet in the prompt responsible for this was: Return the source ID with the most relevance to your answer.\\n\\nWe update that piece of the prompt to: ALWAYS return the source ID with the most relevance to your answer prior to answering the question .\\n\\nWe then tested everything using LangSmith evals, to make sure that it fixes the issue before pushing to production.\\n\\nou can now clearly see the citations coming through with the responses in the traces, and we’re good to ship the changes to the prompt to prod.\\n\\nThe Verdict: Are We Betting The House On LangSmith?\\n\\nWhen a product like LangSmith comes along, it can feel really natural to default to the path of least resistance and offhand all responsibility. As we start to add additional functionality to HelpHub, there’s an inherent risk that GPT is going to lead users astray, and that’s just not an option we’re willing to entertain.\\n\\nSo, in short, yes, we are putting a lot of trust right now in LangSmith and scaling our prototyping and debugging rapidly. The systems we’re building internally have already been instrumental in improving user experience, and as you’ve read earlier, many of these insights and improvements have come directly from those real-time traces from users chatting with HelpHub in the wild.\\n\\nLeveraging User Feedback For Improvements:\\n\\nWe believe that every piece of user feedback, whether positive or negative, is a goldmine of insights. And with LangSmith\\'s help (plus a little ingenuity on our side), we\\'ve turned these insights into actionable improvements.\\n\\nHere\\'s how our feedback loop works:\\n\\nReal-time Feedback Collection: As users interact with HelpHub, they have the opportunity to provide feedback on the AI-generated responses, signaling with a simple \"thumbs up\" or \"thumbs down\". In-depth Analysis with LangSmith: Instead of just collecting feedback in the form or positive or negative signals, we delve deeper (particularly for the negative signals). Using LangSmith, we’re able to attribute each signal to an individual trace. In that trace, we can map the exact sequence of steps the model took to generate that response. We essentially replay GPT\\'s thought process and LangChain’s actions, giving us the insights into what went right and where it veered off track. Categorizing Into Datasets: Central to our refinement process is LangSmith\\'s use of dynamic datasets. We maintain multiple datasets, each tailored to different query types and user interactions. These datasets are essentially compilations of identical states of our environment at the time the trace was captured. This ensures that when there\\'s an update to the prompt or LLM, a new dataset starts to compile those traces, preventing any contamination. Automating ENG-team Signals: When a user provides feedback, say a thumbs down, it\\'s immediately flagged to our team via Slack. We built this little snippet to help the team screen traces and prioritize the ones that need attention right away. Iterating Quickly: We rigorously review the feedback, analyze the corresponding traces, and then, based on our insights, make informed adjustments to the model\\'s role, prompts, or configurations to try and curb whatever jankiness was happening. This iterative process ensures our AI chatbot is continually refining its understanding, resonating more with user needs, and exceeding expectations over time.\\n\\nBy combining granular AI insights through LangSmith with user feedback, we’ve created a pretty tight loop of perpetual improvement with HelpHub. This was such an important unlock for us as we build in tactical functionality to our AI.\\n\\n\\n\\nAdvice for Product Teams Considering LangSmith\\n\\nWhen you\\'re in the thick of product development, the whirlwind of AI and LLMs can be overwhelming. We\\'ve been there, boots on the ground, making sense of it all. From our journey with LangSmith and HelpHub, here\\'s some hard-earned wisdom for fellow product teams.\\n\\nStart with Data, and Start Now:\\n\\nAI thrives on data. But don’t wait for the \\'perfect\\' moment or the \\'perfect\\' dataset. Start collecting now. Setting up LangSmith takes literally 5 minutes if you’re already using LangChain. Every bit of data, every interaction, adds a layer of understanding. But, a word to the wise: quality matters. Make sure the data reflects real-world scenarios, ensuring your AI resonates with genuine user needs.\\n\\nDive Deep with Traces: Don\\'t just skim the surface. Use LangSmith\\'s trace feature to dive deep into AI decision-making. Every trace is a lesson, a chance to improve. Experiment with Prompts: One of LangSmith\\'s standout features is its ability to test a new prompt across multiple examples without manual entry each time. This makes it incredibly efficient to iterate on your setup, ensuring you get the desired output from the AI. Note, in addition, the Playground is also an amazing tool to dig around with for testing prompts and adjustments to traces too. Lean on the Community: There\\'s a whole community of LangSmith users out there. Swap stories, share challenges, and celebrate successes. You\\'re not alone on this journey. Stay on Your Toes: AI doesn’t stand still, and neither should you. Keep an eye on LangSmith\\'s updates. New features? Dive in. Test, iterate, refine.\\n\\nConclusion\\n\\nAfter diving deep with LangSmith\\'s traces, experimenting with prompts, testing, and iterating on our LLM environment, here\\'s the real talk: LangSmith isn\\'t just a tool for us - it\\'s become a critical inclusion in our stack. We\\'ve moved from crossing our fingers and toes hoping our AI works to knowing exactly how and why.\\n\\nSo, to our fellow AI product people trailblazers, dive into LangSmith. You’d be silly not to if you’re already using LangChain!'),\n",
       " Document(metadata={'document_id': '/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_announcing-langsmith_.txt', 'source': '/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_announcing-langsmith_.txt', 'type': 'parent'}, page_content='URL: https://blog.langchain.dev/announcing-langsmith/\\nTitle: Announcing LangSmith, a unified platform for debugging, testing, evaluating, and monitoring your LLM applications\\n\\nLangChain exists to make it as easy as possible to develop LLM-powered applications.\\n\\nWe started with an open-source Python package when the main blocker for building LLM-powered applications was getting a simple prototype working. We remember seeing Nat Friedman tweet in late 2022 that there was “not enough tinkering happening.” The LangChain open-source packages are aimed at addressing this and we see lots of tinkering happening now (Nat agrees)–people are building everything from chatbots over internal company documents to an AI dungeon master for a Dungeons and Dragons game.\\n\\nThe blocker has now changed. While it’s easy to build a prototype of an application in ~5 lines of LangChain code, it’s still deceptively hard to take an application from prototype to production. The main issue that we see today is application performance–something that works ~30% of the time is good enough for a Twitter demo, but not nearly good enough for production.\\n\\nToday, we’re introducing LangSmith, a platform to help developers close the gap between prototype and production. It’s designed for building and iterating on products that can harness the power–and wrangle the complexity–of LLMs.\\n\\nLangSmith is now in closed beta. So if you’re looking for a robust, unified, system for debugging, testing, evaluating, and monitoring your LLM applications, sign up here.\\n\\n\\n\\nHow did we get here?\\n\\nGiven the stochastic nature of LLMs, it is not easy–and there’s currently no straightforward way–to answer the simple question of “what’s happening in these models?,” let alone getting them to work reliably. The builders we hear from are running into the same roadblocks (and it’s true for our team, too):\\n\\nUnderstanding what exactly the final prompt to the LLM call is (after all the prompt template formatting, this final prompt can be long and obfuscated)\\n\\nUnderstanding what exactly is returned from the LLM call at each step (before it is post-processed or transformed in any way)\\n\\nUnderstanding the exact sequence of calls to LLM (or other resources), and how they are chained together\\n\\nTracking token usage\\n\\nManaging costs\\n\\nTracking (and debugging) latency\\n\\nNot having a good dataset to evaluate their application over\\n\\nNot having good metrics with which to evaluate their application\\n\\nUnderstanding how users are interacting with the product\\n\\nAll of these problems have parallels in traditional software engineering. And, in response, a set of practices and tools for debugging, testing, logging, monitoring, etc. has emerged to help developers abstract away common infrastructure and focus on what really matters - building their applications. LLM application developers deserve the same.\\n\\nLangSmith aspires to be that platform. Over the last few months, we’ve been working directly with some early design partners and testing it on our own internal workflows, and we’ve found LangSmith helps teams in 5 core ways:\\n\\nDebugging\\n\\nLangSmith gives you full visibility into model inputs and output of every step in the chain of events. This makes it easy for teams to experiment with new chains and prompt templates, and spot the source of unexpected results, errors, or latency issues. We’ll also expose latency and token usage so that you can identify which calls are causing issues.\\n\\nWe’ve also made it easy to change and rerun examples from the UI. We added this feature after seeing teams take logs of bad examples and copy-paste into the OpenAI playground to tweak the prompt until they got a good result. We wanted to eliminate that friction, and now with the click of a button, you can go from a log to a playground where you can actively edit. This is currently supported for both OpenAI and Anthropic models, with support for more coming soon. We’re also working on supporting this for chains in general.\\n\\nThis deep visibility into model performance has been particularly helpful for teams developing complex applications. LangSmith helped Streamlit and Snowflake implement agents that could intelligently and reliably answer questions about their data.\\n\\n\"LangChain has been instrumental in helping us prototype intelligent agents at Snowflake,” said Adrien Treuille, Director of Product at Snowflake. “LangSmith was easy to integrate, and the agnostic open source API made it very flexible to adapt to our implementation,” tacked on Richard Meng, Senior Software Engineer at Snowflake.\\n\\nBoston Consulting Group also built a highly-customized, and highly performant, series of applications on top of LangChain’s framework by relying on this same infrastructure.\\n\\n“We are proud of being one of the early LangChain design partners and users of LangSmith,” Said Dan Sack, Managing Director and Partner at BCG. “The use of LangSmith has been key to bringing production-ready LLM applications to our clients. LangSmith\\'s ease of integration and intuitive UI enabled us to have an evaluation pipeline up and running very quickly. Additionally, tracing and evaluating the complex agent prompt chains is much easier, reducing the time required to debug and refine our prompts, and giving us the confidence to move to deployment.”\\n\\nIn another example of debugging in action, we partnered with DeepLearningAI to equip learners in the recently-released LangChain courses with access to LangSmith. This allowed students to easily visualize the exact sequence of calls, and the inputs and outputs at each step in the chain with precision. Students can understand exactly what the chains, prompts, and LLMs were doing, which helps build intuition as they learn to create new and more sophisticated applications.\\n\\n\\n\\nTesting\\n\\nOne of the main questions we see developers grapple with is: “If I change this chain/prompt, how does that affect my outputs?” The most effective way to answer this question is to curate a dataset of examples that you care about, and then run any changed prompts/chains over this dataset. LangSmith first makes it easy to create these datasets from traces or by uploading datasets you’ve curated manually. You can then easily run chains and prompts over those data sets.\\n\\nThe first helpful step is simply manually looking at the new inputs and outputs. Although this may seem unsatisfyingly basic, it actually has some benefits - many of the companies we’ve spoken to actually like some manual touch points because it allows them to develop better intuition about how to interact with LLMs. This intuition can prove incredibly valuable when trying to think about how to improve the application. The main unlock we hope to provide is a clear interface for letting developers easily see the inputs and outputs for each data point, as without that visibility they cannot build up that intuition.\\n\\nToday, we primarily hear from teams that want to bring their prototype into production, and are narrowing in on specific prompts they’d like to improve. Klarna is building industry-leading AI integrations that go beyond a simple call to a language model, and instead rely on a series of calls. As they focus on a specific section, LangSmith has provided the tools and data they need to ensure no regressions occur.\\n\\nIn parallel, we’re starting to hear from more and more ambitious teams that are striving for a more effective approach.\\n\\nEvaluating\\n\\nLangSmith integrates seamlessly with our open source collection of evaluation modules. These modules have two main types of evaluation: heuristics and LLMs. Heuristic Evaluations will use logic like regexes to evaluate the correctness of an answer. LLM evaluations will use LLMs to evaluate themselves.\\n\\nWe are extremely bullish on LLM assisted evaluation over the long term. Critics of this approach will say that it’s conceptually shaky and practically costly (time and money). But, we’ve been seeing some very compelling evidence come out of top labs that this is a viable strategy. And, as we collectively make improvements to these models–both private and open source–and usage becomes more ubiquitous, we expect costs to come down considerably.\\n\\nMonitoring\\n\\nWhile debugging, testing, and evaluating can help you get from prototype to production, the work doesn’t stop once you ship. Developers need to actively track performance, and ideally, optimize that performance based on feedback. We consistently see developers relying on LangSmith to track the system-level performance of their application (like latency and cost), track the model/chain performance (through associating feedback with runs), debug issues (diving into a particular run that went wrong), and establish a broad understanding of how users are interacting with their application and what their experience is like.\\n\\nAmbitious startups like Mendable, Multi-On and Quivr, who are already serving thousands of users are actively using LangSmith to not only monitor overall usage, but also use those insights to take action on critical issues.\\n\\n“Thanks to Langchain smith we were able to analyze our LLM calls, understand the performance of the different chain methods ( stuff vs reduce) for QA and improve upon it. It even helped us debug and understand errors we made. We are consistently using it to improve our prompt engineering and look forward to the new features,” said Stan Girard, Head of GenAI at Theodo and creator of Quivr.\\n\\n\\n\\nA unified platform\\n\\nWhile each of these product areas provide unique value, often at a specific point in time in the development process, we believe a great deal of the long term impact of LangSmith will come from having a single, fully-integrated hub to do this work from. We see teams with all kinds of Rube Goldberg-machine-like processes for managing their LLM applications, and we want to make that a thing of the past.\\n\\nAs a very simple example, we considered it to be table stakes for LangSmith to help users easily create datasets from existing logs and use them immediately for testing and evaluation, seamlessly connecting the logging/debugging workflows to the testing/evaluation ones.\\n\\n\\n\\nFintual, a Latin American startup with big dreams to help their citizens build wealth through a personalized financial advisor, found LangSmith early in their LLM development journey. “As soon as we heard about LangSmith, we moved our entire development stack onto it. We could build the evaluation, testing, and monitoring tools we needed in-house. But it would be 1000x worse, take us 10x longer, and require a team 2x the size,” said Fintual leader Jose Pena.\\n\\n“Because we are building financial products, the bar for accuracy, personalization, and security is particularly high. LangSmith helps us build products we are confident putting in front of users.”\\n\\nWe can’t wait to bring these benefits to more teams. And we’ve got a long list of features on the roadmap like analytics, playgrounds, collaboration, in-context learning, prompt creation, and more.\\n\\nFinally, we recognize that we cannot build ALL the functionality you will need to make it easy to make your applications production ready today. We’ve made it possible to export datasets in the format OpenAI evals expects so you can contribute them there. This data can be also used directly to fine tune your models on the Fireworks platform (and we aim to make it easy to plug into other fine-tuning systems as well). Finally, we’ve made logs exportable in a generic format and worked with teams like Context to ensure that you can load them into their analytics engine and run analytics over them in there.\\n\\n\\n\\nWe can’t wait to see what you build.'),\n",
       " Document(metadata={'document_id': '/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_announcing-langsmith_?ref=commandbar.ghost.io.txt', 'source': '/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_announcing-langsmith_?ref=commandbar.ghost.io.txt', 'type': 'parent'}, page_content='URL: https://blog.langchain.dev/announcing-langsmith/?ref=commandbar.ghost.io\\nTitle: Announcing LangSmith, a unified platform for debugging, testing, evaluating, and monitoring your LLM applications\\n\\nLangChain exists to make it as easy as possible to develop LLM-powered applications.\\n\\nWe started with an open-source Python package when the main blocker for building LLM-powered applications was getting a simple prototype working. We remember seeing Nat Friedman tweet in late 2022 that there was “not enough tinkering happening.” The LangChain open-source packages are aimed at addressing this and we see lots of tinkering happening now (Nat agrees)–people are building everything from chatbots over internal company documents to an AI dungeon master for a Dungeons and Dragons game.\\n\\nThe blocker has now changed. While it’s easy to build a prototype of an application in ~5 lines of LangChain code, it’s still deceptively hard to take an application from prototype to production. The main issue that we see today is application performance–something that works ~30% of the time is good enough for a Twitter demo, but not nearly good enough for production.\\n\\nToday, we’re introducing LangSmith, a platform to help developers close the gap between prototype and production. It’s designed for building and iterating on products that can harness the power–and wrangle the complexity–of LLMs.\\n\\nLangSmith is now in closed beta. So if you’re looking for a robust, unified, system for debugging, testing, evaluating, and monitoring your LLM applications, sign up here.\\n\\n\\n\\nHow did we get here?\\n\\nGiven the stochastic nature of LLMs, it is not easy–and there’s currently no straightforward way–to answer the simple question of “what’s happening in these models?,” let alone getting them to work reliably. The builders we hear from are running into the same roadblocks (and it’s true for our team, too):\\n\\nUnderstanding what exactly the final prompt to the LLM call is (after all the prompt template formatting, this final prompt can be long and obfuscated)\\n\\nUnderstanding what exactly is returned from the LLM call at each step (before it is post-processed or transformed in any way)\\n\\nUnderstanding the exact sequence of calls to LLM (or other resources), and how they are chained together\\n\\nTracking token usage\\n\\nManaging costs\\n\\nTracking (and debugging) latency\\n\\nNot having a good dataset to evaluate their application over\\n\\nNot having good metrics with which to evaluate their application\\n\\nUnderstanding how users are interacting with the product\\n\\nAll of these problems have parallels in traditional software engineering. And, in response, a set of practices and tools for debugging, testing, logging, monitoring, etc. has emerged to help developers abstract away common infrastructure and focus on what really matters - building their applications. LLM application developers deserve the same.\\n\\nLangSmith aspires to be that platform. Over the last few months, we’ve been working directly with some early design partners and testing it on our own internal workflows, and we’ve found LangSmith helps teams in 5 core ways:\\n\\nDebugging\\n\\nLangSmith gives you full visibility into model inputs and output of every step in the chain of events. This makes it easy for teams to experiment with new chains and prompt templates, and spot the source of unexpected results, errors, or latency issues. We’ll also expose latency and token usage so that you can identify which calls are causing issues.\\n\\nWe’ve also made it easy to change and rerun examples from the UI. We added this feature after seeing teams take logs of bad examples and copy-paste into the OpenAI playground to tweak the prompt until they got a good result. We wanted to eliminate that friction, and now with the click of a button, you can go from a log to a playground where you can actively edit. This is currently supported for both OpenAI and Anthropic models, with support for more coming soon. We’re also working on supporting this for chains in general.\\n\\nThis deep visibility into model performance has been particularly helpful for teams developing complex applications. LangSmith helped Streamlit and Snowflake implement agents that could intelligently and reliably answer questions about their data.\\n\\n\"LangChain has been instrumental in helping us prototype intelligent agents at Snowflake,” said Adrien Treuille, Director of Product at Snowflake. “LangSmith was easy to integrate, and the agnostic open source API made it very flexible to adapt to our implementation,” tacked on Richard Meng, Senior Software Engineer at Snowflake.\\n\\nBoston Consulting Group also built a highly-customized, and highly performant, series of applications on top of LangChain’s framework by relying on this same infrastructure.\\n\\n“We are proud of being one of the early LangChain design partners and users of LangSmith,” Said Dan Sack, Managing Director and Partner at BCG. “The use of LangSmith has been key to bringing production-ready LLM applications to our clients. LangSmith\\'s ease of integration and intuitive UI enabled us to have an evaluation pipeline up and running very quickly. Additionally, tracing and evaluating the complex agent prompt chains is much easier, reducing the time required to debug and refine our prompts, and giving us the confidence to move to deployment.”\\n\\nIn another example of debugging in action, we partnered with DeepLearningAI to equip learners in the recently-released LangChain courses with access to LangSmith. This allowed students to easily visualize the exact sequence of calls, and the inputs and outputs at each step in the chain with precision. Students can understand exactly what the chains, prompts, and LLMs were doing, which helps build intuition as they learn to create new and more sophisticated applications.\\n\\n\\n\\nTesting\\n\\nOne of the main questions we see developers grapple with is: “If I change this chain/prompt, how does that affect my outputs?” The most effective way to answer this question is to curate a dataset of examples that you care about, and then run any changed prompts/chains over this dataset. LangSmith first makes it easy to create these datasets from traces or by uploading datasets you’ve curated manually. You can then easily run chains and prompts over those data sets.\\n\\nThe first helpful step is simply manually looking at the new inputs and outputs. Although this may seem unsatisfyingly basic, it actually has some benefits - many of the companies we’ve spoken to actually like some manual touch points because it allows them to develop better intuition about how to interact with LLMs. This intuition can prove incredibly valuable when trying to think about how to improve the application. The main unlock we hope to provide is a clear interface for letting developers easily see the inputs and outputs for each data point, as without that visibility they cannot build up that intuition.\\n\\nToday, we primarily hear from teams that want to bring their prototype into production, and are narrowing in on specific prompts they’d like to improve. Klarna is building industry-leading AI integrations that go beyond a simple call to a language model, and instead rely on a series of calls. As they focus on a specific section, LangSmith has provided the tools and data they need to ensure no regressions occur.\\n\\nIn parallel, we’re starting to hear from more and more ambitious teams that are striving for a more effective approach.\\n\\nEvaluating\\n\\nLangSmith integrates seamlessly with our open source collection of evaluation modules. These modules have two main types of evaluation: heuristics and LLMs. Heuristic Evaluations will use logic like regexes to evaluate the correctness of an answer. LLM evaluations will use LLMs to evaluate themselves.\\n\\nWe are extremely bullish on LLM assisted evaluation over the long term. Critics of this approach will say that it’s conceptually shaky and practically costly (time and money). But, we’ve been seeing some very compelling evidence come out of top labs that this is a viable strategy. And, as we collectively make improvements to these models–both private and open source–and usage becomes more ubiquitous, we expect costs to come down considerably.\\n\\nMonitoring\\n\\nWhile debugging, testing, and evaluating can help you get from prototype to production, the work doesn’t stop once you ship. Developers need to actively track performance, and ideally, optimize that performance based on feedback. We consistently see developers relying on LangSmith to track the system-level performance of their application (like latency and cost), track the model/chain performance (through associating feedback with runs), debug issues (diving into a particular run that went wrong), and establish a broad understanding of how users are interacting with their application and what their experience is like.\\n\\nAmbitious startups like Mendable, Multi-On and Quivr, who are already serving thousands of users are actively using LangSmith to not only monitor overall usage, but also use those insights to take action on critical issues.\\n\\n“Thanks to Langchain smith we were able to analyze our LLM calls, understand the performance of the different chain methods ( stuff vs reduce) for QA and improve upon it. It even helped us debug and understand errors we made. We are consistently using it to improve our prompt engineering and look forward to the new features,” said Stan Girard, Head of GenAI at Theodo and creator of Quivr.\\n\\n\\n\\nA unified platform\\n\\nWhile each of these product areas provide unique value, often at a specific point in time in the development process, we believe a great deal of the long term impact of LangSmith will come from having a single, fully-integrated hub to do this work from. We see teams with all kinds of Rube Goldberg-machine-like processes for managing their LLM applications, and we want to make that a thing of the past.\\n\\nAs a very simple example, we considered it to be table stakes for LangSmith to help users easily create datasets from existing logs and use them immediately for testing and evaluation, seamlessly connecting the logging/debugging workflows to the testing/evaluation ones.\\n\\n\\n\\nFintual, a Latin American startup with big dreams to help their citizens build wealth through a personalized financial advisor, found LangSmith early in their LLM development journey. “As soon as we heard about LangSmith, we moved our entire development stack onto it. We could build the evaluation, testing, and monitoring tools we needed in-house. But it would be 1000x worse, take us 10x longer, and require a team 2x the size,” said Fintual leader Jose Pena.\\n\\n“Because we are building financial products, the bar for accuracy, personalization, and security is particularly high. LangSmith helps us build products we are confident putting in front of users.”\\n\\nWe can’t wait to bring these benefits to more teams. And we’ve got a long list of features on the roadmap like analytics, playgrounds, collaboration, in-context learning, prompt creation, and more.\\n\\nFinally, we recognize that we cannot build ALL the functionality you will need to make it easy to make your applications production ready today. We’ve made it possible to export datasets in the format OpenAI evals expects so you can contribute them there. This data can be also used directly to fine tune your models on the Fireworks platform (and we aim to make it easy to plug into other fine-tuning systems as well). Finally, we’ve made logs exportable in a generic format and worked with teams like Context to ensure that you can load them into their analytics engine and run analytics over them in there.\\n\\n\\n\\nWe can’t wait to see what you build.'),\n",
       " Document(metadata={'document_id': '/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_fine-tune-your-llms-with-langsmith-and-lilac_.txt', 'source': '/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_fine-tune-your-llms-with-langsmith-and-lilac_.txt', 'type': 'parent'}, page_content='URL: https://blog.langchain.dev/fine-tune-your-llms-with-langsmith-and-lilac/\\nTitle: Fine-tune your LLMs with LangSmith and Lilac\\n\\nIn taking your LLM from prototype into production, many have turned to fine-tuning models to get more consistent and high-quality behavior in their applications. Services like OpenAI and HuggingFace make it easy to fine-tune a model on your application-specific data. All it takes is a JSON file!\\n\\nThe tricky part is deciding what to include in that data. Once your LLM is deployed, it could be prompted given any input - how do you make sure it will respond appropriately for the user or machine it is meant to interact with?\\n\\nFor this, there is no real substitute for high-quality data taken from your unique application context. This is where LangSmith and Lilac can help out.\\n\\nLangSmith + Lilac\\n\\nTo understand and improve any language model application, it’s important to be able to quickly explore and organize the data the model is seeing. To achieve this, LangSmith and Lilac provide complementary capabilities:\\n\\nLangSmith: Efficiently collects, connects, and manages datasets generated by your LLM applications at scale. Use this to capture quality examples (and failure cases) and user feedback you can use for fine-tuning.\\n\\nEfficiently collects, connects, and manages datasets generated by your LLM applications at scale. Use this to capture quality examples (and failure cases) and user feedback you can use for fine-tuning. Lilac: Offers advanced analytics to structure, filter, and refine datasets, making it easy to continuously improve your data pipeline.\\n\\nWe wanted to share how to connect these two powerful tools to kickstart your fine-tuning workflows.\\n\\nFine-tuning a Q&A Chatbot\\n\\nIn the following sections, we will use LangSmith and Lilac to curate a dataset to fine-tune an LLM powering a chatbot that uses retrieval-augmented generation (RAG) to answer questions about your documentation. For our example, we will use a dataset sampled from a Q&A app for LangChain’s docs. The overall process is outlined in the image below:\\n\\nDataset Curation Pipeline with LangSmith + Lilac\\n\\nThe main steps are:\\n\\nCapture traces from the prototype and convert to a candidate dataset Import into Lilac to label, filter, and enrich. Fine-tune a model on the enriched dataset. Use the fine-tuned model in an improved application.\\n\\nCapture traces\\n\\nLangChain make it easy to design a prototype using prompt chaining. At first, the application may not be fully optimized or may run into errors when the prompt engineering is incomplete, but we can quickly create an alpha version of a feature to kickstart the dataset curation process. When building with LangChain, we can easily trace all the execution steps to LangSmith by setting a couple of environment variables.\\n\\nThen in LangSmith, we can select runs to add to a candidate dataset in the UI or programmatically (see the notebook).\\n\\nImport to Lilac\\n\\n💡 The sections below give a high level overview of the Lilac UI. For a deeper dive reproducing this workflow, see the python cookbook.\\n\\nLilac provides a native integration with LangSmith datasets. After installing Lilac locally, set the LANGCHAIN_API_KEY in the environment and you should see a list of LangSmith datasets auto-populated in the Lilac UI. Select the one you’ve earmarked for fine-tuning, and Lilac will handle the rest.\\n\\nThe “Add dataset” page in the Lilac UI with the LangSmith data loader.\\n\\nCurate your dataset\\n\\nNow that we have our dataset in Lilac, we can run Lilac’s signals, concepts and labels to help organize and filter the dataset. Our goal is to select distinct examples demonstrating good language model generations for a variety of input types. Let’s see how Lilac can help us structure our dataset.\\n\\nSignals\\n\\nRight off the bat, Lilac provides two useful signals you can apply to your dataset: Near-duplicates and PII detection. Filtering near-duplicates for inputs is important to make sure the model gets diverse information and reduce changes of memorization. To compute a signal from the UI, expand the schema in the top left corner, and select “Compute Signal” from the context menu of the field you want to enrich.\\n\\nComputing a signal via the context menu of the answer field in the Lilac schema viewer.\\n\\nConcepts\\n\\nIn addition to signals, Lilac offers concepts, a powerful way to organize the data along axes that you care about. A concept is simply a collection of positive (text that is related to the concept) and negative examples (either the opposite, or unrelated to the concept). Lilac comes with several built-in concepts, like toxicity, profanity, sentiment, etc, or you can create your own. Before we apply a concept to the dataset, we need to compute text embeddings on the field that we care about.\\n\\nComputing embeddings for the question field to enable concept and semantic search via Lilac’s search box.\\n\\nOnce we’ve computed embeddings, we can preview a concept by selecting it from the search box menu.\\n\\nSelecting profanity on the answer field for previewing.\\n\\nTo compute a concept for the entire dataset, choose “Compute concept” from the context menu in the schema viewer.\\n\\nComputing a concept via the context menu of the answer field in the schema viewer.\\n\\nIn addition to concepts, embeddings enable two other useful functionalities for exploring the data: semantic search and finding similar examples.\\n\\nSemantic search for “forget all previous instructions” via Lilac’s search box.\\n\\nFinding questions similar to “what is 1213 divided….” via the Lilac UI.\\n\\nLabels\\n\\nIn addition to automated labeling with signals and concepts, Lilac allows you to tag individual rows with custom labels that can be later used to prune your dataset.\\n\\nAdding a calculation label to an example in Lilac.\\n\\nWhen you add a new label, just like signals and concepts, it creates a new top-level column in your dataset. These can then be used to power additional analytics.\\n\\nExport the dataset\\n\\nOnce we’ve computed the information needed for filtering, you can export the enriched dataset via python, as shown in the notebook or via Lilac’s UI, which will create a browser download of a json file. We recommend the python API for downloading large amounts of data, or if you need a better control over the selection of data.\\n\\nLilac’s Download data modal dialog.\\n\\nOnce we exported the enriched dataset, we can easily filter out the examples in python using the enriched fields.\\n\\nFine-tune\\n\\nWith the dataset in hand, it’s time to fine-tune! It’s easy to convert from LangChain’s message format to the formats expected by OpenAI, HuggingFace or other training frameworks. You can check out the linked notebook for more info!\\n\\nUse in your Chain\\n\\nOnce we have the fine-tuned LLM, we can switch to it with a update to the “model” argument in our LLM.\\n\\nfrom langchain.chat_models import ChatOpenAI llm = ChatOpenAI(model=\"ft:gpt-3.5-turbo-0613:{openaiOrg}::{modelId}\")\\n\\nAssuming we’ve structured the data appropriately, this model will have more awareness for the structure and style you wish to use in generating responses.\\n\\nConclusion\\n\\nThis is a simple overview of the process for going from traces to fine-tuned model by integrating Lilac and LangSmith. With the data process in place, you can continuously improve each components in your contextual reasoning application LangSmith makes it easy to collect user and model-assisted feedback to save time when capturing data, and Lilac helps you analyze, label, and organize all the text data so you can refine your model appropriately.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve only child documents\n",
    "child_results = retriever.retrieve(query=\"what is langsmith\", retrieve_parents=False, retrieve_children=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'parent_id': '/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt', 'type': 'child'}, page_content='What Is LangSmith?\\n\\nLangSmith is a framework built on the shoulders of LangChain. It’s designed to track the inner workings of LLMs and AI agents within your product.\\n\\nThose LLM inner-workings can be categorized into 4 main buckets - each with its own flair of usefulness. Here’s a breakdown of how they all work in unison and what you can expect.\\n\\n\\n\\nDebugging:'),\n",
       " Document(metadata={'parent_id': '/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt', 'type': 'child'}, page_content='What Are LangSmith Traces?'),\n",
       " Document(metadata={'parent_id': '/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt', 'type': 'child'}, page_content='If you’re at the intersection of product, LLMs, and user experience, we’ve just walked so you can run. Time to read on.\\n\\nWhat Is LangSmith?\\n\\nLangSmith is a framework built on the shoulders of LangChain. It’s designed to track the inner workings of LLMs and AI agents within your product.'),\n",
       " Document(metadata={'parent_id': '/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt', 'type': 'child'}, page_content='But is LangSmith robust enough for us to rely on entirely for our LLM-powered QA? Or is it just another nice-to-have feature for our ENG team?\\n\\nIf you’re at the intersection of product, LLMs, and user experience, we’ve just walked so you can run. Time to read on.\\n\\nWhat Is LangSmith?')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "child_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If you start with a search that returns information from child chunks and then want to retrieve more comprehensive information (e.g., the entire parent document or additional related chunks), you can adjust your retrieval process as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt\n",
      "/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt\n",
      "/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt\n",
      "/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt\n"
     ]
    }
   ],
   "source": [
    "# Assuming initial_results is a list of documents or chunks below is top 4 child documents.\n",
    "for result in child_results:\n",
    "    parent_id = result.metadata.get('parent_id')\n",
    "    print(parent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://blog.langchain.dev/peering-into-the-soul-of-ai-decision-making-with-langsmith/\n",
      "Title: Peering Into the Soul of AI Decision-Making with LangSmith\n",
      "\n",
      "Editor's Note: This post was written by Paul Thomson from Commandbar. They've been awesome partners as they brought their application into production with LangSmith, and we're excited to share their story getting there.\n",
      "\n",
      "Do you ever wonder why you’re getting unhinged responses from ChatGPT sometimes? Or why the heck Midjourney is giving your creations 7 weird fingers? As intelligent as AI is supposed to be, it does produce some pretty unintelligent responses sometimes.\n",
      "\n",
      "Now, if you’re using GPT to write your next “let ‘em down easy breakup message”, the stakes are low - it doesn’t really matter. But if a core product feature is leveraging AI and your customers depend on super-intelligent perfection, you’re going to want some security and assurances that the outputs are up to scratch. Enter, LangSmith.\n",
      "\n",
      "Since the launch of HelpHub, we were trying to do things on hard mode when it came to iterating and improving functionality. That is, of course, until the LangChain team tantalized us onto their LangSmith beta. What we didn’t expect was how immediate the downstream improvements were to our flagship AI-powered product.\n",
      "\n",
      "But is LangSmith robust enough for us to rely on entirely for our LLM-powered QA? Or is it just another nice-to-have feature for our ENG team?\n",
      "\n",
      "If you’re at the intersection of product, LLMs, and user experience, we’ve just walked so you can run. Time to read on.\n",
      "\n",
      "What Is LangSmith?\n",
      "\n",
      "LangSmith is a framework built on the shoulders of LangChain. It’s designed to track the inner workings of LLMs and AI agents within your product.\n",
      "\n",
      "Those LLM inner-workings can be categorized into 4 main buckets - each with its own flair of usefulness. Here’s a breakdown of how they all work in unison and what you can expect.\n",
      "\n",
      "\n",
      "\n",
      "Debugging:\n",
      "\n",
      "When your LLM starts throwing curveballs instead of answers, you don't just want to sit there catching them. With LangSmith, you can roll up your sleeves and play detective. We use the debugging tools to dive into perplexing agent loops, frustratingly slow chains, and to scrutinize prompts like they're suspects in a lineup.\n",
      "\n",
      "Testing:\n",
      "\n",
      "Testing LLM applications without LangSmith is like trying to assemble IKEA furniture without the manual: sure, you could wing it, but do you really want to risk it? Baked into LangSmith is the option to utilize existing datasets or create new ones, and run them against your chains. Visual feedback on outputs and accuracy metrics are presented within the interface, streamlining the testing process for our eng team (we really like this).\n",
      "\n",
      "Evaluating:\n",
      "\n",
      "Beyond mere testing, evaluation in LangSmith delves into the performance nuances of LLM runs. While the built-in evaluators offer a preliminary analysis, the true power lies in guiding your focus towards crucial examples (more on how we do that later). As your datasets grow, LangSmith ensures you never miss a beat, making evaluations both comprehensive and insightful. Because \"good enough\" isn't in your vocabulary, right?\n",
      "\n",
      "Monitoring:\n",
      "\n",
      "Think of LangSmith's monitoring as your AI’s babysitter: always vigilant, never distracted, and ready to report every little mischief. It'll give you the play-by-play, ensure everything's in order, and notify you if things get out of hand. We even went a step ahead and piped these flags directly into Slack giving us almost realtime monitoring when our users hit a deadend with chat conversations.\n",
      "\n",
      "https://twitter.com/zhanghaili0610\n",
      "\n",
      "LangChain vs LangSmith: What’s the difference?\n",
      "\n",
      "While LangChain is the muscle doing the heavy lifting with Chains, Prompts, and Agents, understanding the 'why' behind the decisions LLMs make is a maze we often found ourselves lost in. That's where LangSmith shines, acting as an AI compass built into LangChain, guiding us through the intricate decision pathways and results that our chatbot generates.\n",
      "\n",
      "\"LangChain's (the company's) goal is to make it as easy as possible to develop LLM applications\"\n",
      "\n",
      "said Harrison Chase, co-founder and CEO of LangChain.\n",
      "\n",
      "\"To that end, we realized pretty early that what was needed - and missing - wasn't just an open source tool like LangChain, but also a complementary platform for managing these new types of applications. To that end, we built LangSmith - which is usable with or without LangChain and let's users easily debug, monitor, test, evaluate, and now (with the recently launched Hub) share and collaborate on their LLM applications.”\n",
      "\n",
      "\n",
      "\n",
      "What Are LangSmith Traces?\n",
      "\n",
      "Traces in the world of LangSmith are analogous to logs when programming; they allow us to easily see what text came in and out of chains and LLMs. Think of them as detailed breadcrumbs illuminating the AI's journey. Each trace, like a footprint on a sandy beach, represents a pivotal AI decision. Traces don't merely depict the path taken; they shed light on the underlying thought process and actions taken at each juncture.\n",
      "\n",
      "Here’s what one of our traces looks like inside LangSmith:\n",
      "\n",
      "All the individual traces are consolidated into datasets:\n",
      "\n",
      "\n",
      "\n",
      "Do You Really Need To Use LangSmith?\n",
      "\n",
      "When generative AI works, it feels like watching a viral “satisfying video” montage - so delightful. But when it doesn’t, it sucks, and sometimes it sucks real bad.\n",
      "\n",
      "Take it from us, as we integrated AI more heavily and widely across our products, we’ve been more conscious than ever that the quality of the outputs matches the quality and trust that our customers have in our product. G2 review flex here.\n",
      "\n",
      "Truth is, until we powered up LangSmith, we truly had no way of postmorteming responses from OpenAI or testing how prompt changes, or even upgrading to a new model like GPT-4, would affect the answers.\n",
      "\n",
      "How We Use LangSmith at CommandBar: AI-Powered User Assistance\n",
      "\n",
      "We’ve covered a lot of ground so far on how LangSmith works. From here on out, we’re ripping the covers off and showing you what’s under the hood of our HelpHub <> LangSmith setup. To give you a little context, first, let’s dive into what HelpHub is.\n",
      "\n",
      "HelpHub is a GPT-Powered chatbot for any site. It syncs with any public URL or your existing help center to assist users in getting instantaneous support while circumventing customer service teams or manually screening docs.\n",
      "\n",
      "While we utilize our own search with ElasticSearch, we rely on LangChain to merge those search results into meaningful prompts for HelpHub. This synergy allows us to search hundreds of docs and sites in milliseconds, source relevant context, compute reasoning, and deliver an answer to our users (with citations!) almost instantly.\n",
      "\n",
      "It’s through this core integration with LangChain that we’re able to capture traces via LangSmith. primarily to finetune and optimize our chatbot’s functionality and sandbox future improvements for our users.\n",
      "\n",
      "Accurate AI Responses Are A Necessity, Not A Nice-To-Have\n",
      "\n",
      "We pride ourselves on offering precise and relevant answers to user queries and that has always been a strong USP for us. However, with the aforementioned challenges of unhinged AI-generated responses not always aligning with user expectations, once we flicked on LangSmith, we took our prototyping and QA from mediocre guesswork to David Blane quality witchcraft.\n",
      "\n",
      "Since its integration, LangSmith has traced over X0 million tokens for HelpHub (about XM tokens a week!).\n",
      "\n",
      "Real-world Example of LangSmith In Our Production\n",
      "\n",
      "Below is an example from Gus, one of our mighty talented LangSmith connoisseurs, caught in one of our traces.\n",
      "\n",
      "What he’s referring to in the screenshot is the fact that each prompt from HelpHub should reference the source document that it’s referencing when giving users an answer. We do this primarily to legitimize the LLMs response and give our HelpHub customer’s peace of mind that their end users are in fact getting to the help resource they need (instead of an LLM just hallucinating and giving any response it wants.)\n",
      "\n",
      "From here, we went into LangSmith and saw that the LLM actually returned no source, even though we asked it to. Ideally, the source should be returned on the first line in the “Output” section above the actual answer.\n",
      "\n",
      "We updated our prompt to be more firm when asking for the sources:\n",
      "\n",
      "Previously the snippet in the prompt responsible for this was: Return the source ID with the most relevance to your answer.\n",
      "\n",
      "We update that piece of the prompt to: ALWAYS return the source ID with the most relevance to your answer prior to answering the question .\n",
      "\n",
      "We then tested everything using LangSmith evals, to make sure that it fixes the issue before pushing to production.\n",
      "\n",
      "ou can now clearly see the citations coming through with the responses in the traces, and we’re good to ship the changes to the prompt to prod.\n",
      "\n",
      "The Verdict: Are We Betting The House On LangSmith?\n",
      "\n",
      "When a product like LangSmith comes along, it can feel really natural to default to the path of least resistance and offhand all responsibility. As we start to add additional functionality to HelpHub, there’s an inherent risk that GPT is going to lead users astray, and that’s just not an option we’re willing to entertain.\n",
      "\n",
      "So, in short, yes, we are putting a lot of trust right now in LangSmith and scaling our prototyping and debugging rapidly. The systems we’re building internally have already been instrumental in improving user experience, and as you’ve read earlier, many of these insights and improvements have come directly from those real-time traces from users chatting with HelpHub in the wild.\n",
      "\n",
      "Leveraging User Feedback For Improvements:\n",
      "\n",
      "We believe that every piece of user feedback, whether positive or negative, is a goldmine of insights. And with LangSmith's help (plus a little ingenuity on our side), we've turned these insights into actionable improvements.\n",
      "\n",
      "Here's how our feedback loop works:\n",
      "\n",
      "Real-time Feedback Collection: As users interact with HelpHub, they have the opportunity to provide feedback on the AI-generated responses, signaling with a simple \"thumbs up\" or \"thumbs down\". In-depth Analysis with LangSmith: Instead of just collecting feedback in the form or positive or negative signals, we delve deeper (particularly for the negative signals). Using LangSmith, we’re able to attribute each signal to an individual trace. In that trace, we can map the exact sequence of steps the model took to generate that response. We essentially replay GPT's thought process and LangChain’s actions, giving us the insights into what went right and where it veered off track. Categorizing Into Datasets: Central to our refinement process is LangSmith's use of dynamic datasets. We maintain multiple datasets, each tailored to different query types and user interactions. These datasets are essentially compilations of identical states of our environment at the time the trace was captured. This ensures that when there's an update to the prompt or LLM, a new dataset starts to compile those traces, preventing any contamination. Automating ENG-team Signals: When a user provides feedback, say a thumbs down, it's immediately flagged to our team via Slack. We built this little snippet to help the team screen traces and prioritize the ones that need attention right away. Iterating Quickly: We rigorously review the feedback, analyze the corresponding traces, and then, based on our insights, make informed adjustments to the model's role, prompts, or configurations to try and curb whatever jankiness was happening. This iterative process ensures our AI chatbot is continually refining its understanding, resonating more with user needs, and exceeding expectations over time.\n",
      "\n",
      "By combining granular AI insights through LangSmith with user feedback, we’ve created a pretty tight loop of perpetual improvement with HelpHub. This was such an important unlock for us as we build in tactical functionality to our AI.\n",
      "\n",
      "\n",
      "\n",
      "Advice for Product Teams Considering LangSmith\n",
      "\n",
      "When you're in the thick of product development, the whirlwind of AI and LLMs can be overwhelming. We've been there, boots on the ground, making sense of it all. From our journey with LangSmith and HelpHub, here's some hard-earned wisdom for fellow product teams.\n",
      "\n",
      "Start with Data, and Start Now:\n",
      "\n",
      "AI thrives on data. But don’t wait for the 'perfect' moment or the 'perfect' dataset. Start collecting now. Setting up LangSmith takes literally 5 minutes if you’re already using LangChain. Every bit of data, every interaction, adds a layer of understanding. But, a word to the wise: quality matters. Make sure the data reflects real-world scenarios, ensuring your AI resonates with genuine user needs.\n",
      "\n",
      "Dive Deep with Traces: Don't just skim the surface. Use LangSmith's trace feature to dive deep into AI decision-making. Every trace is a lesson, a chance to improve. Experiment with Prompts: One of LangSmith's standout features is its ability to test a new prompt across multiple examples without manual entry each time. This makes it incredibly efficient to iterate on your setup, ensuring you get the desired output from the AI. Note, in addition, the Playground is also an amazing tool to dig around with for testing prompts and adjustments to traces too. Lean on the Community: There's a whole community of LangSmith users out there. Swap stories, share challenges, and celebrate successes. You're not alone on this journey. Stay on Your Toes: AI doesn’t stand still, and neither should you. Keep an eye on LangSmith's updates. New features? Dive in. Test, iterate, refine.\n",
      "\n",
      "Conclusion\n",
      "\n",
      "After diving deep with LangSmith's traces, experimenting with prompts, testing, and iterating on our LLM environment, here's the real talk: LangSmith isn't just a tool for us - it's become a critical inclusion in our stack. We've moved from crossing our fingers and toes hoping our AI works to knowing exactly how and why.\n",
      "\n",
      "So, to our fellow AI product people trailblazers, dive into LangSmith. You’d be silly not to if you’re already using LangChain!\n",
      "URL: https://blog.langchain.dev/neum-x-langchain/\n",
      "Title: NeumAI x LangChain: Efficiently maintaining context in sync for AI applications\n",
      "\n",
      "Editors Note: This post was written by the NeumAI team and cross-posted from their blog. Keeping source data relevant and up-to-date efficiently is a challenge many builders are facing. It's especially painful for teams that are building on top of datasources constantly changing like team documentation (a use-case we see a lot of). Following up on our blog yesterday about making ingestion pipelines more production ready, we're really excited to highlight this because it continues in that vein. It adds scheduling and orchestration onto the ingestion pipeline, part of which is powered by LangChain text splitters.\n",
      "\n",
      "Last week, we released a blogpost about doing Q&A with thousands of documents and how Neum AI can help developers build large-scale AI apps to support that scenario. In this post, we want to dive deeper into a common problem with building large scale AI applications: Keeping context up to date in a cost-effectively way.\n",
      "\n",
      "Intro\n",
      "\n",
      "Let’s set up some context first (see what we did there ;)). Data is the most important part when building AI applications. If the data you are training the model with is of low quality, then your model with perform poorly. If the data you are using for your prompts is low quality, then your model responses will not be accurate. There are many more examples on why data is important but it is really the fundamental part for bringing accuracy to our AI models.\n",
      "\n",
      "Specifically here, let’s delve in context. Many have done chatbots where a massive prompt is passed to the model. This can become problematic for a couple of reasons.\n",
      "\n",
      "You might reach a context limit depending on the model you use The more tokens you pass the more costly your operation becomes\n",
      "\n",
      "And so, people have started to include context in the prompt that is fetched depending on the user’s query so as to only pass a subset of relevant information to the model for it to perform accurately. This is also called Retrieval Augmented Generation (RAG). Those who have built this know what I’m talking about but if you aren’t you can check these two blog posts by Pinecone and LangChain for more information.\n",
      "\n",
      "Source: Pinecone docs\n",
      "\n",
      "One problem not too many seem to talk about is how relevant this context is.\n",
      "\n",
      "Imagine you are creating a chatbot over a constantly-changing data source like a restaurant’s menu or some team documentation. You can easily build a chatbot with some of the libraries and tools explained in the previous post - like LangChain, Pinecone, etc. I won’t go into too many details but at a high level it goes something like this:\n",
      "\n",
      "Get your source data Vectorize it using some embedding model (This is crucial so that whenever we bring context to the prompt, the “search” based on the user query is done semantically and fast) Bring the context to the prompt of your model (like GPT-4 for example) and run the model. Output to the user\n",
      "\n",
      "This poses a trivial question. What if your source data changes?\n",
      "\n",
      "It could be that the restaurant is no longer offering an item from the menu. That an internal documents or wiki was just updated with some new content.\n",
      "\n",
      "Will our chatbot respond with high accuracy?\n",
      "\n",
      "Chances are, no. Unless you have a way to give your AI model, up to date context, it probably won’t know that the pepperoni pizza is no longer available or that the documentation for onboarding a new team member to the team changed. It will respond with whatever context had been stored before in the vector store (or even without any context!)\n",
      "\n",
      "ChatGPT response with no context\n",
      "\n",
      "‍\n",
      "\n",
      "Enter Neum\n",
      "\n",
      "With Neum we automatically synchronize your source data with your vector store. This means that whenever an AI application wants to query the vector db for semantic search or bringing context to an AI model, the information will always be updated. It is important to note that the quality of your model also depends on how you vectorize the data. At Neum, we leverage different LangChain tools to partition the source data depending on the use case.‍\n",
      "\n",
      "Neum pipeline builder example for syncing your data between Notion and Pinecone\n",
      "\n",
      "‍\n",
      "\n",
      "These are amongst the top things that are needed when building this synchronization for your LLM data.\n",
      "\n",
      "Setting up the infrastructure required to sync the sources Setting up your scheduler or real-time pipelines to update the data Handling errors if something goes wrong at any given point And most importantly, efficiently vectorizing to reduce costs\n",
      "\n",
      "Now, let’s briefly talk about costs.\n",
      "\n",
      "OpenAI embeddings pricing model currently is $0.0001/1k tokens. That might not look like much but at large scale, it translates roughly to 10k per 1TB of data. If your source data is not large, you might get away with it by constantly vectorizing and storing your data in the vector store.\n",
      "\n",
      "But what if you have lots of documents? What if you have millions and millions of rows in your database? Vectorizing everything all the time will not only be inefficient but very costly!\n",
      "\n",
      "At Neum, we’ve developed tech to help detect differences and only vectorize the necessary information, thus keeping the context up-to-date but in an efficient and cost-saving way.\n",
      "\n",
      "See it to believe it\n",
      "\n",
      "To prove this we created a sample chatbot for our Notion workspace that is updated automatically as the Notion is updated with more content. It allows users to as questions and get up-to-date responses if something changed internally. The sample is built with Vercel as frontend and Pinecone as the vector store. Internally, Neum leverages LangChain for its text splitter tools.\n",
      "\n",
      "Behind the scenes, Neum is not only ensuring that updates are extracted, embedded and loaded into Pinecone, but also makes sure that we are only updating data that needs to be. If a section of the Notion workspace didn’t change, we don’t re-embed it. If a section changed, then it is re-embedded. This approach delivers a better user experience by having up to date data that is also more cost effective by only using resources where needed.\n",
      "\n",
      "Take a look at the 2min video below for an in-depth look of how it works!\n",
      "\n",
      "You can reach out to founders@tryneum.com if interested in these topics!\n",
      "URL: https://blog.langchain.dev/week-of-8-7/\n",
      "Title: [Week of 8/7] LangChain Release Notes\n",
      "\n",
      "New in LangSmith\n",
      "\n",
      "Teams: You can now collaborate in LangSmith! We’ve released support for teams of up to five people. If you need more, get in touch at support@langchain.dev\n",
      "\n",
      "You can now collaborate in LangSmith! We’ve released support for teams of up to five people. If you need more, get in touch at support@langchain.dev Cookbooks: Learn how to get more out of LangSmith's debugging, testing, and feedback functionality with these end-to-end recipes in the LangSmith Cookbook repository. Cookbooks added so far: How to use LangSmith if you’re NOT using LangChain\n",
      "\n",
      "Learn how to get more out of LangSmith's debugging, testing, and feedback functionality with these end-to-end recipes in the LangSmith Cookbook repository. Cookbooks added so far: How to use LangSmith if you’re NOT using LangChain What use-cases would you like to see? Tell us at hello@langchain.dev\n",
      "\n",
      "UI updates: resizable columns in table views and enhanced navigation\n",
      "\n",
      "resizable columns in table views and enhanced navigation Performance improvements and Bug Fixes: fixed stuttering occurring on run tree navigation\n",
      "\n",
      "New in Open Source:\n",
      "\n",
      "LangChain Expression Language: a new syntax to create chains with composition. webinar recording here. Cookbook of examples rewriting popular chains here.\n",
      "\n",
      "a new syntax to create chains with composition. webinar recording here. Cookbook of examples rewriting popular chains here. Conversational Retrieval Agents: There have been several emerging trends in LLM applications over the past few months: RAG, chat interfaces, agents. Our newest functionality - conversational retrieval agents - combines them all. blog post here.\n",
      "\n",
      "There have been several emerging trends in LLM applications over the past few months: RAG, chat interfaces, agents. Our newest functionality - conversational retrieval agents - combines them all. blog post here. Parent Document Retriever: A new retrieval algorithm that creates small chunks (to allow embeddings to have semantic meaning) and fetches the parent documents those chunks came from (to capture full context). Docs here.\n",
      "\n",
      "A new retrieval algorithm that creates small chunks (to allow embeddings to have semantic meaning) and fetches the parent documents those chunks came from (to capture full context). Docs here. Text Splitting Playground: Chunking text into appropriate splits is seemingly trivial yet very nuanced. Open sourcing a playground to help explore different text splitting strategies. GitHub here. Hosted Playground here.\n",
      "\n",
      "Chunking text into appropriate splits is seemingly trivial yet very nuanced. Open sourcing a playground to help explore different text splitting strategies. GitHub here. Hosted Playground here. Use Case Documentation: We’ve updated documentation to better highlight all the use cases of LangChain. Big shout out to community members Francisco Pingham and Manuel Soria for their help here!\n",
      "\n",
      "We’ve updated documentation to better highlight all the use cases of LangChain. Big shout out to community members Francisco Pingham and Manuel Soria for their help here! LangChain.js + Next.js starter template: more streaming and prompt customization developer experience improvements via popular use-cases you can clone, remix, and deploy on Vercel in a few clicks\n",
      "\n",
      "In case you missed it\n",
      "\n",
      "Use-cases we love\n",
      "\n",
      "Thank you’s\n",
      "\n",
      "Want this in your inbox every other week? Sign up here.\n",
      "URL: https://blog.langchain.dev/ted-ai-hackathon-kickoff/\n",
      "Title: TED AI Hackathon Kickoff (and projects we’d love to see)\n",
      "\n",
      "The TED AI Hackathon kicks off October 14 and we’re excited to be their partner in bringing it to life! TED has a long history of inspiring and educating great thinkers and builders and we can’t wait to see what everyone builds.\n",
      "\n",
      "You can register here and here's a video walkthrough of the event.\n",
      "\n",
      "We’re anticipating (and hoping!) there are many of you who might be building with LLMs for the first time. So, we thought we’d share some resources to help you get started and ideas for apps that we’d love to see (and share out with the broader community!). We’ll also be sponsoring a prize for the winner of the best LLM application category!\n",
      "\n",
      "Getting Started with LangChain\n",
      "\n",
      "Here’s how to install LangChain, set up your environment, and start building. It’s as simple as: 'pip install langchain'\n",
      "\n",
      "Here’s our Quickstart guide, designed to familiarize yourself with the LangChain framework through building your first LangChain application.\n",
      "\n",
      "Here’s a link to use-case specific walkthroughs for question and answering, interacting with APIs, chatbots, extraction, code understanding, summarization, tagging, web scraping, and more!\n",
      "\n",
      "YouTube tutorials: Here’s a list of tutorials created by our community that we think are especially helpful.\n",
      "\n",
      "Inspiration: Check out our gallery of some of our favorite applications built on top of LangChain.\n",
      "\n",
      "You can also get your feet wet by contributing to our open source repo. Here’s a list of good first issues to try.\n",
      "\n",
      "LangChain Prompt Hub\n",
      "\n",
      "We recently launched LangChain Hub, a home for submitting, discovering, inspecting, and remixing prompts. It’s still (very) early days–we see it as not only a useful tool for helping developers build faster, but also as a way for the entire community to get collectively smarter on prompting overall.\n",
      "\n",
      "\n",
      "\n",
      "We hope you’ll contribute prompts and try each other’s out–here’s how.\n",
      "\n",
      "\n",
      "\n",
      "Projects we’d love to see (and share with the LangChain community!)\n",
      "\n",
      "applications that use open source models\n",
      "\n",
      "innovative retrieval tactics\n",
      "\n",
      "practical agents\n",
      "\n",
      "adventurous agents\n",
      "\n",
      "apps that connect unique data sources/format(s)\n",
      "\n",
      "\n",
      "\n",
      "We’ll be tweeting out our favorite projects throughout the week and collecting them for a blog post to close out the Hackathon.\n",
      "\n",
      "Tag us on Twitter (@langchainai) or send us a note at hello@langchain.dev. We can’t wait to see what you build!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the parent document using the parent_id from the child chunk's metadata. For illustration I pick the first one.\n",
    "parent_id = child_results[0].metadata.get('parent_id')\n",
    "parent_results = retriever.retrieve(query=parent_id, retrieve_parents=True, retrieve_children=False)\n",
    "\n",
    "# Display the entire parent document content\n",
    "for parent_result in parent_results:\n",
    "    print(parent_result.page_content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
