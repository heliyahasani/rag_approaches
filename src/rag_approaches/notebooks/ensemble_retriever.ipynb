{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Ensemble Retriever combines the strengths of multiple retrieval methods to improve the overall retrieval performance. By using different retrievers (like BM25 and FAISS) together, you can leverage the advantages of each method and mitigate their individual weaknesses.\n",
    "\n",
    "Weighted Scoring:\n",
    "\n",
    " - Each retriever produces its own set of results with associated relevance scores. The ensemble retriever then combines these scores, often using a weighted sum or another aggregation method, to produce a final ranking of documents.\n",
    "Ranking and Fusion:\n",
    "\n",
    "- The ensemble can rank the documents based on the combined scores and either merge the results (by taking the top-ranked documents from each retriever) or re-rank the combined set of documents.\n",
    "Adaptive Strategies:\n",
    "\n",
    "Some ensemble retrievers adaptively adjust the weights given to each retriever based on the query or the context. For example, certain queries might benefit more from dense retrieval (FAISS), while others might benefit from keyword-based retrieval (BM25)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.schema import Document\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory path (adjust as needed)\n",
    "directory_path = \"/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post\"\n",
    "\n",
    "# Use glob to find all .txt files in the directory and subdirectories\n",
    "txt_files = glob.glob(f\"{directory_path}**/*.txt\", recursive=True)\n",
    "\n",
    "# Initialize an empty list for loaders\n",
    "loaders = [TextLoader(path) for path in txt_files]\n",
    "\n",
    "# Initialize an empty list to store documents\n",
    "docs = []\n",
    "\n",
    "# Loop through each loader, load the document, and extend the docs list\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heliya/Desktop/rag_approaches/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_evaluating-rag-pipelines-with-ragas-langsmith_.txt'}, page_content='URL: https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/\\nTitle: Evaluating RAG pipelines with Ragas + LangSmith\\n\\nEditor\\'s Note: This post was written in collaboration with the Ragas team. One of the things we think and talk about a lot at LangChain is how the industry will evolve to identify new monitoring and evaluation metrics that evolve beyond traditional ML ops metrics. Ragas is an exciting new framework that helps developers evaluate QA pipelines in new ways. This post shows how LangSmith and Ragas can be a powerful combination for teams that want to build reliable LLM apps.\\n\\nHow important evals are to the team is a major differentiator between folks rushing out hot garbage and those seriously building products in the space.\\n\\nThis HackerNews comment emphasizes the importance of having a robust eval strategy when taking your application from a cool demo to a production-ready product. This is especially true when building LLM applications because of the underlying stochastic nature of the models powering them. You can easily build really impressive LLM applications with a few short training and verifying it on a few examples but to make it more robust you have to test it on a test dataset that matches the production distribution. This is an evolutionary process and something you will be doing throughout the lifecycle of your application but let\\'s see how Ragas and LangSmith can help you here.\\n\\nThis article is going to be specifically about QA systems (or RAG systems). Every QA pipeline has 2 components\\n\\nRetriever - which retrieves the most relevant information needed to answer the query Generator - which generates the answer with the retrieved information.\\n\\nWhen evaluating a QA pipeline you have to evaluate both of these separately and together to get an overall score as well as the individual scores to pinpoint the aspects you want to improve. For example, using a better chunking strategy or embedding model can improve the retriever while using a different model or prompt can bring about changes to the generation step. You might want to measure the tendency for hallucinations in the generation step or the recall for the retrieval step.\\n\\nBut what metrics and datasets can you use to measure and benchmark these components? There are many conventional metrics and benchmarks for evaluating QA systems like ROUGE and BLUE but they have poor correlation with human judgment. Furthermore, the correlation between the performance of your pipeline over a standard benchmark (like Beir) and production data can vary depending on the distribution shift between both of them. Moreover, building such a golden test set is an expensive and time-consuming task.\\n\\nLeveraging LLMs for evaluations\\n\\nLeveraging a strong LLM for reference-free evaluation is an upcoming solution that has shown a lot of promise. They correlate better with human judgment than traditional metrics and also require less human annotation. Papers like G-Eval have experimented with this and given promising results but there are certain shortcomings too.\\n\\nLLM prefers outputs their own outputs and when asked to compare between different outputs the relative position of those outputs matters more. LLMs can also have a bias toward a value when asked to score given a range and they also prefer longer responses. Refer to the Large Language Models are not Fair Evaluators paper for more. Ragas aims to work around these limitations of using LLMs to evaluate your QA pipelines while also providing actionable metrics using as little annotated data as possible, cheaper, and faster.\\n\\nIntroducing Ragas\\n\\nRagas is a framework that helps you evaluate your QA pipelines across these different aspects. It provides you with a few metrics to evaluate the different aspects of your QA systems namely\\n\\nmetrics to evaluate retrieval: offers context_relevancy and context_recall which give you the measure of the performance of your retrieval system. metrics to evaluate generation: offers faithfulness which measures hallucinations and answer_relevancy which measures how to the point the answers are to the question.\\n\\nThe harmonic mean of these 4 aspects gives you the ragas score which is a single measure of the performance of your QA system across all the important aspects.\\n\\nMost of the measurements do not require any labeled data, making it easier for users to run it without worrying about building a human-annotated test dataset first. In order to run ragas all you need is a few questions and if your using context_recall , a reference answer. (The option to cold start your test dataset is also in the roadmap)\\n\\nNow let\\'s see Ragas in action and try this by evaluating a QA chain build with Langchain.\\n\\nEvaluating a QA chain\\n\\nWe’re going to build a QA chain over the NYC Wikipedia page and run our evaluations on top of it. This is a pretty standard QA chain but feel free to check out the docs.\\n\\nfrom langchain.document_loaders import WebBaseLoader from langchain.indexes import VectorstoreIndexCreator from langchain.chains import RetrievalQA from langchain.chat_models import ChatOpenAI # load the Wikipedia page and create index loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/New_York_City\") index = VectorstoreIndexCreator().from_loaders([loader]) # create the QA chain llm = ChatOpenAI() qa_chain = RetrievalQA.from_chain_type( llm, retriever=index.vectorstore.as_retriever(), return_source_documents=True ) # testing it out question = \"How did New York City get its name?\" result = qa_chain({\"query\": question}) result[\"result\"] # output # \\'New York City got its name in 1664 when it was renamed after the Duke of York, who later became King James II of England. The city was originally called New Amsterdam by Dutch colonists and was renamed New York when it came under British control.\\'\\n\\nin order to use Ragas with LangChain, first import all the metrics you want to use from ragas.metrics. Next import the RagasEvaluatorChain which is a langchain chain wrapper to convert a ragas metric into a langchain EvaluationChain.\\n\\nfrom ragas.metrics import faithfulness, answer_relevancy, context_relevancy, context_recall from ragas.langchain import RagasEvaluatorChain # make eval chains eval_chains = { m.name: RagasEvaluatorChain(metric=m) for m in [faithfulness, answer_relevancy, context_relevancy, context_recall] }\\n\\nOnce the evaluator chains are created you can call the chain via the __call__() method with the outputs from the QA chain to run the evaluations\\n\\n# evaluate for name, eval_chain in eval_chains.items(): score_name = f\"{name}_score\" print(f\"{score_name}: {eval_chain(result)[score_name]}\") # output # faithfulness_score: 1.0 # answer_relevancy_score: 0.9193459596511587 # context_relevancy_score: 0.07480974380786602 # context_recall_score: 0.9193459596511587\\n\\nRagas uses LLMs under the hood to do the evaluations but leverages them in different ways to get the measurements we care about while overcoming the biases they have. Let\\'s learn more about how they work under the hood to see how.\\n\\nUnder the hood\\n\\nOptional, not required to follow the next steps but helps understand the inner workings of ragas.\\n\\nAll the metrics are documented here but in this section let\\'s try and understand how exactly each Ragas metric works.\\n\\nFaithfulness: measures the factual accuracy of the generated answer with the context provided. This is done in 2 steps. First, given a question and generated answer, Ragas uses an LLM to figure out the statements that the generated answer makes. This gives a list of statements whose validity we have we have to check. In step 2, given the list of statements and the context returned, Ragas uses an LLM to check if the statements provided are supported by the context. The number of correct statements is summed up and divided by the total number of statements in the generated answer to obtain the score for a given example. Answer Relevancy: measures how relevant and to the point the answer is to the question. For a given generated answer Ragas uses an LLM to find out the probable questions that the generated answer would be an answer to and computes similarity to the actual question asked. Context Relevancy: measures the signal-to-noise ratio in the retrieved contexts. Given a question, Ragas calls LLM to figure out sentences from the retrieved context that are needed to answer the question. A ratio between the sentences required and the total sentences in the context gives you the score Context Recall: measures the ability of the retriever to retrieve all the necessary information needed to answer the question. Ragas calculates this by using the provided ground_truth answer and using an LLM to check if each statement from it can be found in the retrieved context. If it is not found that means the retriever was not able to retrieve the information needed to support that statement.\\n\\nUnderstanding how each Ragas metric works gives you clues as to how the evaluation was performed making these metrics reproducible and more understandable. One easy way to visualize the results from Ragas is to use the traces from LangSmith and LangSmith’s evaluation features. Let\\'s look more into that now\\n\\nVisualising the Evaluations with LangSmith\\n\\nWhile Ragas provides you with a few insightful metrics, it does not help you in the process of continuously evaluation of your QA pipeline in production. But this is where LangSmith comes in.\\n\\nLangSmith is a platform that helps to debug, test, evaluate, and monitor chains and agents built on any LLM framework. LangSmith offers the following benefits\\n\\na platform to create and store a test dataset and run evaluations. a platform to visualise and dig into the evaluation results. Makes Ragas metrics explainable and reproducible. The ability to continuously add test examples from production logs if your app is also monitored with LangSmith.\\n\\nWith RagasEvaluatorChain you can use the Ragas metrics for running LangSmith evaluations as well. To know more about LangSmith evaluations check out the quickstart.\\n\\nLet\\'s start with your need to create a dataset inside LangSmith to store the test examples. We’re going to start off with a small dataset with 5 questions and answers to those questions (used only for the context_recall metric).\\n\\n# test dataset eval_questions = [ \"What is the population of New York City as of 2020?\", \"Which borough of New York City has the highest population?\", \"What is the economic significance of New York City?\", \"How did New York City get its name?\", \"What is the significance of the Statue of Liberty in New York City?\", ] eval_answers = [ \"8,804,000\", # incorrect answer \"Queens\", # incorrect answer \"New York City\\'s economic significance is vast, as it serves as the global financial capital, housing Wall Street and major financial institutions. Its diverse economy spans technology, media, healthcare, education, and more, making it resilient to economic fluctuations. NYC is a hub for international business, attracting global companies, and boasts a large, skilled labor force. Its real estate market, tourism, cultural industries, and educational institutions further fuel its economic prowess. The city\\'s transportation network and global influence amplify its impact on the world stage, solidifying its status as a vital economic player and cultural epicenter.\", \"New York City got its name when it came under British control in 1664. King Charles II of England granted the lands to his brother, the Duke of York, who named the city New York in his own honor.\", \\'The Statue of Liberty in New York City holds great significance as a symbol of the United States and its ideals of liberty and peace. It greeted millions of immigrants who arrived in the U.S. by ship in the late 19th and early 20th centuries, representing hope and freedom for those seeking a better life. It has since become an iconic landmark and a global symbol of cultural diversity and freedom.\\', ] examples = [{\"query\": q, \"ground_truths\": [eval_answers[i]]} for i, q in enumerate(eval_questions)]\\n\\n# dataset creation from langsmith import Client from langsmith.utils import LangSmithError client = Client() dataset_name = \"NYC test\" try: # check if dataset exists dataset = client.read_dataset(dataset_name=dataset_name) print(\"using existing dataset: \", dataset.name) except LangSmithError: # if not create a new one with the generated query examples dataset = client.create_dataset( dataset_name=dataset_name, description=\"NYC test dataset\" ) for q in eval_questions: client.create_example( inputs={\"query\": q}, dataset_id=dataset.id, ) print(\"Created a new dataset: \", dataset.name)\\n\\nIf you go to the LangSmith dashboard and check the datasets section you should be able to see the dataset we just created.\\n\\nLangSmith dataset tab with the NYC dataset we created\\n\\nTo run the evaluations you have to call the run_on_dataset() function from the LangSmith SDK. but before that, you have to create a chain factory that will return a new QA chain every time this is called. This is so that any states in the QA chain are not reused when evaluating with individual examples. Make sure the QA chains return the context if using metrics that check on context.\\n\\n# factory function that return a new qa chain def create_qa_chain(return_context=True): qa_chain = RetrievalQA.from_chain_type( llm, retriever=index.vectorstore.as_retriever(), return_source_documents=return_context, ) return qa_chain\\n\\nNow let\\'s configure and run the evaluation. You use RunEvalConfig to configure the evaluation with the metrics/evaluator chains that you want to run against and the prediction_key for the returned result. Now call run_on_dataset and LangSmith to use the dataset we uploaded and run it against the chain from the factory and evaluate with the custom_evaluators we provided and upload the results.\\n\\nfrom langchain.smith import RunEvalConfig, run_on_dataset evaluation_config = RunEvalConfig( custom_evaluators=[eval_chains.values()], prediction_key=\"result\", ) result = run_on_dataset( client, dataset_name, create_qa_chain, evaluation=evaluation_config, input_mapper=lambda x: x, ) # output # View the evaluation results for project \\'2023-08-24-03-36-45-RetrievalQA\\' at: # https://smith.langchain.com/projects/p/9fb78371-150e-49cc-a927-b1247fdb9e8d?eval=true\\n\\nOpen up the results and you will see something like this\\n\\nThis shows the output for each example in the training dataset and the Feedback columns show the evaluation results. Now if you want to dive more into the reasons for the scores and how to improve them, click on any single example and open the Feedback tab.\\n\\nThis will show you each score and if you open the pop-up icon it will point you to the evaluation run of the RagasEvaluatorChain.\\n\\nYou can analyze each result to see why it was so and this will give you ideas on how to improve it.\\n\\nNow if your QA pipeline also uses LangSmith for logging, tracing, and monitoring you can leverage the add-to-dataset feature to set up a continuous evaluation pipeline that keeps adding interesting data points (based on human feedback of other indirect methods) into the test to keep the test dataset up to date with a more comprehensive dataset with wider coverage.\\n\\nConclusion\\n\\nRagas enhances QA system evaluation by addressing limitations in traditional metrics and leveraging Large Language Models. LangSmith provides a platform for running evaluations and visualizing results.\\n\\nBy using Ragas and LangSmith, you can ensure your QA systems are robust and ready for real-world applications, making the development process more efficient and reliable.'),\n",
       " Document(metadata={'source': '/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt'}, page_content='URL: https://blog.langchain.dev/chat-with-your-data-using-openai-pinecone-airbyte-langchain/\\nTitle: Chat with your data using OpenAI, Pinecone, Airbyte and Langchain\\n\\nEditor’s Note: This blog post was written in collaboration with Airbyte. Their new vector database destination makes it really easy for data to retrieve relevant context for question answering use cases via LangChain. We\\'re seeing more and more teams seek ways to integrate diverse data sources–and keep them up-to-date automatically–and this is a fantastic way to do it!\\n\\nAside from the specific use case highlighted here, we\\'re also very excited about this integration in general. It combines the hundreds of sources in AirByte with their robust scheduling and orchestration framework, and leverages the advanced transformation logic in LangChain along with LangChain\\'s 50+ embedding provider integrations and 50+ vectorstore integrations.\\n\\nLearn how to build a connector development support bot for Slack that knows all your APIs, open feature requests and previous Slack conversations by heart\\n\\nIn a previous article, we explained how Dagster and Airbyte can be leveraged to power LLM-supported use cases. Our newly introduced vector database destination makes this even easier as it removes the need to orchestrate chunking and embedding manually - instead the sources can be directly connected to the vector database through an Airbyte connection.\\n\\nThis tutorial walks you through a real-world use case of how to leverage vector databases and LLMs to make sense out of your unstructured data. By the end of this, you will:\\n\\nKnow how to extract unstructured data from a variety of sources using Airbyte\\n\\nKnow how to use Airbyte to efficiently load data into a vector database, preparing the data for LLM usage along the way\\n\\nKnow how to integrate a vector database into your LLM to ask questions about your proprietary data\\n\\nWhat we will build\\n\\nTo better illustrate how this can look in practice, let’s use something that’s relevant for Airbyte itself.\\n\\nAirbyte is a highly extensible system that allows users to develop their own connectors to extract data from any API or internal systems. Helpful information for connector developers can be found in different places:\\n\\nThe official connector development documentation website\\n\\nGithub issues documenting existing feature requests, known bugs and work in progress\\n\\nThe community Slack help channel\\n\\nThis article describes how to tie together all of these diverse sources to offer a single chat interface to access information about connector development - a bot that can answer questions in plain english about the code base, documentation and reference previous conversations:\\n\\n\\n\\nIn these examples, information from the documentation website and existing Github issues is combined in a single answer.\\n\\nPrerequisites\\n\\nFor following through the whole process, you will need the following accounts. However, you can also work with your own custom sources and use a local vector store to avoid all but the OpenAI account:\\n\\nSource-specific accounts\\n\\nApify account\\n\\nGithub account\\n\\nSlack account\\n\\nDestination-specific accounts\\n\\nOpenAI account\\n\\nPinecone account\\n\\nAirbyte instance (local or cloud)\\n\\nStep 1 - Fetch Github issues\\n\\nAirbyte’s feature and bug tracking is handled by the Github issue tracker of the Airbyte open source repository. These issues contain important information people need to look up regularly.\\n\\nTo fetch Github issues, create a new source using the Github connector.\\n\\nIf you are using Airbyte Cloud , you can easily authenticate using the “Authenticate your GitHub account”, otherwise follow the instructions in the documentation on the right side of how to set up a personal access token in the Github UI.\\n\\nNext, configure a cutoff date for issues and specify the repositories that should be synced. In this case I’m going with “2023-07-01T00:00:00Z” and “airbytehq/airbyte” to sync recent issues from the main Airbyte repository:\\n\\nStep 2 - Load into vector database\\n\\nNow we have our first source ready, but Airbyte doesn’t know yet where to put the data. The next step is to configure the destination. To do so, pick the “Vector Database (powered by LangChain)”. There is some preprocessing that Airbyte is doing for you so that the data is vector ready:\\n\\nSeparating text and metadata fields and splitting up records into multiple documents to keep each document focused on a single topic and to make sure the text fits into the context window of the LLM that’s going to be used for question answering\\n\\nEmbedding the text of every document using the configured embedding service, turning the text into a vector to do similarity search on\\n\\nIndexing the documents into the vector database (uploading the vector from the embedding service along with the metadata object)\\n\\nThe vector database destination currently supports two different vector databases (with more to come) - Pinecone, which is a hosted service with a free tier and Chroma which stores the vector database in a local file.\\n\\nFor using Pinecone, sign up for a free trial account and create an index using a starter pod. Set the dimensions to 1536 as that’s the size of the OpenAI embeddings we will be using\\n\\nOnce the index is ready, configure the vector database destination in Airbyte:\\n\\nSet chunk size to 1000 (the size refers to number of tokens, not characters, so this is roughly 4KB of text. The best chunking is dependent on the data you are dealing with)\\n\\n(the size refers to number of tokens, not characters, so this is roughly 4KB of text. The best chunking is dependent on the data you are dealing with) Configure the records fields to treat as text fields which will be embedded. All other fields will be handled as metadata. For now, set it “ title ” and “ body ” as these are the relevant feels in the issue stream of the Github source\\n\\n” and “ ” as these are the relevant feels in the issue stream of the Github source Set your OpenAI api key for powering the embedding service. You can find your API key in the API keys section of the platform.openai.com/account page\\n\\nFor the indexing step, copy over index, environment and api key from the Pinecone UI. You can find the API key and the environment in the “API Keys” section in the UI\\n\\nStep 3 - Create a connection\\n\\nOnce the destination is set up successfully, set up a connection from the Github source to the vector database destination. In the configuration flow, pick the existing source and destination. When configuring the connection, make sure to only use the “issues” stream, as this is the one we are interested in.\\n\\nSide note: Airbyte allows to make this sync more efficient in a production environment:\\n\\nTo keep the metadata focused, you can click on the stream name to select the individual fields you want to sync. For example if the “assignee” or the “milestone” field is never relevant to you, you can uncheck it and it won’t be synced to the destination.\\n\\nThe sync mode can be used to sync issues incrementally while deduplicating the records in the vector database so no stale data will show up in searches\\n\\n\\n\\nIf everything went well, there should be a connection now syncing data from Github to Pinecone via the vector store destination. Give the sync a few minutes to run. Once the first run has completed, you can check the Pinecone index management page to see a bunch of indexed vectors ready to be queried.\\n\\nEach vector is associated with a metadata object that’s filled with the fields that were not mentioned as “text fields” in the destination configuration. These fields will be retrieved along with the embedded text and can be leveraged by our chatbot in later sections. This is how a vector with metadata looks like when retrieved from Pinecone:\\n\\n{ \"id\": \"599d75c8-517c-4f37-88df-ff16576bd607\", \"values\": [0.0076571689, ..., 0.0138477711], \"metadata\": { \"_airbyte_stream\": \"issues\", \"_record_id\": 1556650122, \"author_association\": \"CONTRIBUTOR\", \"comments\": 3, \"created_at\": \"2023-01-25T13:21:50Z\", // ... \"text\": \"...The acceptance-test-config.yml file is in a legacy format. Please migrate to the latest format...\", \"updated_at\": \"2023-07-17T09:20:56Z\", } }\\n\\nOn subsequent runs, Airbyte will only re-embed and update the vectors for the issues that changed since the last sync - this will speed up subsequent runs while making sure your data is always up-to-date and available.\\n\\n\\n\\nStep 4 - Chat interface\\n\\nThe data is ready, now let’s wire it up with our LLM to answer questions in natural language. As we already used OpenAI for the embedding, the easiest approach is to use it as well for the question answering.\\n\\nWe will use Langchain as an orchestration framework to tie all the bits together.\\n\\nFirst, install a few pip packages locally:\\n\\npip install pinecone-client langchain openai\\n\\nThe basic functionality here works the following way:\\n\\nUser asks a question\\n\\nThe question is embedded using the same model used for generating the vectors in the vector database (OpenAI in this case)\\n\\nThe question vector is sent to the vector database and documents with similar vectors are returned - as the vectors represent the meaning of the text, the question and the answer to the question will have very similar vectors and relevant documents will be returned\\n\\nThe text of all documents with the relevant metadata are put together into a single string and sent to the LLM together with the question the user asked and the instruction to answer the user’s question based on the provided context\\n\\nThe LLM answers the question based on the provided context\\n\\nThe answer is presented to the user\\n\\nThis flow is often referred to as retrieval augmented generation. The RetrievalQA class from the Langchain framework already implements the basic interaction. The simplest version of our question answering bot only has to provide the vector store and the used LLM:\\n\\n# chatbot.py import os import pinecone from langchain.chains import RetrievalQA from langchain.embeddings import OpenAIEmbeddings from langchain.llms import OpenAI from langchain.vectorstores import Pinecone embeddings = OpenAIEmbeddings() pinecone.init(api_key=os.environ[\"PINECONE_KEY\"], environment=os.environ[\"PINECONE_ENV\"]) index = pinecone.Index(os.environ[\"PINECONE_INDEX\"]) vector_store = Pinecone(index, embeddings.embed_query, \"text\") qa = RetrievalQA.from_chain_type(llm=OpenAI(temperature=0), chain_type=\"stuff\", retriever=vector_store.as_retriever()) print(\"Connector development help bot. What do you want to know?\") while True: query = input(\"\") answer = qa.run(query) print(answer) print(\"\\n\\nWhat else can I help you with:\")\\n\\nTo run this script, you need to set OpenAI and Pinecone credentials as environment variables:\\n\\nexport OPENAI_API_KEY=... export PINECONE_KEY=... export PINECONE_ENV=... export PINECONE_INDEX=... python chatbot.py\\n\\nThis works in general, but it has some limitations. By default, only the text fields are passed into the prompt of the LLM, so it doesn’t know what the context of a text is and it also can’t give a reference back to where it found its information:\\n\\nConnector development help bot. What do you want to know? > Can you give me information about how to authenticate via a login endpoint that returns a session token? Yes, the GenericSessionTokenAuthenticator should be supported in the UI[...]\\n\\nFrom here, there’s lots of fine tuning to do to optimize our chat bot. For example we can improve the prompt to contain more information based on the metadata fields and be more specific for our use case:\\n\\nprompt_template = \"\"\"You are a question-answering bot operating on Github issues and documentation pages for a product called connector builder. The documentation pages document what can be done, the issues document future plans and bugs. Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer. Always state were you got this information from (and the github issue number if applicable). If the answer is based on a Github issue that\\'s not closed yet, add \\'This issue is not closed yet - the feature might not be shipped yet\\' to the answer. {context} Question: {question} Helpful Answer:\"\"\" prompt = PromptTemplate( template=prompt_template, input_variables=[\"context\", \"question\"] ) class ConnectorDevelopmentPrompt(PromptTemplate): def format_document(doc: Document, prompt: PromptTemplate) -> str: if doc.metadata[\"_airbyte_stream\"] == \"issues\": return f\"Excerpt from Github issue: {doc.page_content}, issue number: {doc.metadata[\\'number\\']}, issue state: {doc.metadata[\\'state\\']}\" else: return super().format_document(doc, prompt) document_prompt = ConnectorDevelopmentPrompt(input_variables=[\"page_content\"], template=\"{page_content}\") qa = RetrievalQA.from_chain_type(llm=OpenAI(temperature=0), chain_type=\"stuff\", retriever=vector_store.as_retriever(), chain_type_kwargs={\"prompt\": prompt, \"document_prompt\": document_prompt})\\n\\nThe full script also be found on Github\\n\\nThis revised version of the RetrievalQA chain customizes the prompts that are sent to the LLM after the context has been retrieved:\\n\\nThe basic prompt template sets the broader context what this question is about (previously the LLM had to guess from the documents)\\n\\nIt also changes the way documents are added to the prompt - by default, only the text is added, but the ConnectorDevelopmentPrompt implementation sets the context where the data is coming from and also adds relevant metadata to the prompt so the LLM can base its answer on more than just the text\\n\\nConnector development help bot. What do you want to know? > Can you give me information about how to authenticate via a login endpoint that returns a session token? You can use the GenericSessionTokenAuthenticator to authenticate via a login endpoint that returns a session token. This is documented in the Connector Builder documentation with an example of how the request flow functions (e.g. metabase). This issue is not closed yet - the feature might not be shipped yet (Github issue #26341).\\n\\n\\n\\nStep 5 - Put it on Slack\\n\\nSo far this helper can only be used locally. However, using the python slack sdk it’s easy to turn this into a Slack bot itself.\\n\\nTo do so, we need to set up a Slack “App” first. Go to https://api.slack.com/apps and create a new app based on the manifest here (this saves you some work configuring permissions by hand). After you set up your app, install it to the workspace you want to integrate with. This will generate a “Bot User OAuth Access Token” you need to note down. Afterwards, go to the “Basic information” page of your app, scroll down to “App-Level Tokens” and create a new token. Note down this “app level token” as well.\\n\\nWithin the regular Slack client, your app can be added to a slack channel by clicking the channel name and going to the “Integrations” tab:\\n\\nAfter this, your Slack app is ready to receive pings from users to answer questions - the next step is to call Slack from within python code, so we need to install the python client library:\\n\\npip install slack_sdk\\n\\nAfterwards, we can extend our existing chatbot script with a Slack integration:\\n\\nfrom slack_sdk import WebClient from slack_sdk.socket_mode import SocketModeClient from slack_sdk.socket_mode.request import SocketModeRequest from slack_sdk.socket_mode.response import SocketModeResponse slack_web_client = WebClient(token=os.environ[\"SLACK_BOT_TOKEN\"]) handled_messages = {} def process(client: SocketModeClient, socket_mode_request: SocketModeRequest): if socket_mode_request.type == \"events_api\": event = socket_mode_request.payload.get(\"event\", {}) client_msg_id = event.get(\"client_msg_id\") if event.get(\"type\") == \"app_mention\" and not handled_messages.get(client_msg_id): handled_messages[client_msg_id] = True channel_id = event.get(\"channel\") text = event.get(\"text\") result = qa.answer(text) slack_web_client.chat_postMessage(channel=channel_id, text=result) return SocketModeResponse(envelope_id=socket_mode_request.envelope_id) socket_mode_client = SocketModeClient( app_token=os.environ[\"SLACK_APP_TOKEN\"], web_client=slack_web_client ) socket_mode_client.socket_mode_request_listeners.append(process) socket_mode_client.connect() print(\"listening\") from threading import Event Event().wait()\\n\\nThe full script also be found on Github\\n\\nTo run the script, the environment variables for the slack bot token and app token need to be added as environment variables as well:\\n\\nexport SLACK_BOT_TOKEN=... export SLACK_APP_TOKEN=... python chatbot.py\\n\\nRunning this, you should be able to ping the development bot application in the channel you added it to like a user and it will respond to questions by running the RetrievalQA chain that loads relevant context from the vector database and uses an LLM to formulate a nice answer:\\n\\nAll the code can also be found on Github\\n\\n\\n\\nStep 6 - Additional data source: Scrape documentation website\\n\\nGithub issues are helpful, but there is more information we want our development bot to know.\\n\\nThe documentation page for connector development is a very important source of information to answer questions, so it definitely needs to be included. The easiest way to make sure the bot has the same information as what’s published, is to scrape the website. For this case, we are going to use the Apify service to take care of the scraping and turning the website into a nicely structured dataset. This dataset can be extracted using the Airbyte Apify Dataset source connector.\\n\\nFirst, log into Apify and navigate to the store. Choose the “Web Scraper” actor as a basis - it already implements most of the functionality we need\\n\\nNext, create a new task and configure it to scrape all pages of the documentation, extracting the page title and all of the content:\\n\\nSet Start URLs to https://docs.airbyte.com/connector-development/connector-builder-ui/overview/ , the intro page of the documentation linking to other pages\\n\\nSet Link selector to a[href] to follow all links from every page\\n\\nSet Glob Patterns to https://docs.airbyte.com/connector-development/connector-builder-ui/* to limit the scraper to stick to the documentation and not crawl the whole internet\\n\\nConfigure the Page function to extract the page title and the content - in this case the content element can be found using the CSS class name\\n\\n\\n\\n\\n\\nasync function pageFunction(context) { const $ = context.jQuery; const pageTitle = $(\\'title\\').first().text(); const content = $(\\'.markdown\\').first().text(); return { url: context.request.url, pageTitle, content }; }\\n\\nRunning this actor will complete quickly and give us a nicely consumable dataset with a column for the page title and the content:\\n\\nNow it’s time to connect Airbyte to the Apify data set - go to the Airbyte web UI and add your second Source - pick “Apify Dataset”\\n\\nTo set up the Source, you only need to copy the dataset ID that’s shown in the “Storage” tap of the “Run” in the Apify UI\\n\\nOnce the source is set up, follow the same steps as for the Github source to set up a connection moving data from the Apify dataset to the vector store. As the relevant text content is sitting in different fields, you also need to update the vector store destination - add data.pageTitle and data.content to the “text fields” of the destination and save.\\n\\nStep 7 - Additional data source: Fetch Slack messages\\n\\nAnother valuable source of information relevant to connector development are Slack messages from the public help channel. These can be loaded in a very similar fashion. Create a new source using the Slack connector. When using cloud, you can authenticate using the “Authenticate your Slack account” button for simple setup, otherwise follow the instructions in the documentation on the right hand side how to create a Slack “App” with the required permissions and add it to your workspace. To avoid fetching messages from all channels, set the channel name filter to the correct channel.\\n\\nAs for Apify and Github, a new connection needs to be created to move data from Slack to Pinecone. Also add text to the “text fields” of the destination to make sure the relevant data gets embedded properly so similarity searches will yield the right results.\\n\\nIf everything went well, there should be three connections now, all syncing data from their respective sources to the centralized vector store destination using a Pinecone index.\\n\\nBy adjusting the frequency of the connections, you can control how often Airbyte will rerun the connection to make sure the knowledge base of our chat bot stays up to date. As Github and Slack are frequently updated and support efficient incremental updates, it makes sense to set them to a daily frequency or higher. The documentation pages don’t change as often, so they can be kept at a lower frequency or even just be triggered on demand when there are changes.\\n\\nAs we have more sources now, let’s improve our prompt to make sure the LLM has all necessary information to formulate a good answer:\\n\\nclass ConnectorDevelopmentPrompt(PromptTemplate): def format_document(doc: Document, prompt: PromptTemplate) -> str: if doc.metadata[\"_airbyte_stream\"] == \"DatasetItems\": return f\"Excerpt from documentation page: {doc.page_content}\" elif doc.metadata[\"_airbyte_stream\"] == \"issues\": return f\"Excerpt from Github issue: {doc.page_content}, issue number: {doc.metadata[\\'number\\']}, issue state: {doc.metadata[\\'state\\']}\" elif doc.metadata[\"_airbyte_stream\"] == \"threads\" or doc.metadata[\"_airbyte_stream\"] == \"channel_messages\": return f\"Excerpt from Slack thread: {doc.page_content}\" else: return super().format_document(doc, prompt)\\n\\nBy default the RetrievalQA chain retrieves the top 5 matching documents, so if it’s applicable the answer will be based on multiple sources at the same time:\\n\\nConnector development help bot. What do you want to know? > What different authentication methods are supported by the builder? Can I authenticate a login endpoint that returns a session token? The authentication methods supported by the builder are Basic HTTP, Bearer Token, API Key, and OAuth. The builder does not currently support authenticating a login endpoint that returns a session token, but this feature is planned and can be tracked in the Github issue #26341. This issue is not closed yet - the feature might not be shipped yet.\\n\\nThe first sentence about Basic HTTP, Bearer Token, API Key and OAuth is retrieved from the documentation page about authentication, while the second sentence is referring to the same Github issue as before.\\n\\nWrapping up\\n\\nWe covered a lot of ground here - stepping back a bit, we accomplished the following parts:\\n\\nSet up a pipeline that loads unstructured data from multiple sources into a vector database\\n\\nImplement an application that can answer plain text questions about the unstructured data in a general way\\n\\nExpose this application as a Slack bot\\n\\nWith data flowing through this system, Airbyte will make sure the data in your vector database will always be up-to-date while only syncing records that changed in the connected source, minimizing the load on embedding and vector database services while also providing an overview over the current state of running pipelines.\\n\\nThis setup isn’t using a single black box service that encapsulates all the details and leaves us with limited options for tweaking behavior and controlling data processing - instead it’s composed out of multiple components that be easily extended in various places:\\n\\nThe large catalog of Airbyte sources and the connector builder for integrating specialized sources allow to easily load just about any data into a vector db using a single tool\\n\\nLangchain is very extensible and allows you to leverage LLMs in different ways beyond this simple application, including enriching data from other sources, keeping a chat history to be able to have full conversations and more\\n\\nIf you are interested in leveraging Airbyte to ship data to your LLM-based applications, take a moment to fill out our survey so we can make sure to prioritize the most important features.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the bm25 retriever and faiss retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "bm25_retriever.k = 2\n",
    "bm25_retriever.get_relevant_documents(\"langsmith\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_announcing-langsmith_.txt'}, page_content='URL: https://blog.langchain.dev/announcing-langsmith/\\nTitle: Announcing LangSmith, a unified platform for debugging, testing, evaluating, and monitoring your LLM applications\\n\\nLangChain exists to make it as easy as possible to develop LLM-powered applications.\\n\\nWe started with an open-source Python package when the main blocker for building LLM-powered applications was getting a simple prototype working. We remember seeing Nat Friedman tweet in late 2022 that there was “not enough tinkering happening.” The LangChain open-source packages are aimed at addressing this and we see lots of tinkering happening now (Nat agrees)–people are building everything from chatbots over internal company documents to an AI dungeon master for a Dungeons and Dragons game.\\n\\nThe blocker has now changed. While it’s easy to build a prototype of an application in ~5 lines of LangChain code, it’s still deceptively hard to take an application from prototype to production. The main issue that we see today is application performance–something that works ~30% of the time is good enough for a Twitter demo, but not nearly good enough for production.\\n\\nToday, we’re introducing LangSmith, a platform to help developers close the gap between prototype and production. It’s designed for building and iterating on products that can harness the power–and wrangle the complexity–of LLMs.\\n\\nLangSmith is now in closed beta. So if you’re looking for a robust, unified, system for debugging, testing, evaluating, and monitoring your LLM applications, sign up here.\\n\\n\\n\\nHow did we get here?\\n\\nGiven the stochastic nature of LLMs, it is not easy–and there’s currently no straightforward way–to answer the simple question of “what’s happening in these models?,” let alone getting them to work reliably. The builders we hear from are running into the same roadblocks (and it’s true for our team, too):\\n\\nUnderstanding what exactly the final prompt to the LLM call is (after all the prompt template formatting, this final prompt can be long and obfuscated)\\n\\nUnderstanding what exactly is returned from the LLM call at each step (before it is post-processed or transformed in any way)\\n\\nUnderstanding the exact sequence of calls to LLM (or other resources), and how they are chained together\\n\\nTracking token usage\\n\\nManaging costs\\n\\nTracking (and debugging) latency\\n\\nNot having a good dataset to evaluate their application over\\n\\nNot having good metrics with which to evaluate their application\\n\\nUnderstanding how users are interacting with the product\\n\\nAll of these problems have parallels in traditional software engineering. And, in response, a set of practices and tools for debugging, testing, logging, monitoring, etc. has emerged to help developers abstract away common infrastructure and focus on what really matters - building their applications. LLM application developers deserve the same.\\n\\nLangSmith aspires to be that platform. Over the last few months, we’ve been working directly with some early design partners and testing it on our own internal workflows, and we’ve found LangSmith helps teams in 5 core ways:\\n\\nDebugging\\n\\nLangSmith gives you full visibility into model inputs and output of every step in the chain of events. This makes it easy for teams to experiment with new chains and prompt templates, and spot the source of unexpected results, errors, or latency issues. We’ll also expose latency and token usage so that you can identify which calls are causing issues.\\n\\nWe’ve also made it easy to change and rerun examples from the UI. We added this feature after seeing teams take logs of bad examples and copy-paste into the OpenAI playground to tweak the prompt until they got a good result. We wanted to eliminate that friction, and now with the click of a button, you can go from a log to a playground where you can actively edit. This is currently supported for both OpenAI and Anthropic models, with support for more coming soon. We’re also working on supporting this for chains in general.\\n\\nThis deep visibility into model performance has been particularly helpful for teams developing complex applications. LangSmith helped Streamlit and Snowflake implement agents that could intelligently and reliably answer questions about their data.\\n\\n\"LangChain has been instrumental in helping us prototype intelligent agents at Snowflake,” said Adrien Treuille, Director of Product at Snowflake. “LangSmith was easy to integrate, and the agnostic open source API made it very flexible to adapt to our implementation,” tacked on Richard Meng, Senior Software Engineer at Snowflake.\\n\\nBoston Consulting Group also built a highly-customized, and highly performant, series of applications on top of LangChain’s framework by relying on this same infrastructure.\\n\\n“We are proud of being one of the early LangChain design partners and users of LangSmith,” Said Dan Sack, Managing Director and Partner at BCG. “The use of LangSmith has been key to bringing production-ready LLM applications to our clients. LangSmith\\'s ease of integration and intuitive UI enabled us to have an evaluation pipeline up and running very quickly. Additionally, tracing and evaluating the complex agent prompt chains is much easier, reducing the time required to debug and refine our prompts, and giving us the confidence to move to deployment.”\\n\\nIn another example of debugging in action, we partnered with DeepLearningAI to equip learners in the recently-released LangChain courses with access to LangSmith. This allowed students to easily visualize the exact sequence of calls, and the inputs and outputs at each step in the chain with precision. Students can understand exactly what the chains, prompts, and LLMs were doing, which helps build intuition as they learn to create new and more sophisticated applications.\\n\\n\\n\\nTesting\\n\\nOne of the main questions we see developers grapple with is: “If I change this chain/prompt, how does that affect my outputs?” The most effective way to answer this question is to curate a dataset of examples that you care about, and then run any changed prompts/chains over this dataset. LangSmith first makes it easy to create these datasets from traces or by uploading datasets you’ve curated manually. You can then easily run chains and prompts over those data sets.\\n\\nThe first helpful step is simply manually looking at the new inputs and outputs. Although this may seem unsatisfyingly basic, it actually has some benefits - many of the companies we’ve spoken to actually like some manual touch points because it allows them to develop better intuition about how to interact with LLMs. This intuition can prove incredibly valuable when trying to think about how to improve the application. The main unlock we hope to provide is a clear interface for letting developers easily see the inputs and outputs for each data point, as without that visibility they cannot build up that intuition.\\n\\nToday, we primarily hear from teams that want to bring their prototype into production, and are narrowing in on specific prompts they’d like to improve. Klarna is building industry-leading AI integrations that go beyond a simple call to a language model, and instead rely on a series of calls. As they focus on a specific section, LangSmith has provided the tools and data they need to ensure no regressions occur.\\n\\nIn parallel, we’re starting to hear from more and more ambitious teams that are striving for a more effective approach.\\n\\nEvaluating\\n\\nLangSmith integrates seamlessly with our open source collection of evaluation modules. These modules have two main types of evaluation: heuristics and LLMs. Heuristic Evaluations will use logic like regexes to evaluate the correctness of an answer. LLM evaluations will use LLMs to evaluate themselves.\\n\\nWe are extremely bullish on LLM assisted evaluation over the long term. Critics of this approach will say that it’s conceptually shaky and practically costly (time and money). But, we’ve been seeing some very compelling evidence come out of top labs that this is a viable strategy. And, as we collectively make improvements to these models–both private and open source–and usage becomes more ubiquitous, we expect costs to come down considerably.\\n\\nMonitoring\\n\\nWhile debugging, testing, and evaluating can help you get from prototype to production, the work doesn’t stop once you ship. Developers need to actively track performance, and ideally, optimize that performance based on feedback. We consistently see developers relying on LangSmith to track the system-level performance of their application (like latency and cost), track the model/chain performance (through associating feedback with runs), debug issues (diving into a particular run that went wrong), and establish a broad understanding of how users are interacting with their application and what their experience is like.\\n\\nAmbitious startups like Mendable, Multi-On and Quivr, who are already serving thousands of users are actively using LangSmith to not only monitor overall usage, but also use those insights to take action on critical issues.\\n\\n“Thanks to Langchain smith we were able to analyze our LLM calls, understand the performance of the different chain methods ( stuff vs reduce) for QA and improve upon it. It even helped us debug and understand errors we made. We are consistently using it to improve our prompt engineering and look forward to the new features,” said Stan Girard, Head of GenAI at Theodo and creator of Quivr.\\n\\n\\n\\nA unified platform\\n\\nWhile each of these product areas provide unique value, often at a specific point in time in the development process, we believe a great deal of the long term impact of LangSmith will come from having a single, fully-integrated hub to do this work from. We see teams with all kinds of Rube Goldberg-machine-like processes for managing their LLM applications, and we want to make that a thing of the past.\\n\\nAs a very simple example, we considered it to be table stakes for LangSmith to help users easily create datasets from existing logs and use them immediately for testing and evaluation, seamlessly connecting the logging/debugging workflows to the testing/evaluation ones.\\n\\n\\n\\nFintual, a Latin American startup with big dreams to help their citizens build wealth through a personalized financial advisor, found LangSmith early in their LLM development journey. “As soon as we heard about LangSmith, we moved our entire development stack onto it. We could build the evaluation, testing, and monitoring tools we needed in-house. But it would be 1000x worse, take us 10x longer, and require a team 2x the size,” said Fintual leader Jose Pena.\\n\\n“Because we are building financial products, the bar for accuracy, personalization, and security is particularly high. LangSmith helps us build products we are confident putting in front of users.”\\n\\nWe can’t wait to bring these benefits to more teams. And we’ve got a long list of features on the roadmap like analytics, playgrounds, collaboration, in-context learning, prompt creation, and more.\\n\\nFinally, we recognize that we cannot build ALL the functionality you will need to make it easy to make your applications production ready today. We’ve made it possible to export datasets in the format OpenAI evals expects so you can contribute them there. This data can be also used directly to fine tune your models on the Fireworks platform (and we aim to make it easy to plug into other fine-tuning systems as well). Finally, we’ve made logs exportable in a generic format and worked with teams like Context to ensure that you can load them into their analytics engine and run analytics over them in there.\\n\\n\\n\\nWe can’t wait to see what you build.'),\n",
       " Document(metadata={'source': '/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_announcing-langsmith_?ref=commandbar.ghost.io.txt'}, page_content='URL: https://blog.langchain.dev/announcing-langsmith/?ref=commandbar.ghost.io\\nTitle: Announcing LangSmith, a unified platform for debugging, testing, evaluating, and monitoring your LLM applications\\n\\nLangChain exists to make it as easy as possible to develop LLM-powered applications.\\n\\nWe started with an open-source Python package when the main blocker for building LLM-powered applications was getting a simple prototype working. We remember seeing Nat Friedman tweet in late 2022 that there was “not enough tinkering happening.” The LangChain open-source packages are aimed at addressing this and we see lots of tinkering happening now (Nat agrees)–people are building everything from chatbots over internal company documents to an AI dungeon master for a Dungeons and Dragons game.\\n\\nThe blocker has now changed. While it’s easy to build a prototype of an application in ~5 lines of LangChain code, it’s still deceptively hard to take an application from prototype to production. The main issue that we see today is application performance–something that works ~30% of the time is good enough for a Twitter demo, but not nearly good enough for production.\\n\\nToday, we’re introducing LangSmith, a platform to help developers close the gap between prototype and production. It’s designed for building and iterating on products that can harness the power–and wrangle the complexity–of LLMs.\\n\\nLangSmith is now in closed beta. So if you’re looking for a robust, unified, system for debugging, testing, evaluating, and monitoring your LLM applications, sign up here.\\n\\n\\n\\nHow did we get here?\\n\\nGiven the stochastic nature of LLMs, it is not easy–and there’s currently no straightforward way–to answer the simple question of “what’s happening in these models?,” let alone getting them to work reliably. The builders we hear from are running into the same roadblocks (and it’s true for our team, too):\\n\\nUnderstanding what exactly the final prompt to the LLM call is (after all the prompt template formatting, this final prompt can be long and obfuscated)\\n\\nUnderstanding what exactly is returned from the LLM call at each step (before it is post-processed or transformed in any way)\\n\\nUnderstanding the exact sequence of calls to LLM (or other resources), and how they are chained together\\n\\nTracking token usage\\n\\nManaging costs\\n\\nTracking (and debugging) latency\\n\\nNot having a good dataset to evaluate their application over\\n\\nNot having good metrics with which to evaluate their application\\n\\nUnderstanding how users are interacting with the product\\n\\nAll of these problems have parallels in traditional software engineering. And, in response, a set of practices and tools for debugging, testing, logging, monitoring, etc. has emerged to help developers abstract away common infrastructure and focus on what really matters - building their applications. LLM application developers deserve the same.\\n\\nLangSmith aspires to be that platform. Over the last few months, we’ve been working directly with some early design partners and testing it on our own internal workflows, and we’ve found LangSmith helps teams in 5 core ways:\\n\\nDebugging\\n\\nLangSmith gives you full visibility into model inputs and output of every step in the chain of events. This makes it easy for teams to experiment with new chains and prompt templates, and spot the source of unexpected results, errors, or latency issues. We’ll also expose latency and token usage so that you can identify which calls are causing issues.\\n\\nWe’ve also made it easy to change and rerun examples from the UI. We added this feature after seeing teams take logs of bad examples and copy-paste into the OpenAI playground to tweak the prompt until they got a good result. We wanted to eliminate that friction, and now with the click of a button, you can go from a log to a playground where you can actively edit. This is currently supported for both OpenAI and Anthropic models, with support for more coming soon. We’re also working on supporting this for chains in general.\\n\\nThis deep visibility into model performance has been particularly helpful for teams developing complex applications. LangSmith helped Streamlit and Snowflake implement agents that could intelligently and reliably answer questions about their data.\\n\\n\"LangChain has been instrumental in helping us prototype intelligent agents at Snowflake,” said Adrien Treuille, Director of Product at Snowflake. “LangSmith was easy to integrate, and the agnostic open source API made it very flexible to adapt to our implementation,” tacked on Richard Meng, Senior Software Engineer at Snowflake.\\n\\nBoston Consulting Group also built a highly-customized, and highly performant, series of applications on top of LangChain’s framework by relying on this same infrastructure.\\n\\n“We are proud of being one of the early LangChain design partners and users of LangSmith,” Said Dan Sack, Managing Director and Partner at BCG. “The use of LangSmith has been key to bringing production-ready LLM applications to our clients. LangSmith\\'s ease of integration and intuitive UI enabled us to have an evaluation pipeline up and running very quickly. Additionally, tracing and evaluating the complex agent prompt chains is much easier, reducing the time required to debug and refine our prompts, and giving us the confidence to move to deployment.”\\n\\nIn another example of debugging in action, we partnered with DeepLearningAI to equip learners in the recently-released LangChain courses with access to LangSmith. This allowed students to easily visualize the exact sequence of calls, and the inputs and outputs at each step in the chain with precision. Students can understand exactly what the chains, prompts, and LLMs were doing, which helps build intuition as they learn to create new and more sophisticated applications.\\n\\n\\n\\nTesting\\n\\nOne of the main questions we see developers grapple with is: “If I change this chain/prompt, how does that affect my outputs?” The most effective way to answer this question is to curate a dataset of examples that you care about, and then run any changed prompts/chains over this dataset. LangSmith first makes it easy to create these datasets from traces or by uploading datasets you’ve curated manually. You can then easily run chains and prompts over those data sets.\\n\\nThe first helpful step is simply manually looking at the new inputs and outputs. Although this may seem unsatisfyingly basic, it actually has some benefits - many of the companies we’ve spoken to actually like some manual touch points because it allows them to develop better intuition about how to interact with LLMs. This intuition can prove incredibly valuable when trying to think about how to improve the application. The main unlock we hope to provide is a clear interface for letting developers easily see the inputs and outputs for each data point, as without that visibility they cannot build up that intuition.\\n\\nToday, we primarily hear from teams that want to bring their prototype into production, and are narrowing in on specific prompts they’d like to improve. Klarna is building industry-leading AI integrations that go beyond a simple call to a language model, and instead rely on a series of calls. As they focus on a specific section, LangSmith has provided the tools and data they need to ensure no regressions occur.\\n\\nIn parallel, we’re starting to hear from more and more ambitious teams that are striving for a more effective approach.\\n\\nEvaluating\\n\\nLangSmith integrates seamlessly with our open source collection of evaluation modules. These modules have two main types of evaluation: heuristics and LLMs. Heuristic Evaluations will use logic like regexes to evaluate the correctness of an answer. LLM evaluations will use LLMs to evaluate themselves.\\n\\nWe are extremely bullish on LLM assisted evaluation over the long term. Critics of this approach will say that it’s conceptually shaky and practically costly (time and money). But, we’ve been seeing some very compelling evidence come out of top labs that this is a viable strategy. And, as we collectively make improvements to these models–both private and open source–and usage becomes more ubiquitous, we expect costs to come down considerably.\\n\\nMonitoring\\n\\nWhile debugging, testing, and evaluating can help you get from prototype to production, the work doesn’t stop once you ship. Developers need to actively track performance, and ideally, optimize that performance based on feedback. We consistently see developers relying on LangSmith to track the system-level performance of their application (like latency and cost), track the model/chain performance (through associating feedback with runs), debug issues (diving into a particular run that went wrong), and establish a broad understanding of how users are interacting with their application and what their experience is like.\\n\\nAmbitious startups like Mendable, Multi-On and Quivr, who are already serving thousands of users are actively using LangSmith to not only monitor overall usage, but also use those insights to take action on critical issues.\\n\\n“Thanks to Langchain smith we were able to analyze our LLM calls, understand the performance of the different chain methods ( stuff vs reduce) for QA and improve upon it. It even helped us debug and understand errors we made. We are consistently using it to improve our prompt engineering and look forward to the new features,” said Stan Girard, Head of GenAI at Theodo and creator of Quivr.\\n\\n\\n\\nA unified platform\\n\\nWhile each of these product areas provide unique value, often at a specific point in time in the development process, we believe a great deal of the long term impact of LangSmith will come from having a single, fully-integrated hub to do this work from. We see teams with all kinds of Rube Goldberg-machine-like processes for managing their LLM applications, and we want to make that a thing of the past.\\n\\nAs a very simple example, we considered it to be table stakes for LangSmith to help users easily create datasets from existing logs and use them immediately for testing and evaluation, seamlessly connecting the logging/debugging workflows to the testing/evaluation ones.\\n\\n\\n\\nFintual, a Latin American startup with big dreams to help their citizens build wealth through a personalized financial advisor, found LangSmith early in their LLM development journey. “As soon as we heard about LangSmith, we moved our entire development stack onto it. We could build the evaluation, testing, and monitoring tools we needed in-house. But it would be 1000x worse, take us 10x longer, and require a team 2x the size,” said Fintual leader Jose Pena.\\n\\n“Because we are building financial products, the bar for accuracy, personalization, and security is particularly high. LangSmith helps us build products we are confident putting in front of users.”\\n\\nWe can’t wait to bring these benefits to more teams. And we’ve got a long list of features on the roadmap like analytics, playgrounds, collaboration, in-context learning, prompt creation, and more.\\n\\nFinally, we recognize that we cannot build ALL the functionality you will need to make it easy to make your applications production ready today. We’ve made it possible to export datasets in the format OpenAI evals expects so you can contribute them there. This data can be also used directly to fine tune your models on the Fireworks platform (and we aim to make it easy to plug into other fine-tuning systems as well). Finally, we’ve made logs exportable in a generic format and worked with teams like Context to ensure that you can load them into their analytics engine and run analytics over them in there.\\n\\n\\n\\nWe can’t wait to see what you build.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faiss_vectorstore = FAISS.from_documents(docs, embedding)\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "faiss_retriever.get_relevant_documents(\"langsmith\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever],\n",
    "                                       weights=[0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ensemble_retriever.get_relevant_documents(\"Integration of Langsmith\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_integrating-chatgpt-with-google-drive-and-notion-data_.txt'}, page_content='URL: https://blog.langchain.dev/integrating-chatgpt-with-google-drive-and-notion-data/\\nTitle: Tavrn x LangChain: Integrating Noah: ChatGPT with Google Drive and Notion data\\n\\nEditor\\'s Note: This post was written in collaboration with the Tavrn team. They were able to build a new personal assistant app, Noah, that\\'s highly personalized and highly context-aware using LangChain (with some interesting retrieval tactics) and LangSmith (for fine-tuning chains and prompts).\\n\\nChatGPT is already an indispensable tool for many in the workplace. Its impressive general purpose performance makes it extremely versatile to assist in workflows ranging from creative brainstorming to coding. In order to get the best outputs from ChatGPT, users are familiar with the process of prompting - providing the chat with as much context and instructions as possible so the output is satisfactory.\\n\\n\\n\\nThe POV of this laborious user experience usually involves multiple rounds of copy/pasting of parts of multiple documents that contain relevant information to the prompt. Given that ChatGPT has no context whatsoever on the user or his work, the output quality highly depends on information the user provides. For instance, if a user wants ChatGPT\\'s best help to prioritize which product features to build first, he will have to:\\n\\n\\n\\n1. Find and open all documents that could potentially have useful context to ChatGPT (e.g. product meeting notes, user feedback reports, information about the product itself)\\n\\n\\n\\n2. Read through each document, copy the relevant parts and paste on ChatGPT\\n\\n\\n\\n3. Hope it all fits the character limit of ChatGPT and that he did not forget to include any important context\\n\\n\\n\\nThis inefficient, manual process of always having to supply the best context to ChatGPT prevents users from utilizing it for more complex use cases like the one illustrated above. We built Noah to resolve the context fetching problem and allow users to experience an AI copilot that always efficiently retrieves the best possible context to answer user queries.\\n\\n\\n\\nSimplicity and user-friendliness are core to Noah. We take care of all the heavy-lifting in the background. In just a few clicks, users can sync hundreds of files from their own Google Drive and Notion and start getting help from Noah.\\n\\n\\n\\nPowered by LangChain, Noah unlocks a more powerful and relevant use of LLMs in the workplace: a personal AI assistant that provides help specifically to users and their work. In the product prioritization example above, Noah would take care of all three steps (finding relevant documents, selecting the most relevant parts in each document, adding each part to the LLM prompt) so the user\\'s sole input to the chat can be \"which product features should I prioritize?\"\\n\\n\\n\\nTo get started on Noah, users select the tools from which they would like to sync files.\\n\\nAfter the tool is selected, users can either choose specific files or quickly select their 200 most recent files from Google Drive or Notion.\\n\\n\\n\\nOnce users select their files, Noah processes the documents using optimized, context-aware document loaders in addition to state-of-the-art embeddings models. We tried multiple forms of semantic chunking but LangChain\\'s CharacterTextSplitter with around 2,400 characters per chunk outperformed all the others for all types of documents - spreadsheets, documents, PDFs, slides.\\n\\n\\n\\nThen, once a user asks a question, Noah fetches the most relevant content across multiple sources utilizing cosine similarity vector search and passes them to multi-chain LLM calls where the best possible answer is obtained. We also tried other forms of retrieval but cosine similarity substantially outperformed the others.\\n\\n\\n\\n\\n\\nLangsmith was extra useful to us when fine-tuning which chains and prompts to use for the final user answer. Among the learnings, the optimal memory \"k\" parameter for ConversationBufferWindowMemory is 1 otherwise the answers get unreliable with so much historical context. Additionally, after the chunks are retrieved, we pass them into an intermediary, GPT-4 powered chain to filter out any conflicting information, prioritizing more recent sources.\\n\\n\\n\\nFinally, Noah provides the answer, with the appropriate sources cited.\\n\\n\\n\\nTo get started with Noah and boost your productivity, access https://tavrn.art/noah.\\n\\n\\n\\n\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': '/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_announcing-langsmith_?ref=commandbar.ghost.io.txt'}, page_content='URL: https://blog.langchain.dev/announcing-langsmith/?ref=commandbar.ghost.io\\nTitle: Announcing LangSmith, a unified platform for debugging, testing, evaluating, and monitoring your LLM applications\\n\\nLangChain exists to make it as easy as possible to develop LLM-powered applications.\\n\\nWe started with an open-source Python package when the main blocker for building LLM-powered applications was getting a simple prototype working. We remember seeing Nat Friedman tweet in late 2022 that there was “not enough tinkering happening.” The LangChain open-source packages are aimed at addressing this and we see lots of tinkering happening now (Nat agrees)–people are building everything from chatbots over internal company documents to an AI dungeon master for a Dungeons and Dragons game.\\n\\nThe blocker has now changed. While it’s easy to build a prototype of an application in ~5 lines of LangChain code, it’s still deceptively hard to take an application from prototype to production. The main issue that we see today is application performance–something that works ~30% of the time is good enough for a Twitter demo, but not nearly good enough for production.\\n\\nToday, we’re introducing LangSmith, a platform to help developers close the gap between prototype and production. It’s designed for building and iterating on products that can harness the power–and wrangle the complexity–of LLMs.\\n\\nLangSmith is now in closed beta. So if you’re looking for a robust, unified, system for debugging, testing, evaluating, and monitoring your LLM applications, sign up here.\\n\\n\\n\\nHow did we get here?\\n\\nGiven the stochastic nature of LLMs, it is not easy–and there’s currently no straightforward way–to answer the simple question of “what’s happening in these models?,” let alone getting them to work reliably. The builders we hear from are running into the same roadblocks (and it’s true for our team, too):\\n\\nUnderstanding what exactly the final prompt to the LLM call is (after all the prompt template formatting, this final prompt can be long and obfuscated)\\n\\nUnderstanding what exactly is returned from the LLM call at each step (before it is post-processed or transformed in any way)\\n\\nUnderstanding the exact sequence of calls to LLM (or other resources), and how they are chained together\\n\\nTracking token usage\\n\\nManaging costs\\n\\nTracking (and debugging) latency\\n\\nNot having a good dataset to evaluate their application over\\n\\nNot having good metrics with which to evaluate their application\\n\\nUnderstanding how users are interacting with the product\\n\\nAll of these problems have parallels in traditional software engineering. And, in response, a set of practices and tools for debugging, testing, logging, monitoring, etc. has emerged to help developers abstract away common infrastructure and focus on what really matters - building their applications. LLM application developers deserve the same.\\n\\nLangSmith aspires to be that platform. Over the last few months, we’ve been working directly with some early design partners and testing it on our own internal workflows, and we’ve found LangSmith helps teams in 5 core ways:\\n\\nDebugging\\n\\nLangSmith gives you full visibility into model inputs and output of every step in the chain of events. This makes it easy for teams to experiment with new chains and prompt templates, and spot the source of unexpected results, errors, or latency issues. We’ll also expose latency and token usage so that you can identify which calls are causing issues.\\n\\nWe’ve also made it easy to change and rerun examples from the UI. We added this feature after seeing teams take logs of bad examples and copy-paste into the OpenAI playground to tweak the prompt until they got a good result. We wanted to eliminate that friction, and now with the click of a button, you can go from a log to a playground where you can actively edit. This is currently supported for both OpenAI and Anthropic models, with support for more coming soon. We’re also working on supporting this for chains in general.\\n\\nThis deep visibility into model performance has been particularly helpful for teams developing complex applications. LangSmith helped Streamlit and Snowflake implement agents that could intelligently and reliably answer questions about their data.\\n\\n\"LangChain has been instrumental in helping us prototype intelligent agents at Snowflake,” said Adrien Treuille, Director of Product at Snowflake. “LangSmith was easy to integrate, and the agnostic open source API made it very flexible to adapt to our implementation,” tacked on Richard Meng, Senior Software Engineer at Snowflake.\\n\\nBoston Consulting Group also built a highly-customized, and highly performant, series of applications on top of LangChain’s framework by relying on this same infrastructure.\\n\\n“We are proud of being one of the early LangChain design partners and users of LangSmith,” Said Dan Sack, Managing Director and Partner at BCG. “The use of LangSmith has been key to bringing production-ready LLM applications to our clients. LangSmith\\'s ease of integration and intuitive UI enabled us to have an evaluation pipeline up and running very quickly. Additionally, tracing and evaluating the complex agent prompt chains is much easier, reducing the time required to debug and refine our prompts, and giving us the confidence to move to deployment.”\\n\\nIn another example of debugging in action, we partnered with DeepLearningAI to equip learners in the recently-released LangChain courses with access to LangSmith. This allowed students to easily visualize the exact sequence of calls, and the inputs and outputs at each step in the chain with precision. Students can understand exactly what the chains, prompts, and LLMs were doing, which helps build intuition as they learn to create new and more sophisticated applications.\\n\\n\\n\\nTesting\\n\\nOne of the main questions we see developers grapple with is: “If I change this chain/prompt, how does that affect my outputs?” The most effective way to answer this question is to curate a dataset of examples that you care about, and then run any changed prompts/chains over this dataset. LangSmith first makes it easy to create these datasets from traces or by uploading datasets you’ve curated manually. You can then easily run chains and prompts over those data sets.\\n\\nThe first helpful step is simply manually looking at the new inputs and outputs. Although this may seem unsatisfyingly basic, it actually has some benefits - many of the companies we’ve spoken to actually like some manual touch points because it allows them to develop better intuition about how to interact with LLMs. This intuition can prove incredibly valuable when trying to think about how to improve the application. The main unlock we hope to provide is a clear interface for letting developers easily see the inputs and outputs for each data point, as without that visibility they cannot build up that intuition.\\n\\nToday, we primarily hear from teams that want to bring their prototype into production, and are narrowing in on specific prompts they’d like to improve. Klarna is building industry-leading AI integrations that go beyond a simple call to a language model, and instead rely on a series of calls. As they focus on a specific section, LangSmith has provided the tools and data they need to ensure no regressions occur.\\n\\nIn parallel, we’re starting to hear from more and more ambitious teams that are striving for a more effective approach.\\n\\nEvaluating\\n\\nLangSmith integrates seamlessly with our open source collection of evaluation modules. These modules have two main types of evaluation: heuristics and LLMs. Heuristic Evaluations will use logic like regexes to evaluate the correctness of an answer. LLM evaluations will use LLMs to evaluate themselves.\\n\\nWe are extremely bullish on LLM assisted evaluation over the long term. Critics of this approach will say that it’s conceptually shaky and practically costly (time and money). But, we’ve been seeing some very compelling evidence come out of top labs that this is a viable strategy. And, as we collectively make improvements to these models–both private and open source–and usage becomes more ubiquitous, we expect costs to come down considerably.\\n\\nMonitoring\\n\\nWhile debugging, testing, and evaluating can help you get from prototype to production, the work doesn’t stop once you ship. Developers need to actively track performance, and ideally, optimize that performance based on feedback. We consistently see developers relying on LangSmith to track the system-level performance of their application (like latency and cost), track the model/chain performance (through associating feedback with runs), debug issues (diving into a particular run that went wrong), and establish a broad understanding of how users are interacting with their application and what their experience is like.\\n\\nAmbitious startups like Mendable, Multi-On and Quivr, who are already serving thousands of users are actively using LangSmith to not only monitor overall usage, but also use those insights to take action on critical issues.\\n\\n“Thanks to Langchain smith we were able to analyze our LLM calls, understand the performance of the different chain methods ( stuff vs reduce) for QA and improve upon it. It even helped us debug and understand errors we made. We are consistently using it to improve our prompt engineering and look forward to the new features,” said Stan Girard, Head of GenAI at Theodo and creator of Quivr.\\n\\n\\n\\nA unified platform\\n\\nWhile each of these product areas provide unique value, often at a specific point in time in the development process, we believe a great deal of the long term impact of LangSmith will come from having a single, fully-integrated hub to do this work from. We see teams with all kinds of Rube Goldberg-machine-like processes for managing their LLM applications, and we want to make that a thing of the past.\\n\\nAs a very simple example, we considered it to be table stakes for LangSmith to help users easily create datasets from existing logs and use them immediately for testing and evaluation, seamlessly connecting the logging/debugging workflows to the testing/evaluation ones.\\n\\n\\n\\nFintual, a Latin American startup with big dreams to help their citizens build wealth through a personalized financial advisor, found LangSmith early in their LLM development journey. “As soon as we heard about LangSmith, we moved our entire development stack onto it. We could build the evaluation, testing, and monitoring tools we needed in-house. But it would be 1000x worse, take us 10x longer, and require a team 2x the size,” said Fintual leader Jose Pena.\\n\\n“Because we are building financial products, the bar for accuracy, personalization, and security is particularly high. LangSmith helps us build products we are confident putting in front of users.”\\n\\nWe can’t wait to bring these benefits to more teams. And we’ve got a long list of features on the roadmap like analytics, playgrounds, collaboration, in-context learning, prompt creation, and more.\\n\\nFinally, we recognize that we cannot build ALL the functionality you will need to make it easy to make your applications production ready today. We’ve made it possible to export datasets in the format OpenAI evals expects so you can contribute them there. This data can be also used directly to fine tune your models on the Fireworks platform (and we aim to make it easy to plug into other fine-tuning systems as well). Finally, we’ve made logs exportable in a generic format and worked with teams like Context to ensure that you can load them into their analytics engine and run analytics over them in there.\\n\\n\\n\\nWe can’t wait to see what you build.'),\n",
       " Document(metadata={'source': '/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_multion-x-langchain-powering-next-gen-web-automation-navigation-with-ai_.txt'}, page_content='URL: https://blog.langchain.dev/multion-x-langchain-powering-next-gen-web-automation-navigation-with-ai/\\nTitle: MultiOn x LangChain: Powering Next-Gen Web Automation & Navigation with AI\\n\\nEditor\\'s Note: This post was written in collaboration with MultiOn. We\\'re really excited about the way they\\'re using Agents to automate and streamline online interactions. They are one of the first real world, production agent applications that we know of. Their integration with LangChain as a Toolkit makes it quick and easy to personalize and automate everyday web tasks.\\n\\nMultiOn: Your Personal AI Agent Now on LangChain\\n\\nWhether it\\'s searching for information, filling out forms, or navigating complex websites, daily web tasks can often be tedious and time-consuming. That\\'s why we\\'re thrilled to introduce MultiOn, a next-generation personal AI assistant designed to interact with the web, to handle these tasks on your behalf.\\n\\nOperating much like the sci-fi concept of JARVIS, MultiOn leverages cutting-edge AI technology to interact with your browser to perform tasks for you in real-time, from ordering you dinner, booking flights, scheduling, finding information online, to even filling out forms. And the best part? MultiOn is now integrated directly within LangChain as a Toolkit, making it even easier to automate your everyday web tasks & build custom agents and applications that can take actions on the web.\\n\\nSeamless Integration with LangChain\\n\\nWith MultiOn directly integrated into LangChain, the power of Autonomous Web AI Agents is now at the fingertips of all users.\\n\\nThe integration unlocks numerous advantages. It provides LangChain users with an AI-powered tool that can automate a variety of everyday web tasks, from information retrieval to interaction with web services on their behalf. This integration not only enhances the functionality of LangChain but also takes the Action ability of agents to the next level - to now interact with any website!\\n\\nHere is a glimpse of how you can use MultiOn within LangChain to interact with the website in just 3 Lines of Code 🔥:\\n\\nImport MultiOn as a LangChain Toolkit to add it to any custom Agent:\\n\\n\\n\\n# IMPORTS from langchain import OpenAI from langchain.agents import initialize_agent, AgentType from langchain.agents.agent_toolkits import MultionToolkit import multion multion.login() # MultiOn -> Login to the MultiOn Website # Initialize Agent agent = initialize_agent( tools=MultionToolkit().get_tools(), llm=OpenAI(temperature=0), agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose = True ) print(agent.run(\"Show Beautiful Pictures of New York\"))\\n\\nGet more samples at the MultiOn API repository.\\n\\n\\n\\nLangChain Agent Demo:\\n\\nOther\\n\\nMultiOn Scheduler App: Schedule recurring tasks that run periodically, such as “wishing happy birthday to friends on fb” everyday.\\n\\nGroup Dinner reservation Agent: Add MultiOn to a sms group chat and ask it to help book a group dinner on Opentable\\n\\nJoin the MultiOn Community!\\n\\n\\n\\nWe’re very enthusiastic about the potential for Autonomous Web AI Agents, and more broadly, exploring new ways to harness the power of AI to improve online experiences. We believe that Actions are key to building powerful AI applications, and we want to empower developers & the open source community to build AI that can interact with the Web by building on top of MultiOn. Please check our documentation, contribute to adding examples, and join our Discord to experience the future of web task automation!\\n\\nStay tuned for more updates on our journey, and don\\'t hesitate to reach us out at info@multion.ai if you have any questions or suggestions. We\\'re always looking to hear from users and improve MultiOn to best serve your needs 🚀'),\n",
       " Document(metadata={'source': '/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/blog_post/blog.langchain.dev_announcing-langsmith_.txt'}, page_content='URL: https://blog.langchain.dev/announcing-langsmith/\\nTitle: Announcing LangSmith, a unified platform for debugging, testing, evaluating, and monitoring your LLM applications\\n\\nLangChain exists to make it as easy as possible to develop LLM-powered applications.\\n\\nWe started with an open-source Python package when the main blocker for building LLM-powered applications was getting a simple prototype working. We remember seeing Nat Friedman tweet in late 2022 that there was “not enough tinkering happening.” The LangChain open-source packages are aimed at addressing this and we see lots of tinkering happening now (Nat agrees)–people are building everything from chatbots over internal company documents to an AI dungeon master for a Dungeons and Dragons game.\\n\\nThe blocker has now changed. While it’s easy to build a prototype of an application in ~5 lines of LangChain code, it’s still deceptively hard to take an application from prototype to production. The main issue that we see today is application performance–something that works ~30% of the time is good enough for a Twitter demo, but not nearly good enough for production.\\n\\nToday, we’re introducing LangSmith, a platform to help developers close the gap between prototype and production. It’s designed for building and iterating on products that can harness the power–and wrangle the complexity–of LLMs.\\n\\nLangSmith is now in closed beta. So if you’re looking for a robust, unified, system for debugging, testing, evaluating, and monitoring your LLM applications, sign up here.\\n\\n\\n\\nHow did we get here?\\n\\nGiven the stochastic nature of LLMs, it is not easy–and there’s currently no straightforward way–to answer the simple question of “what’s happening in these models?,” let alone getting them to work reliably. The builders we hear from are running into the same roadblocks (and it’s true for our team, too):\\n\\nUnderstanding what exactly the final prompt to the LLM call is (after all the prompt template formatting, this final prompt can be long and obfuscated)\\n\\nUnderstanding what exactly is returned from the LLM call at each step (before it is post-processed or transformed in any way)\\n\\nUnderstanding the exact sequence of calls to LLM (or other resources), and how they are chained together\\n\\nTracking token usage\\n\\nManaging costs\\n\\nTracking (and debugging) latency\\n\\nNot having a good dataset to evaluate their application over\\n\\nNot having good metrics with which to evaluate their application\\n\\nUnderstanding how users are interacting with the product\\n\\nAll of these problems have parallels in traditional software engineering. And, in response, a set of practices and tools for debugging, testing, logging, monitoring, etc. has emerged to help developers abstract away common infrastructure and focus on what really matters - building their applications. LLM application developers deserve the same.\\n\\nLangSmith aspires to be that platform. Over the last few months, we’ve been working directly with some early design partners and testing it on our own internal workflows, and we’ve found LangSmith helps teams in 5 core ways:\\n\\nDebugging\\n\\nLangSmith gives you full visibility into model inputs and output of every step in the chain of events. This makes it easy for teams to experiment with new chains and prompt templates, and spot the source of unexpected results, errors, or latency issues. We’ll also expose latency and token usage so that you can identify which calls are causing issues.\\n\\nWe’ve also made it easy to change and rerun examples from the UI. We added this feature after seeing teams take logs of bad examples and copy-paste into the OpenAI playground to tweak the prompt until they got a good result. We wanted to eliminate that friction, and now with the click of a button, you can go from a log to a playground where you can actively edit. This is currently supported for both OpenAI and Anthropic models, with support for more coming soon. We’re also working on supporting this for chains in general.\\n\\nThis deep visibility into model performance has been particularly helpful for teams developing complex applications. LangSmith helped Streamlit and Snowflake implement agents that could intelligently and reliably answer questions about their data.\\n\\n\"LangChain has been instrumental in helping us prototype intelligent agents at Snowflake,” said Adrien Treuille, Director of Product at Snowflake. “LangSmith was easy to integrate, and the agnostic open source API made it very flexible to adapt to our implementation,” tacked on Richard Meng, Senior Software Engineer at Snowflake.\\n\\nBoston Consulting Group also built a highly-customized, and highly performant, series of applications on top of LangChain’s framework by relying on this same infrastructure.\\n\\n“We are proud of being one of the early LangChain design partners and users of LangSmith,” Said Dan Sack, Managing Director and Partner at BCG. “The use of LangSmith has been key to bringing production-ready LLM applications to our clients. LangSmith\\'s ease of integration and intuitive UI enabled us to have an evaluation pipeline up and running very quickly. Additionally, tracing and evaluating the complex agent prompt chains is much easier, reducing the time required to debug and refine our prompts, and giving us the confidence to move to deployment.”\\n\\nIn another example of debugging in action, we partnered with DeepLearningAI to equip learners in the recently-released LangChain courses with access to LangSmith. This allowed students to easily visualize the exact sequence of calls, and the inputs and outputs at each step in the chain with precision. Students can understand exactly what the chains, prompts, and LLMs were doing, which helps build intuition as they learn to create new and more sophisticated applications.\\n\\n\\n\\nTesting\\n\\nOne of the main questions we see developers grapple with is: “If I change this chain/prompt, how does that affect my outputs?” The most effective way to answer this question is to curate a dataset of examples that you care about, and then run any changed prompts/chains over this dataset. LangSmith first makes it easy to create these datasets from traces or by uploading datasets you’ve curated manually. You can then easily run chains and prompts over those data sets.\\n\\nThe first helpful step is simply manually looking at the new inputs and outputs. Although this may seem unsatisfyingly basic, it actually has some benefits - many of the companies we’ve spoken to actually like some manual touch points because it allows them to develop better intuition about how to interact with LLMs. This intuition can prove incredibly valuable when trying to think about how to improve the application. The main unlock we hope to provide is a clear interface for letting developers easily see the inputs and outputs for each data point, as without that visibility they cannot build up that intuition.\\n\\nToday, we primarily hear from teams that want to bring their prototype into production, and are narrowing in on specific prompts they’d like to improve. Klarna is building industry-leading AI integrations that go beyond a simple call to a language model, and instead rely on a series of calls. As they focus on a specific section, LangSmith has provided the tools and data they need to ensure no regressions occur.\\n\\nIn parallel, we’re starting to hear from more and more ambitious teams that are striving for a more effective approach.\\n\\nEvaluating\\n\\nLangSmith integrates seamlessly with our open source collection of evaluation modules. These modules have two main types of evaluation: heuristics and LLMs. Heuristic Evaluations will use logic like regexes to evaluate the correctness of an answer. LLM evaluations will use LLMs to evaluate themselves.\\n\\nWe are extremely bullish on LLM assisted evaluation over the long term. Critics of this approach will say that it’s conceptually shaky and practically costly (time and money). But, we’ve been seeing some very compelling evidence come out of top labs that this is a viable strategy. And, as we collectively make improvements to these models–both private and open source–and usage becomes more ubiquitous, we expect costs to come down considerably.\\n\\nMonitoring\\n\\nWhile debugging, testing, and evaluating can help you get from prototype to production, the work doesn’t stop once you ship. Developers need to actively track performance, and ideally, optimize that performance based on feedback. We consistently see developers relying on LangSmith to track the system-level performance of their application (like latency and cost), track the model/chain performance (through associating feedback with runs), debug issues (diving into a particular run that went wrong), and establish a broad understanding of how users are interacting with their application and what their experience is like.\\n\\nAmbitious startups like Mendable, Multi-On and Quivr, who are already serving thousands of users are actively using LangSmith to not only monitor overall usage, but also use those insights to take action on critical issues.\\n\\n“Thanks to Langchain smith we were able to analyze our LLM calls, understand the performance of the different chain methods ( stuff vs reduce) for QA and improve upon it. It even helped us debug and understand errors we made. We are consistently using it to improve our prompt engineering and look forward to the new features,” said Stan Girard, Head of GenAI at Theodo and creator of Quivr.\\n\\n\\n\\nA unified platform\\n\\nWhile each of these product areas provide unique value, often at a specific point in time in the development process, we believe a great deal of the long term impact of LangSmith will come from having a single, fully-integrated hub to do this work from. We see teams with all kinds of Rube Goldberg-machine-like processes for managing their LLM applications, and we want to make that a thing of the past.\\n\\nAs a very simple example, we considered it to be table stakes for LangSmith to help users easily create datasets from existing logs and use them immediately for testing and evaluation, seamlessly connecting the logging/debugging workflows to the testing/evaluation ones.\\n\\n\\n\\nFintual, a Latin American startup with big dreams to help their citizens build wealth through a personalized financial advisor, found LangSmith early in their LLM development journey. “As soon as we heard about LangSmith, we moved our entire development stack onto it. We could build the evaluation, testing, and monitoring tools we needed in-house. But it would be 1000x worse, take us 10x longer, and require a team 2x the size,” said Fintual leader Jose Pena.\\n\\n“Because we are building financial products, the bar for accuracy, personalization, and security is particularly high. LangSmith helps us build products we are confident putting in front of users.”\\n\\nWe can’t wait to bring these benefits to more teams. And we’ve got a long list of features on the roadmap like analytics, playgrounds, collaboration, in-context learning, prompt creation, and more.\\n\\nFinally, we recognize that we cannot build ALL the functionality you will need to make it easy to make your applications production ready today. We’ve made it possible to export datasets in the format OpenAI evals expects so you can contribute them there. This data can be also used directly to fine tune your models on the Fireworks platform (and we aim to make it easy to plug into other fine-tuning systems as well). Finally, we’ve made logs exportable in a generic format and worked with teams like Context to ensure that you can load them into their analytics engine and run analytics over them in there.\\n\\n\\n\\nWe can’t wait to see what you build.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
