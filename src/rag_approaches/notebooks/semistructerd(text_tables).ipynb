{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi-structured RAG (Tables and Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text and tables are can be found in many documents.Conventional RAG may have difficulties when dealing with semi-structured data for two main reasons which are text splitting could fracture tables and contaminate the data during retrieval and using embedded tables in semantic similarity searches could provide difficulties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heliya/Desktop/rag_approaches/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from unstructured.partition.pdf import partition_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Get elements\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=\"/home/heliya/Desktop/rag_approaches/src/rag_approaches/dataset/llama.pdf\",\n",
    "    # Unstructured first finds embedded image blocks\n",
    "    extract_images_in_pdf=False,\n",
    "    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
    "    # Titles are any sub-section of the document\n",
    "    infer_table_structure=True,\n",
    "    # Post processing to aggregate text once we have the title\n",
    "    chunking_strategy=\"by_title\",\n",
    "    # Chunking params to aggregate text blocks\n",
    "    # Attempt to create a new chunk 3800 chars\n",
    "    # Attempt to keep chunks > 2000 chars\n",
    "    max_characters=4000,\n",
    "    new_after_n_chars=3800,\n",
    "    combine_text_under_n_chars=2000,\n",
    "    image_output_dir_path=\"/home/heliya/Desktop/rag_approaches/src/rag_approaches/images\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"<class 'unstructured.documents.elements.CompositeElement'>\": 39,\n",
       " \"<class 'unstructured.documents.elements.Table'>\": 16,\n",
       " \"<class 'unstructured.documents.elements.TableChunk'>\": 3}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary to store counts of each type\n",
    "category_counts = {}\n",
    "\n",
    "for element in raw_pdf_elements:\n",
    "    category = str(type(element))\n",
    "    if category in category_counts:\n",
    "        category_counts[category] += 1\n",
    "    else:\n",
    "        category_counts[category] = 1\n",
    "\n",
    "# Unique_categories will have unique elements\n",
    "unique_categories = set(category_counts.keys())\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "class Element(BaseModel):\n",
    "    type: str\n",
    "    text: Any\n",
    "\n",
    "\n",
    "# Categorize by type\n",
    "categorized_elements = []\n",
    "for element in raw_pdf_elements:\n",
    "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"table\", text=str(element)))\n",
    "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"text\", text=str(element)))\n",
    "\n",
    "# Tables\n",
    "table_elements = [e for e in categorized_elements if e.type == \"table\"]\n",
    "print(len(table_elements))\n",
    "\n",
    "\n",
    "# Text\n",
    "text_elements = [e for e in categorized_elements if e.type == \"text\"]\n",
    "print(len(text_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text. \\ \n",
    "Give a concise summary of the table or text. Table or text chunk: {element} \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# Summary chain\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to tables\n",
    "tables = [i.text for i in table_elements]\n",
    "table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to texts\n",
    "texts = [i.text for i in text_elements]\n",
    "text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The table provides information about different datasets and their properties. The CommonCrawl dataset has the highest sampling proportion at 67.0% and the largest disk size at 3.3 TB. The StackExchange dataset has the lowest sampling proportion at 2.0% and a disk size of 78 GB. The other datasets, C4, Github, Wikipedia, Books, and ArXiv, have sampling proportions ranging from 15.0% to 2.5% and disk sizes ranging from 783 GB to 83 GB.',\n",
       " 'The table presents data on four different models with varying dimensions, heads, layers, and learning rates. The first model has a dimension of 4096, 32 heads, 32 layers, and a learning rate of 3.0e−4. The second model has a dimension of 5120, 40 heads, 40 layers, and the same learning rate as the first. The third model has a dimension of 6656, 52 heads, 60 layers, and a learning rate of 1.5e−4. The fourth model has a dimension of 8192, 64 heads, 80 layers, and the same learning rate as the third. All models have 4M parameters and a model size ranging from 1.0T to 1.4T.',\n",
       " 'The text appears to be a list of scores for different models or systems, possibly related to AI or machine learning. The models include BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC-e, ARC-c, OBQA, GPT-3, Gopher, Chinchilla, PaLM, and LLaMA. The scores range from as low as 47.6 to as high as 88.0. The highest score is achieved by the ARC-e model. The GPT-3, Gopher, and Chinchilla models also perform well, with scores above 80. The LLaMA model has varying scores, with the highest being 83.1.',\n",
       " 'The table presents the performance of different AI models (GPT-3, Gopher Chinchilla, PaLM, and LLaMA) with varying sizes (from 7B to 540B) across different shot scenarios (0-shot, 1-shot, 5-shot, 64-shot). The performance is likely measured by a certain metric, with higher values indicating better performance. GPT-3 and Gopher Chinchilla have fewer data points, while PaLM and LLaMA have more comprehensive data across different shot scenarios. The highest performance is observed in LLaMA with 39.9 in the 64-shot scenario.',\n",
       " \"The table presents the performance of three models: Gopher, Chinchilla, and LLaMA, across different training sizes (0-shot, 1-shot, 5-shot, 64-shot). Gopher has a performance of 43.5 at 280B. Chinchilla's performance ranges from 55.4 at 70B to 64.6 at 64-shot. LLaMA shows a progressive increase in performance from 50.0 at 7B to 73.0 at 70.4-shot.\",\n",
       " 'The table compares the performance of three AI models: GPT-3, PaLM, and LLaMA across different parameters. GPT-3 with 175B parameters scored 58.4 on RACE-middle and 45.5 on RACE-high. PaLM showed improvement with increasing parameters, scoring 57.9, 64.3, and 68.1 on RACE-middle and 42.3, 47.5, and 49.1 on RACE-high for 8B, 62B, and 540B parameters respectively. LLaMA also showed improvement with increasing parameters, scoring 61.1, 61.6, 64.1, and 67.9 on RACE-middle and 46.9, 47.2, 48.3, and 51.6 on RACE-high for 7B, 13B, 33B, and 65B parameters respectively.',\n",
       " 'The table presents data on three different models: MATH +maj1@k GSM8k +maj1@k PaLM, Minerva, and LLaMA. Each model is evaluated at different stages, represented by 8B, 62B, and 540B. The MATH model shows a steady increase from 1.5 to 8.8, but lacks data in some areas. The Minerva model has a more significant increase, starting at 14.1 and reaching 78.5. The LLaMA model has a more gradual increase, starting at 2.9 and reaching 69.7. The table also includes additional data points for each model, showing a range of performance metrics.',\n",
       " 'The table presents the performance of different models (LaMDA, PaLM, PaLM-cont, LLaMA) on the HumanEval and MBPP tasks. The performance is measured at different parameters (@1, @100, @80) and different model sizes (8B, 62B, 540B, 137B, 7B, 13B, 33B, 65B). The results vary, with some models performing better at certain tasks and parameters than others. For instance, PaLM model with 62B size performs best at HumanEval @1 with a score of 23.7, while LLaMA model with 65B size performs best at MBPP @80 with a score of 79.3.',\n",
       " 'The table presents the performance of different AI models (GPT-NeoX, GPT-3, Gopher, Chinchilla, PaLM, and LLaMA) across various fields (Humanities, STEM, Social Sciences, and Other). The performance is measured in terms of model size (from 7B to 540B). GPT-3 and Gopher models with 280B and 70B sizes respectively, show the highest performance in all fields. The PaLM model with 540B size also shows high performance, especially in Social Sciences. The LLaMA model performs consistently across all fields with its performance increasing with the model size.',\n",
       " 'The table presents the performance of various models, including OPT GLM, PaLM, PaLM-cont, Chinchilla, LLaMA, OPT-IML-Max, Flan-T5-XXL, Flan-PaLM, Flan-PaLM-cont, and LLaMA-I. The performance is measured in two parameters: the model size (30B, 26.1, 120B, 44.8, 62B, 70B, 65B, 11B) and the corresponding score. The highest score is achieved by Flan-PaLM-cont with a 65B model size, scoring 68.9. The lowest score is for OPT GLM with a 30B model size, scoring 26.1.',\n",
       " \"The table presents data on the Basic Respectful LLaMA model with different parameters. The 7B model has a value of 0.106, the 13B model has a value of 0.104, the 33B model has a value of 0.107, and the 65B model has a value of 0.128. There are also values of 0.081, 0.095, 0.087, and 0.141, but it's unclear what these correspond to.\",\n",
       " 'The table presents the average scores of LLaMA and GPT3 OPT on various categories such as gender, religion, race/color, sexual orientation, age, nationality, disability, physical appearance, and socioeconomic status. The average score for LLaMA is 66.6, while for GPT3 OPT it is 67.2. The highest score for both is in the category of sexual orientation, with LLaMA scoring 81.0 and GPT3 OPT scoring 76.7. The lowest score for LLaMA is in the race/color category with 57.0, while for GPT3 OPT it is in the socioeconomic status category with 62.6. The overall average score across all categories is 69.5.',\n",
       " \"The table appears to present data on the usage of different pronouns (her/she, his/he, their/them/someone) across four categories (7B, 13B, 33B, 65B). The percentages vary across categories, with the highest percentage (81.7%) appearing in the 65B category for the use of 'their/them/someone'. There's also a 'gotcha' category for 'her/she' and 'his/he' with percentages ranging from 55.0% to 75.0%.\",\n",
       " \"The table compares the performance of two AI models, GPT-3 and LLaMA, across different parameters. GPT-3's performance ranges from 0.19 to 0.31, while LLaMA's performance is significantly higher, ranging from 0.29 to 0.57. The performance increases with the increase in the number of parameters for both models.\",\n",
       " 'The table provides information on the power consumption, total power consumption, and carbon emissions of different GPU types. All GPUs are of the A100-80GB type but have different model names. The BLOOM-175B A100-80GB has the highest total power consumption at 475 MWh and carbon emissions at 183 tCO2eq. The LLaMA-65B A100-80GB follows closely with 449 MWh total power consumption and 173 tCO2eq carbon emissions. The GPU with the least power consumption and carbon emissions is the LLaMA-7B A100-80GB, with 36 MWh and 14 tCO2eq respectively.',\n",
       " 'The table provides answers to two questions. Frank Sinatra is identified as the singer of \"Who Wants to be a Millionaire\" in High Society. The author of the book \"The Origin of Species\" is incorrectly listed as \"Target → Turkey\".',\n",
       " 'The table presents the performance of different AI models (GPT-3, Gopher, Chinchilla, LLaMA, LLaMA-1) across various subjects, including STEM, humanities, social science, and others. The performance is measured in numerical values, presumably scores or percentages. The models show varying performance across different subjects. For instance, the Chinchilla model performs exceptionally well in High School Geography and School Politics, while the GPT-3 model shows strong performance in Computer Security. The LLaMA model shows consistent performance across most subjects.',\n",
       " 'The table presents data on various high school and other subjects, categorized into Social Science, STEM, Humanities, and Other. The subjects include Macroeconomics, Mathematics, Microeconomics, Physics, Psychology, Statistics, US History, World History, Human Aging, Human Sexuality, International Law, Jurisprudence, Logical Fallacies, Machine Learning, Management, Marketing, Medical Genetics, Miscellaneous, Moral Disputes, Moral Scenarios, Nutrition, Philosophy, Prehistory, and Professional Accounting. The data seems to represent different metrics or scores, with some subjects scoring higher than others. For instance, Marketing and US History have high scores, while Moral Scenarios and Professional Accounting have lower scores.',\n",
       " 'The table presents data across various professional fields including Law, Medicine, Psychology, Public Relations, Security Studies, and Sociology. The fields are further categorized into Humanities, Other, Social Science, and an unspecified category. The data points range from 30.2 to 91.0, with Sociology in the Social Science category having the highest value of 91.0. The lowest value is 30.2 in the Law field under Humanities. The table seems to represent some form of scoring or rating system across these professional fields.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Initialize Chroma vector store with a single collection\n",
    "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=OpenAIEmbeddings())\n",
    "\n",
    "# Add summarized texts to Chroma\n",
    "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "summary_texts = [\n",
    "    Document(page_content=s, metadata={\"doc_id\": doc_ids[i], \"type\": \"text\"})\n",
    "    for i, s in enumerate(text_summaries)\n",
    "]\n",
    "vectorstore.add_documents(summary_texts)\n",
    "\n",
    "# Add summarized tables to Chroma\n",
    "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "summary_tables = [\n",
    "    Document(page_content=s, metadata={\"doc_id\": table_ids[i], \"type\": \"table\"})\n",
    "    for i, s in enumerate(table_summaries)\n",
    "]\n",
    "vectorstore.add_documents(summary_tables)\n",
    "\n",
    "# Function to retrieve and display the original content based on the doc_id\n",
    "def retrieve_original_content(query, vectorstore):\n",
    "    # Perform the similarity search\n",
    "    results = vectorstore.similarity_search(query)\n",
    "\n",
    "    # Display the results\n",
    "    for result in results:\n",
    "        doc_type = result.metadata.get(\"type\")\n",
    "        doc_id = result.metadata.get(\"doc_id\")\n",
    "        if doc_type == \"text\":\n",
    "            original_content = texts[doc_ids.index(doc_id)]\n",
    "        elif doc_type == \"table\":\n",
    "            original_content = tables[table_ids.index(doc_id)]\n",
    "        \n",
    "        print(f\"Original {doc_type.capitalize()} Content:\")\n",
    "        print(original_content)\n",
    "        print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text Content:\n",
      "Table 3: Zero-shot performance on Common Sense Reasoning tasks.\n",
      "\n",
      "reduce the memory usage of the model by using model and sequence parallelism, as described by Korthikanti et al. (2022). Moreover, we also over- lap the computation of activations and the commu- nication between GPUs over the network (due to all_reduce operations) as much as possible.\n",
      "\n",
      "When training a 65B-parameter model, our code processes around 380 tokens/sec/GPU on 2048 A100 GPU with 80GB of RAM. This means that training over our dataset containing 1.4T tokens takes approximately 21 days.\n",
      "\n",
      "3 Main results\n",
      "\n",
      "We evaluate LLaMA on free-form generation tasks and multiple choice tasks. In the multiple choice tasks, the objective is to select the most appropriate completion among a set of given op- tions, based on a provided context. We select the completion with the highest likelihood given the provided context. We follow Gao et al. (2021) and use the likelihood normalized by the number of characters in the completion, except for certain datasets (OpenBookQA, BoolQ), for which we fol- low Brown et al. (2020), and select a completion based on the likelihood normalized by the likeli- hood of the completion given “Answer:” as context: P (completion|context)/P (completion|“Answer:”).\n",
      "\n",
      "Following previous work (Brown et al., 2020), we consider zero-shot and few-shot tasks, and report results on a total of 20 benchmarks:\n",
      "\n",
      "• Zero-shot. We provide a textual description of the task and a test example. The model either provides an answer using open-ended generation, or ranks the proposed answers.\n",
      "\n",
      "• Few-shot. We provide a few examples of the task (between 1 and 64) and a test example. The model takes this text as input and gener- ates the answer or ranks different options.\n",
      "\n",
      "We compare LLaMA with other foundation mod- els, namely the non-publicly available language models GPT-3 (Brown et al., 2020), Gopher (Rae et al., 2021), Chinchilla (Hoffmann et al., 2022) and PaLM (Chowdhery et al., 2022), as well as the open-sourced OPT models (Zhang et al., 2022), GPT-J (Wang and Komatsuzaki, 2021), and GPT- Neo (Black et al., 2022). In Section 4, we also brieﬂy compare LLaMA with instruction-tuned models such as OPT-IML (Iyer et al., 2022) and Flan-PaLM (Chung et al., 2022).\n",
      "--------------------------------------------------\n",
      "Original Text Content:\n",
      "Table 7: Model performance on quantitative reason- ing datasets. For majority voting, we use the same setup as Minerva, with k = 256 samples for MATH and k = 100 for GSM8k (Minerva 540B uses k = 64 for MATH and and k = 40 for GSM8k). LLaMA-65B outperforms Minerva 62B on GSM8k, although it has not been ﬁne-tuned on mathematical data.\n",
      "\n",
      "docstring. The model needs to generate a Python program that ﬁts the description and satisﬁes the test cases. In Table 8, we compare the pass@1 scores of our models with existing language mod- els that have not been ﬁnetuned on code, namely PaLM and LaMDA (Thoppilan et al., 2022). PaLM and LLaMA were trained on datasets that contain a similar number of code tokens.\n",
      "\n",
      "As show in Table 8, for a similar number of parameters, LLaMA outperforms other gen- eral models such as LaMDA and PaLM, which are not trained or ﬁnetuned speciﬁcally for code. LLaMA with 13B parameters and more outper- forms LaMDA 137B on both HumanEval and MBPP. LLaMA 65B also outperforms PaLM 62B, even when it is trained longer. The pass@1 results reported in this table were obtained by sampling with temperature 0.1. The pass@100 and pass@80 metrics were obtained with temperature 0.8. We use the same method as Chen et al. (2021) to obtain unbiased estimates of the pass@k.\n",
      "\n",
      "It is possible to improve the performance on code by ﬁnetuning on code-speciﬁc tokens. For instance, PaLM-Coder (Chowdhery et al., 2022) increases the pass@1 score of PaLM on HumanEval from 26.2% for PaLM to 36%. Other models trained speciﬁcally for code also perform better than gen- eral models on these tasks (Chen et al., 2021; Ni- jkamp et al., 2022; Fried et al., 2022). Finetuning on code tokens is beyond the scope of this paper.\n",
      "--------------------------------------------------\n",
      "Original Table Content:\n",
      "Truthful Truthful*Inf GPT-3 1.3B 6B 175B 0.31 0.22 0.28 0.19 0.19 0.25 LLaMA 7B 13B 33B 65B 0.33 0.47 0.52 0.57 0.29 0.41 0.48 0.53\n",
      "--------------------------------------------------\n",
      "Original Text Content:\n",
      "A Question Answering\n",
      "\n",
      "We evaluate LLaMA on Natural Questions and TriviaQA. For Natural Questions we use the test split used for open-domain question answering containing 3610 questions. For TriviaQA we evaluate on the dev set of the ﬁltered set. This differs from GPT-3 and PaLM, which evaluate on the test set of the unﬁltered set for which the online evaluation server is not available anymore5.\n",
      "\n",
      "We generate answers using greedy decoding, and extract an answer from the generation by stopping at the ﬁrst line break, ﬁnal dot or comma. Generated answers are evaluated with the standard exact match metric: a generated answer is considered correct if it matches any answer of the list of answers after normalization. For this normalization step we lowercase generated answers and remove articles, punctuation and duplicate whitespaces. Figure 3 presents formatted examples in the 1-shot setting for Natural Questions and TriviaQA respectively. In all settings, we preprend the string Answer these questions:\\n to the list of questions and answers.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example query to retrieve documents\n",
    "query = \"What is the number of training tokens for LLaMA2?\"\n",
    "retrieve_original_content(query, vectorstore)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
